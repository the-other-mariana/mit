{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBneWMxsgA-a"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYGZxKcTgA-i"
      },
      "outputs": [],
      "source": [
        "# Modifications Copyright (C) 2020 Rohan Jagtap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ImPCIsrTE5sF",
        "outputId": "08d02672-f289-4d78-c8c6-28bb35ca440f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/summarizer_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/summarizer_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zbxhyl_zFlWL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K_AjGkWXITKA"
      },
      "outputs": [],
      "source": [
        "news = pd.read_excel(\"/content/news.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S-rYZhayIe9x"
      },
      "outputs": [],
      "source": [
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n",
        "document = news[\"Short\"]\n",
        "summary = news[\"Headline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oXtxc-toIc94",
        "outputId": "eef77b35-9a14-40bb-8d5f-0ee724a24ea3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Headline  \\\n",
              "0  4 ex-bank officials booked for cheating bank o...   \n",
              "1     Supreme Court to go paperless in 6 months: CJI   \n",
              "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
              "3  Why has Reliance been barred from trading in f...   \n",
              "4  Was stopped from entering my own studio at Tim...   \n",
              "\n",
              "                                               Short  \n",
              "0  The CBI on Saturday booked four former officia...  \n",
              "1  Chief Justice JS Khehar has said the Supreme C...  \n",
              "2  At least three people were killed, including a...  \n",
              "3  Mukesh Ambani-led Reliance Industries (RIL) wa...  \n",
              "4  TV news anchor Arnab Goswami has said he was t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2e202f6-9f1e-4c58-9511-122973dbbee8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2e202f6-9f1e-4c58-9511-122973dbbee8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2e202f6-9f1e-4c58-9511-122973dbbee8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2e202f6-9f1e-4c58-9511-122973dbbee8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR2hg9themaN",
        "outputId": "e2aaf76a-f007-49c0-f3b3-d70fa203f01c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55104, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "news.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d4cEp3wmI2BX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z55AhpKIdK7",
        "outputId": "aebe118b-2970-4e72-8746-44d409c1cd71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('According to the Guinness World Records, the most generations alive in a single family have been seven.  The difference between the oldest and the youngest person in the family was about 109 years, when Augusta Bunge&#39;s great-great-great-great grandson was born on January 21, 1989. The family belonged to the United States of America.',\n",
              " 'The most generations alive in a single family have been 7')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "document[30], summary[30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ6LE4MrJjC_",
        "outputId": "caabbd5e-0e8f-4ebc-f7a4-4091d671c090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> 4 ex-bank officials booked for cheating b...\n",
              "1    <go> Supreme Court to go paperless in 6 months...\n",
              "2    <go> At least 3 killed, 30 injured in blast in...\n",
              "3    <go> Why has Reliance been barred from trading...\n",
              "4    <go> Was stopped from entering my own studio a...\n",
              "Name: Headline, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7TqbpEyPMRqa"
      },
      "outputs": [],
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cHw2csoYImsa"
      },
      "outputs": [],
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DWU9Xu7OKVab"
      },
      "outputs": [],
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3ESm-aYR-tvx"
      },
      "outputs": [],
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVyErXAei5_b",
        "outputId": "9f3fb4c2-dbc2-4a27-8b00-6d83297a1687"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[184, 22, 12, 71]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryx9qx90jwXu",
        "outputId": "6f5967d0-b4a7-412f-c407-1eb8e85c817e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this is a test']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoizyBvLKv8h",
        "outputId": "dfba545b-2974-45f6-95d1-fb0596cd5c3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76362, 29661)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ma4o2nGdK5Xb"
      },
      "outputs": [],
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZlO99C-UXK",
        "outputId": "9263858d-d7dd-4f3b-9b2b-6dbe4529cf4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean       368.003049\n",
              "std         26.235510\n",
              "min        280.000000\n",
              "25%        350.000000\n",
              "50%        369.000000\n",
              "75%        387.000000\n",
              "max        469.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "document_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALMwKMx--ZF7",
        "outputId": "1dc38c47-1010-4062-a950-1d1e6913d70e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean        63.620282\n",
              "std          7.267463\n",
              "min         20.000000\n",
              "25%         59.000000\n",
              "50%         63.000000\n",
              "75%         69.000000\n",
              "max         96.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "summary_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cVeMilXr-bpC"
      },
      "outputs": [],
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vEyUBeu7ACRt"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LzO6l3-AB7hJ"
      },
      "outputs": [],
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "slZ5f4P4DurS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wI-fV7eABWN6"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Purv7oyhETDZ"
      },
      "outputs": [],
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "40J2pc2NEXp5"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hN1wVQAdMVYy"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UmjAPLWuMREE"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "w_B6M9OBNBKB"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iIuFrdXnNZEC"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "d9-qoKuTNwKq"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HNuoJoFWO335"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7CVmvs6dPMRC"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BrbnTwijQJ-h"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UmeqkZrIRbSB"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FXHRG-o4R9Mc"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lMTZJdIoSbuy"
      },
      "outputs": [],
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tfiynCLlTL8C"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)  # Cast `step` to float\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY"
      },
      "source": [
        "#### Defining losses and other metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ip1-943kTXXK"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ktKwyvKtTvF6"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uW4LA_45T4Aa"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Ze0u6xxXT7dI"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "d5-RcxqFUCuk"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    encoder_vocab_size,\n",
        "    decoder_vocab_size,\n",
        "    pe_input=encoder_vocab_size,\n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FZxHuyZxU5Pa"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tOc1_3c-VGaL"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xmVOMzkrczgl"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp,\n",
        "            True,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xORKpv69dSW5",
        "outputId": "21a8ddad-d40f-4adf-d975-1f1900e9afc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe truncaron las últimas líneas 5000 del resultado de transmisión.\u001b[0m\n",
            "Epoch 3 Batch 672 Loss 5.6905\n",
            "Epoch 3 Batch 675 Loss 5.6894\n",
            "Epoch 3 Batch 678 Loss 5.6876\n",
            "Epoch 3 Batch 681 Loss 5.6869\n",
            "Epoch 3 Batch 684 Loss 5.6855\n",
            "Epoch 3 Batch 687 Loss 5.6840\n",
            "Epoch 3 Batch 690 Loss 5.6829\n",
            "Epoch 3 Batch 693 Loss 5.6817\n",
            "Epoch 3 Batch 696 Loss 5.6807\n",
            "Epoch 3 Batch 699 Loss 5.6798\n",
            "Epoch 3 Batch 702 Loss 5.6783\n",
            "Epoch 3 Batch 705 Loss 5.6777\n",
            "Epoch 3 Batch 708 Loss 5.6775\n",
            "Epoch 3 Batch 711 Loss 5.6768\n",
            "Epoch 3 Batch 714 Loss 5.6763\n",
            "Epoch 3 Batch 717 Loss 5.6750\n",
            "Epoch 3 Batch 720 Loss 5.6739\n",
            "Epoch 3 Batch 723 Loss 5.6736\n",
            "Epoch 3 Batch 726 Loss 5.6732\n",
            "Epoch 3 Batch 729 Loss 5.6726\n",
            "Epoch 3 Batch 732 Loss 5.6720\n",
            "Epoch 3 Batch 735 Loss 5.6711\n",
            "Epoch 3 Batch 738 Loss 5.6707\n",
            "Epoch 3 Batch 741 Loss 5.6696\n",
            "Epoch 3 Batch 744 Loss 5.6687\n",
            "Epoch 3 Batch 747 Loss 5.6676\n",
            "Epoch 3 Batch 750 Loss 5.6671\n",
            "Epoch 3 Batch 753 Loss 5.6666\n",
            "Epoch 3 Batch 756 Loss 5.6666\n",
            "Epoch 3 Batch 759 Loss 5.6665\n",
            "Epoch 3 Batch 762 Loss 5.6662\n",
            "Epoch 3 Batch 765 Loss 5.6656\n",
            "Epoch 3 Batch 768 Loss 5.6650\n",
            "Epoch 3 Batch 771 Loss 5.6635\n",
            "Epoch 3 Batch 774 Loss 5.6633\n",
            "Epoch 3 Batch 777 Loss 5.6628\n",
            "Epoch 3 Batch 780 Loss 5.6621\n",
            "Epoch 3 Batch 783 Loss 5.6614\n",
            "Epoch 3 Batch 786 Loss 5.6610\n",
            "Epoch 3 Batch 789 Loss 5.6599\n",
            "Epoch 3 Batch 792 Loss 5.6588\n",
            "Epoch 3 Batch 795 Loss 5.6584\n",
            "Epoch 3 Batch 798 Loss 5.6581\n",
            "Epoch 3 Batch 801 Loss 5.6572\n",
            "Epoch 3 Batch 804 Loss 5.6567\n",
            "Epoch 3 Batch 807 Loss 5.6554\n",
            "Epoch 3 Batch 810 Loss 5.6551\n",
            "Epoch 3 Batch 813 Loss 5.6544\n",
            "Epoch 3 Batch 816 Loss 5.6535\n",
            "Epoch 3 Batch 819 Loss 5.6523\n",
            "Epoch 3 Batch 822 Loss 5.6512\n",
            "Epoch 3 Batch 825 Loss 5.6511\n",
            "Epoch 3 Batch 828 Loss 5.6503\n",
            "Epoch 3 Batch 831 Loss 5.6493\n",
            "Epoch 3 Batch 834 Loss 5.6485\n",
            "Epoch 3 Batch 837 Loss 5.6480\n",
            "Epoch 3 Batch 840 Loss 5.6472\n",
            "Epoch 3 Batch 843 Loss 5.6463\n",
            "Epoch 3 Batch 846 Loss 5.6455\n",
            "Epoch 3 Batch 849 Loss 5.6447\n",
            "Epoch 3 Batch 852 Loss 5.6443\n",
            "Epoch 3 Batch 855 Loss 5.6438\n",
            "Epoch 3 Batch 858 Loss 5.6431\n",
            "Epoch 3 Loss 5.6428\n",
            "Time taken for 1 epoch: 330.7445499897003 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.3453\n",
            "Epoch 4 Batch 3 Loss 5.4813\n",
            "Epoch 4 Batch 6 Loss 5.4272\n",
            "Epoch 4 Batch 9 Loss 5.4248\n",
            "Epoch 4 Batch 12 Loss 5.4250\n",
            "Epoch 4 Batch 15 Loss 5.4427\n",
            "Epoch 4 Batch 18 Loss 5.4295\n",
            "Epoch 4 Batch 21 Loss 5.4172\n",
            "Epoch 4 Batch 24 Loss 5.4170\n",
            "Epoch 4 Batch 27 Loss 5.4316\n",
            "Epoch 4 Batch 30 Loss 5.4292\n",
            "Epoch 4 Batch 33 Loss 5.4270\n",
            "Epoch 4 Batch 36 Loss 5.4203\n",
            "Epoch 4 Batch 39 Loss 5.4194\n",
            "Epoch 4 Batch 42 Loss 5.4186\n",
            "Epoch 4 Batch 45 Loss 5.4137\n",
            "Epoch 4 Batch 48 Loss 5.4103\n",
            "Epoch 4 Batch 51 Loss 5.4148\n",
            "Epoch 4 Batch 54 Loss 5.4195\n",
            "Epoch 4 Batch 57 Loss 5.4189\n",
            "Epoch 4 Batch 60 Loss 5.4132\n",
            "Epoch 4 Batch 63 Loss 5.4180\n",
            "Epoch 4 Batch 66 Loss 5.4249\n",
            "Epoch 4 Batch 69 Loss 5.4238\n",
            "Epoch 4 Batch 72 Loss 5.4204\n",
            "Epoch 4 Batch 75 Loss 5.4172\n",
            "Epoch 4 Batch 78 Loss 5.4084\n",
            "Epoch 4 Batch 81 Loss 5.4073\n",
            "Epoch 4 Batch 84 Loss 5.4057\n",
            "Epoch 4 Batch 87 Loss 5.4051\n",
            "Epoch 4 Batch 90 Loss 5.4006\n",
            "Epoch 4 Batch 93 Loss 5.3957\n",
            "Epoch 4 Batch 96 Loss 5.3970\n",
            "Epoch 4 Batch 99 Loss 5.3997\n",
            "Epoch 4 Batch 102 Loss 5.4023\n",
            "Epoch 4 Batch 105 Loss 5.4014\n",
            "Epoch 4 Batch 108 Loss 5.4005\n",
            "Epoch 4 Batch 111 Loss 5.3972\n",
            "Epoch 4 Batch 114 Loss 5.3940\n",
            "Epoch 4 Batch 117 Loss 5.3944\n",
            "Epoch 4 Batch 120 Loss 5.3948\n",
            "Epoch 4 Batch 123 Loss 5.3930\n",
            "Epoch 4 Batch 126 Loss 5.3934\n",
            "Epoch 4 Batch 129 Loss 5.3926\n",
            "Epoch 4 Batch 132 Loss 5.3909\n",
            "Epoch 4 Batch 135 Loss 5.3893\n",
            "Epoch 4 Batch 138 Loss 5.3888\n",
            "Epoch 4 Batch 141 Loss 5.3890\n",
            "Epoch 4 Batch 144 Loss 5.3885\n",
            "Epoch 4 Batch 147 Loss 5.3846\n",
            "Epoch 4 Batch 150 Loss 5.3824\n",
            "Epoch 4 Batch 153 Loss 5.3812\n",
            "Epoch 4 Batch 156 Loss 5.3795\n",
            "Epoch 4 Batch 159 Loss 5.3808\n",
            "Epoch 4 Batch 162 Loss 5.3795\n",
            "Epoch 4 Batch 165 Loss 5.3787\n",
            "Epoch 4 Batch 168 Loss 5.3766\n",
            "Epoch 4 Batch 171 Loss 5.3753\n",
            "Epoch 4 Batch 174 Loss 5.3739\n",
            "Epoch 4 Batch 177 Loss 5.3699\n",
            "Epoch 4 Batch 180 Loss 5.3674\n",
            "Epoch 4 Batch 183 Loss 5.3664\n",
            "Epoch 4 Batch 186 Loss 5.3690\n",
            "Epoch 4 Batch 189 Loss 5.3691\n",
            "Epoch 4 Batch 192 Loss 5.3695\n",
            "Epoch 4 Batch 195 Loss 5.3690\n",
            "Epoch 4 Batch 198 Loss 5.3701\n",
            "Epoch 4 Batch 201 Loss 5.3685\n",
            "Epoch 4 Batch 204 Loss 5.3665\n",
            "Epoch 4 Batch 207 Loss 5.3647\n",
            "Epoch 4 Batch 210 Loss 5.3638\n",
            "Epoch 4 Batch 213 Loss 5.3622\n",
            "Epoch 4 Batch 216 Loss 5.3626\n",
            "Epoch 4 Batch 219 Loss 5.3624\n",
            "Epoch 4 Batch 222 Loss 5.3602\n",
            "Epoch 4 Batch 225 Loss 5.3580\n",
            "Epoch 4 Batch 228 Loss 5.3570\n",
            "Epoch 4 Batch 231 Loss 5.3571\n",
            "Epoch 4 Batch 234 Loss 5.3556\n",
            "Epoch 4 Batch 237 Loss 5.3543\n",
            "Epoch 4 Batch 240 Loss 5.3543\n",
            "Epoch 4 Batch 243 Loss 5.3560\n",
            "Epoch 4 Batch 246 Loss 5.3556\n",
            "Epoch 4 Batch 249 Loss 5.3542\n",
            "Epoch 4 Batch 252 Loss 5.3543\n",
            "Epoch 4 Batch 255 Loss 5.3548\n",
            "Epoch 4 Batch 258 Loss 5.3535\n",
            "Epoch 4 Batch 261 Loss 5.3525\n",
            "Epoch 4 Batch 264 Loss 5.3527\n",
            "Epoch 4 Batch 267 Loss 5.3518\n",
            "Epoch 4 Batch 270 Loss 5.3518\n",
            "Epoch 4 Batch 273 Loss 5.3514\n",
            "Epoch 4 Batch 276 Loss 5.3504\n",
            "Epoch 4 Batch 279 Loss 5.3503\n",
            "Epoch 4 Batch 282 Loss 5.3495\n",
            "Epoch 4 Batch 285 Loss 5.3489\n",
            "Epoch 4 Batch 288 Loss 5.3488\n",
            "Epoch 4 Batch 291 Loss 5.3492\n",
            "Epoch 4 Batch 294 Loss 5.3487\n",
            "Epoch 4 Batch 297 Loss 5.3483\n",
            "Epoch 4 Batch 300 Loss 5.3487\n",
            "Epoch 4 Batch 303 Loss 5.3469\n",
            "Epoch 4 Batch 306 Loss 5.3458\n",
            "Epoch 4 Batch 309 Loss 5.3459\n",
            "Epoch 4 Batch 312 Loss 5.3468\n",
            "Epoch 4 Batch 315 Loss 5.3460\n",
            "Epoch 4 Batch 318 Loss 5.3466\n",
            "Epoch 4 Batch 321 Loss 5.3484\n",
            "Epoch 4 Batch 324 Loss 5.3488\n",
            "Epoch 4 Batch 327 Loss 5.3482\n",
            "Epoch 4 Batch 330 Loss 5.3480\n",
            "Epoch 4 Batch 333 Loss 5.3470\n",
            "Epoch 4 Batch 336 Loss 5.3472\n",
            "Epoch 4 Batch 339 Loss 5.3459\n",
            "Epoch 4 Batch 342 Loss 5.3457\n",
            "Epoch 4 Batch 345 Loss 5.3447\n",
            "Epoch 4 Batch 348 Loss 5.3452\n",
            "Epoch 4 Batch 351 Loss 5.3451\n",
            "Epoch 4 Batch 354 Loss 5.3450\n",
            "Epoch 4 Batch 357 Loss 5.3450\n",
            "Epoch 4 Batch 360 Loss 5.3436\n",
            "Epoch 4 Batch 363 Loss 5.3425\n",
            "Epoch 4 Batch 366 Loss 5.3416\n",
            "Epoch 4 Batch 369 Loss 5.3414\n",
            "Epoch 4 Batch 372 Loss 5.3408\n",
            "Epoch 4 Batch 375 Loss 5.3404\n",
            "Epoch 4 Batch 378 Loss 5.3402\n",
            "Epoch 4 Batch 381 Loss 5.3389\n",
            "Epoch 4 Batch 384 Loss 5.3375\n",
            "Epoch 4 Batch 387 Loss 5.3355\n",
            "Epoch 4 Batch 390 Loss 5.3345\n",
            "Epoch 4 Batch 393 Loss 5.3344\n",
            "Epoch 4 Batch 396 Loss 5.3330\n",
            "Epoch 4 Batch 399 Loss 5.3324\n",
            "Epoch 4 Batch 402 Loss 5.3316\n",
            "Epoch 4 Batch 405 Loss 5.3316\n",
            "Epoch 4 Batch 408 Loss 5.3308\n",
            "Epoch 4 Batch 411 Loss 5.3296\n",
            "Epoch 4 Batch 414 Loss 5.3296\n",
            "Epoch 4 Batch 417 Loss 5.3280\n",
            "Epoch 4 Batch 420 Loss 5.3289\n",
            "Epoch 4 Batch 423 Loss 5.3277\n",
            "Epoch 4 Batch 426 Loss 5.3265\n",
            "Epoch 4 Batch 429 Loss 5.3249\n",
            "Epoch 4 Batch 432 Loss 5.3240\n",
            "Epoch 4 Batch 435 Loss 5.3241\n",
            "Epoch 4 Batch 438 Loss 5.3234\n",
            "Epoch 4 Batch 441 Loss 5.3227\n",
            "Epoch 4 Batch 444 Loss 5.3234\n",
            "Epoch 4 Batch 447 Loss 5.3233\n",
            "Epoch 4 Batch 450 Loss 5.3237\n",
            "Epoch 4 Batch 453 Loss 5.3226\n",
            "Epoch 4 Batch 456 Loss 5.3217\n",
            "Epoch 4 Batch 459 Loss 5.3211\n",
            "Epoch 4 Batch 462 Loss 5.3202\n",
            "Epoch 4 Batch 465 Loss 5.3191\n",
            "Epoch 4 Batch 468 Loss 5.3183\n",
            "Epoch 4 Batch 471 Loss 5.3174\n",
            "Epoch 4 Batch 474 Loss 5.3169\n",
            "Epoch 4 Batch 477 Loss 5.3165\n",
            "Epoch 4 Batch 480 Loss 5.3160\n",
            "Epoch 4 Batch 483 Loss 5.3153\n",
            "Epoch 4 Batch 486 Loss 5.3151\n",
            "Epoch 4 Batch 489 Loss 5.3152\n",
            "Epoch 4 Batch 492 Loss 5.3145\n",
            "Epoch 4 Batch 495 Loss 5.3141\n",
            "Epoch 4 Batch 498 Loss 5.3141\n",
            "Epoch 4 Batch 501 Loss 5.3126\n",
            "Epoch 4 Batch 504 Loss 5.3118\n",
            "Epoch 4 Batch 507 Loss 5.3112\n",
            "Epoch 4 Batch 510 Loss 5.3106\n",
            "Epoch 4 Batch 513 Loss 5.3108\n",
            "Epoch 4 Batch 516 Loss 5.3100\n",
            "Epoch 4 Batch 519 Loss 5.3093\n",
            "Epoch 4 Batch 522 Loss 5.3092\n",
            "Epoch 4 Batch 525 Loss 5.3077\n",
            "Epoch 4 Batch 528 Loss 5.3072\n",
            "Epoch 4 Batch 531 Loss 5.3066\n",
            "Epoch 4 Batch 534 Loss 5.3064\n",
            "Epoch 4 Batch 537 Loss 5.3055\n",
            "Epoch 4 Batch 540 Loss 5.3051\n",
            "Epoch 4 Batch 543 Loss 5.3043\n",
            "Epoch 4 Batch 546 Loss 5.3037\n",
            "Epoch 4 Batch 549 Loss 5.3028\n",
            "Epoch 4 Batch 552 Loss 5.3015\n",
            "Epoch 4 Batch 555 Loss 5.3003\n",
            "Epoch 4 Batch 558 Loss 5.2995\n",
            "Epoch 4 Batch 561 Loss 5.2989\n",
            "Epoch 4 Batch 564 Loss 5.2976\n",
            "Epoch 4 Batch 567 Loss 5.2973\n",
            "Epoch 4 Batch 570 Loss 5.2967\n",
            "Epoch 4 Batch 573 Loss 5.2957\n",
            "Epoch 4 Batch 576 Loss 5.2950\n",
            "Epoch 4 Batch 579 Loss 5.2937\n",
            "Epoch 4 Batch 582 Loss 5.2933\n",
            "Epoch 4 Batch 585 Loss 5.2928\n",
            "Epoch 4 Batch 588 Loss 5.2921\n",
            "Epoch 4 Batch 591 Loss 5.2915\n",
            "Epoch 4 Batch 594 Loss 5.2902\n",
            "Epoch 4 Batch 597 Loss 5.2899\n",
            "Epoch 4 Batch 600 Loss 5.2891\n",
            "Epoch 4 Batch 603 Loss 5.2886\n",
            "Epoch 4 Batch 606 Loss 5.2870\n",
            "Epoch 4 Batch 609 Loss 5.2860\n",
            "Epoch 4 Batch 612 Loss 5.2846\n",
            "Epoch 4 Batch 615 Loss 5.2841\n",
            "Epoch 4 Batch 618 Loss 5.2828\n",
            "Epoch 4 Batch 621 Loss 5.2820\n",
            "Epoch 4 Batch 624 Loss 5.2811\n",
            "Epoch 4 Batch 627 Loss 5.2805\n",
            "Epoch 4 Batch 630 Loss 5.2804\n",
            "Epoch 4 Batch 633 Loss 5.2800\n",
            "Epoch 4 Batch 636 Loss 5.2800\n",
            "Epoch 4 Batch 639 Loss 5.2793\n",
            "Epoch 4 Batch 642 Loss 5.2787\n",
            "Epoch 4 Batch 645 Loss 5.2779\n",
            "Epoch 4 Batch 648 Loss 5.2774\n",
            "Epoch 4 Batch 651 Loss 5.2767\n",
            "Epoch 4 Batch 654 Loss 5.2765\n",
            "Epoch 4 Batch 657 Loss 5.2767\n",
            "Epoch 4 Batch 660 Loss 5.2765\n",
            "Epoch 4 Batch 663 Loss 5.2756\n",
            "Epoch 4 Batch 666 Loss 5.2744\n",
            "Epoch 4 Batch 669 Loss 5.2738\n",
            "Epoch 4 Batch 672 Loss 5.2734\n",
            "Epoch 4 Batch 675 Loss 5.2723\n",
            "Epoch 4 Batch 678 Loss 5.2720\n",
            "Epoch 4 Batch 681 Loss 5.2713\n",
            "Epoch 4 Batch 684 Loss 5.2710\n",
            "Epoch 4 Batch 687 Loss 5.2706\n",
            "Epoch 4 Batch 690 Loss 5.2700\n",
            "Epoch 4 Batch 693 Loss 5.2686\n",
            "Epoch 4 Batch 696 Loss 5.2680\n",
            "Epoch 4 Batch 699 Loss 5.2680\n",
            "Epoch 4 Batch 702 Loss 5.2669\n",
            "Epoch 4 Batch 705 Loss 5.2666\n",
            "Epoch 4 Batch 708 Loss 5.2659\n",
            "Epoch 4 Batch 711 Loss 5.2648\n",
            "Epoch 4 Batch 714 Loss 5.2645\n",
            "Epoch 4 Batch 717 Loss 5.2638\n",
            "Epoch 4 Batch 720 Loss 5.2633\n",
            "Epoch 4 Batch 723 Loss 5.2633\n",
            "Epoch 4 Batch 726 Loss 5.2630\n",
            "Epoch 4 Batch 729 Loss 5.2622\n",
            "Epoch 4 Batch 732 Loss 5.2615\n",
            "Epoch 4 Batch 735 Loss 5.2614\n",
            "Epoch 4 Batch 738 Loss 5.2612\n",
            "Epoch 4 Batch 741 Loss 5.2608\n",
            "Epoch 4 Batch 744 Loss 5.2602\n",
            "Epoch 4 Batch 747 Loss 5.2595\n",
            "Epoch 4 Batch 750 Loss 5.2591\n",
            "Epoch 4 Batch 753 Loss 5.2584\n",
            "Epoch 4 Batch 756 Loss 5.2574\n",
            "Epoch 4 Batch 759 Loss 5.2572\n",
            "Epoch 4 Batch 762 Loss 5.2567\n",
            "Epoch 4 Batch 765 Loss 5.2558\n",
            "Epoch 4 Batch 768 Loss 5.2555\n",
            "Epoch 4 Batch 771 Loss 5.2546\n",
            "Epoch 4 Batch 774 Loss 5.2539\n",
            "Epoch 4 Batch 777 Loss 5.2535\n",
            "Epoch 4 Batch 780 Loss 5.2530\n",
            "Epoch 4 Batch 783 Loss 5.2525\n",
            "Epoch 4 Batch 786 Loss 5.2523\n",
            "Epoch 4 Batch 789 Loss 5.2515\n",
            "Epoch 4 Batch 792 Loss 5.2502\n",
            "Epoch 4 Batch 795 Loss 5.2493\n",
            "Epoch 4 Batch 798 Loss 5.2485\n",
            "Epoch 4 Batch 801 Loss 5.2486\n",
            "Epoch 4 Batch 804 Loss 5.2484\n",
            "Epoch 4 Batch 807 Loss 5.2474\n",
            "Epoch 4 Batch 810 Loss 5.2470\n",
            "Epoch 4 Batch 813 Loss 5.2468\n",
            "Epoch 4 Batch 816 Loss 5.2466\n",
            "Epoch 4 Batch 819 Loss 5.2474\n",
            "Epoch 4 Batch 822 Loss 5.2463\n",
            "Epoch 4 Batch 825 Loss 5.2461\n",
            "Epoch 4 Batch 828 Loss 5.2460\n",
            "Epoch 4 Batch 831 Loss 5.2456\n",
            "Epoch 4 Batch 834 Loss 5.2458\n",
            "Epoch 4 Batch 837 Loss 5.2455\n",
            "Epoch 4 Batch 840 Loss 5.2451\n",
            "Epoch 4 Batch 843 Loss 5.2445\n",
            "Epoch 4 Batch 846 Loss 5.2437\n",
            "Epoch 4 Batch 849 Loss 5.2437\n",
            "Epoch 4 Batch 852 Loss 5.2432\n",
            "Epoch 4 Batch 855 Loss 5.2422\n",
            "Epoch 4 Batch 858 Loss 5.2417\n",
            "Epoch 4 Loss 5.2413\n",
            "Time taken for 1 epoch: 329.574027299881 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.9711\n",
            "Epoch 5 Batch 3 Loss 5.1127\n",
            "Epoch 5 Batch 6 Loss 5.1155\n",
            "Epoch 5 Batch 9 Loss 5.1181\n",
            "Epoch 5 Batch 12 Loss 5.0774\n",
            "Epoch 5 Batch 15 Loss 5.0833\n",
            "Epoch 5 Batch 18 Loss 5.0932\n",
            "Epoch 5 Batch 21 Loss 5.0817\n",
            "Epoch 5 Batch 24 Loss 5.0784\n",
            "Epoch 5 Batch 27 Loss 5.0831\n",
            "Epoch 5 Batch 30 Loss 5.0778\n",
            "Epoch 5 Batch 33 Loss 5.0706\n",
            "Epoch 5 Batch 36 Loss 5.0740\n",
            "Epoch 5 Batch 39 Loss 5.0573\n",
            "Epoch 5 Batch 42 Loss 5.0591\n",
            "Epoch 5 Batch 45 Loss 5.0560\n",
            "Epoch 5 Batch 48 Loss 5.0529\n",
            "Epoch 5 Batch 51 Loss 5.0518\n",
            "Epoch 5 Batch 54 Loss 5.0552\n",
            "Epoch 5 Batch 57 Loss 5.0518\n",
            "Epoch 5 Batch 60 Loss 5.0531\n",
            "Epoch 5 Batch 63 Loss 5.0588\n",
            "Epoch 5 Batch 66 Loss 5.0568\n",
            "Epoch 5 Batch 69 Loss 5.0630\n",
            "Epoch 5 Batch 72 Loss 5.0561\n",
            "Epoch 5 Batch 75 Loss 5.0506\n",
            "Epoch 5 Batch 78 Loss 5.0455\n",
            "Epoch 5 Batch 81 Loss 5.0416\n",
            "Epoch 5 Batch 84 Loss 5.0336\n",
            "Epoch 5 Batch 87 Loss 5.0300\n",
            "Epoch 5 Batch 90 Loss 5.0284\n",
            "Epoch 5 Batch 93 Loss 5.0237\n",
            "Epoch 5 Batch 96 Loss 5.0278\n",
            "Epoch 5 Batch 99 Loss 5.0293\n",
            "Epoch 5 Batch 102 Loss 5.0305\n",
            "Epoch 5 Batch 105 Loss 5.0310\n",
            "Epoch 5 Batch 108 Loss 5.0306\n",
            "Epoch 5 Batch 111 Loss 5.0303\n",
            "Epoch 5 Batch 114 Loss 5.0275\n",
            "Epoch 5 Batch 117 Loss 5.0264\n",
            "Epoch 5 Batch 120 Loss 5.0298\n",
            "Epoch 5 Batch 123 Loss 5.0328\n",
            "Epoch 5 Batch 126 Loss 5.0297\n",
            "Epoch 5 Batch 129 Loss 5.0289\n",
            "Epoch 5 Batch 132 Loss 5.0305\n",
            "Epoch 5 Batch 135 Loss 5.0306\n",
            "Epoch 5 Batch 138 Loss 5.0273\n",
            "Epoch 5 Batch 141 Loss 5.0245\n",
            "Epoch 5 Batch 144 Loss 5.0248\n",
            "Epoch 5 Batch 147 Loss 5.0242\n",
            "Epoch 5 Batch 150 Loss 5.0223\n",
            "Epoch 5 Batch 153 Loss 5.0187\n",
            "Epoch 5 Batch 156 Loss 5.0167\n",
            "Epoch 5 Batch 159 Loss 5.0171\n",
            "Epoch 5 Batch 162 Loss 5.0138\n",
            "Epoch 5 Batch 165 Loss 5.0120\n",
            "Epoch 5 Batch 168 Loss 5.0112\n",
            "Epoch 5 Batch 171 Loss 5.0105\n",
            "Epoch 5 Batch 174 Loss 5.0078\n",
            "Epoch 5 Batch 177 Loss 5.0081\n",
            "Epoch 5 Batch 180 Loss 5.0070\n",
            "Epoch 5 Batch 183 Loss 5.0079\n",
            "Epoch 5 Batch 186 Loss 5.0068\n",
            "Epoch 5 Batch 189 Loss 5.0055\n",
            "Epoch 5 Batch 192 Loss 5.0027\n",
            "Epoch 5 Batch 195 Loss 5.0025\n",
            "Epoch 5 Batch 198 Loss 5.0034\n",
            "Epoch 5 Batch 201 Loss 5.0032\n",
            "Epoch 5 Batch 204 Loss 5.0033\n",
            "Epoch 5 Batch 207 Loss 5.0027\n",
            "Epoch 5 Batch 210 Loss 5.0006\n",
            "Epoch 5 Batch 213 Loss 4.9987\n",
            "Epoch 5 Batch 216 Loss 4.9955\n",
            "Epoch 5 Batch 219 Loss 4.9953\n",
            "Epoch 5 Batch 222 Loss 4.9926\n",
            "Epoch 5 Batch 225 Loss 4.9932\n",
            "Epoch 5 Batch 228 Loss 4.9922\n",
            "Epoch 5 Batch 231 Loss 4.9909\n",
            "Epoch 5 Batch 234 Loss 4.9897\n",
            "Epoch 5 Batch 237 Loss 4.9899\n",
            "Epoch 5 Batch 240 Loss 4.9891\n",
            "Epoch 5 Batch 243 Loss 4.9884\n",
            "Epoch 5 Batch 246 Loss 4.9875\n",
            "Epoch 5 Batch 249 Loss 4.9865\n",
            "Epoch 5 Batch 252 Loss 4.9884\n",
            "Epoch 5 Batch 255 Loss 4.9876\n",
            "Epoch 5 Batch 258 Loss 4.9871\n",
            "Epoch 5 Batch 261 Loss 4.9880\n",
            "Epoch 5 Batch 264 Loss 4.9880\n",
            "Epoch 5 Batch 267 Loss 4.9874\n",
            "Epoch 5 Batch 270 Loss 4.9863\n",
            "Epoch 5 Batch 273 Loss 4.9854\n",
            "Epoch 5 Batch 276 Loss 4.9864\n",
            "Epoch 5 Batch 279 Loss 4.9862\n",
            "Epoch 5 Batch 282 Loss 4.9844\n",
            "Epoch 5 Batch 285 Loss 4.9825\n",
            "Epoch 5 Batch 288 Loss 4.9821\n",
            "Epoch 5 Batch 291 Loss 4.9800\n",
            "Epoch 5 Batch 294 Loss 4.9790\n",
            "Epoch 5 Batch 297 Loss 4.9788\n",
            "Epoch 5 Batch 300 Loss 4.9792\n",
            "Epoch 5 Batch 303 Loss 4.9797\n",
            "Epoch 5 Batch 306 Loss 4.9772\n",
            "Epoch 5 Batch 309 Loss 4.9768\n",
            "Epoch 5 Batch 312 Loss 4.9759\n",
            "Epoch 5 Batch 315 Loss 4.9751\n",
            "Epoch 5 Batch 318 Loss 4.9750\n",
            "Epoch 5 Batch 321 Loss 4.9740\n",
            "Epoch 5 Batch 324 Loss 4.9734\n",
            "Epoch 5 Batch 327 Loss 4.9729\n",
            "Epoch 5 Batch 330 Loss 4.9732\n",
            "Epoch 5 Batch 333 Loss 4.9737\n",
            "Epoch 5 Batch 336 Loss 4.9750\n",
            "Epoch 5 Batch 339 Loss 4.9745\n",
            "Epoch 5 Batch 342 Loss 4.9749\n",
            "Epoch 5 Batch 345 Loss 4.9735\n",
            "Epoch 5 Batch 348 Loss 4.9734\n",
            "Epoch 5 Batch 351 Loss 4.9720\n",
            "Epoch 5 Batch 354 Loss 4.9723\n",
            "Epoch 5 Batch 357 Loss 4.9717\n",
            "Epoch 5 Batch 360 Loss 4.9717\n",
            "Epoch 5 Batch 363 Loss 4.9715\n",
            "Epoch 5 Batch 366 Loss 4.9713\n",
            "Epoch 5 Batch 369 Loss 4.9710\n",
            "Epoch 5 Batch 372 Loss 4.9711\n",
            "Epoch 5 Batch 375 Loss 4.9711\n",
            "Epoch 5 Batch 378 Loss 4.9706\n",
            "Epoch 5 Batch 381 Loss 4.9707\n",
            "Epoch 5 Batch 384 Loss 4.9704\n",
            "Epoch 5 Batch 387 Loss 4.9698\n",
            "Epoch 5 Batch 390 Loss 4.9704\n",
            "Epoch 5 Batch 393 Loss 4.9692\n",
            "Epoch 5 Batch 396 Loss 4.9695\n",
            "Epoch 5 Batch 399 Loss 4.9695\n",
            "Epoch 5 Batch 402 Loss 4.9695\n",
            "Epoch 5 Batch 405 Loss 4.9689\n",
            "Epoch 5 Batch 408 Loss 4.9683\n",
            "Epoch 5 Batch 411 Loss 4.9681\n",
            "Epoch 5 Batch 414 Loss 4.9674\n",
            "Epoch 5 Batch 417 Loss 4.9661\n",
            "Epoch 5 Batch 420 Loss 4.9654\n",
            "Epoch 5 Batch 423 Loss 4.9642\n",
            "Epoch 5 Batch 426 Loss 4.9634\n",
            "Epoch 5 Batch 429 Loss 4.9623\n",
            "Epoch 5 Batch 432 Loss 4.9617\n",
            "Epoch 5 Batch 435 Loss 4.9600\n",
            "Epoch 5 Batch 438 Loss 4.9594\n",
            "Epoch 5 Batch 441 Loss 4.9596\n",
            "Epoch 5 Batch 444 Loss 4.9585\n",
            "Epoch 5 Batch 447 Loss 4.9578\n",
            "Epoch 5 Batch 450 Loss 4.9584\n",
            "Epoch 5 Batch 453 Loss 4.9564\n",
            "Epoch 5 Batch 456 Loss 4.9565\n",
            "Epoch 5 Batch 459 Loss 4.9556\n",
            "Epoch 5 Batch 462 Loss 4.9551\n",
            "Epoch 5 Batch 465 Loss 4.9547\n",
            "Epoch 5 Batch 468 Loss 4.9551\n",
            "Epoch 5 Batch 471 Loss 4.9536\n",
            "Epoch 5 Batch 474 Loss 4.9526\n",
            "Epoch 5 Batch 477 Loss 4.9522\n",
            "Epoch 5 Batch 480 Loss 4.9512\n",
            "Epoch 5 Batch 483 Loss 4.9494\n",
            "Epoch 5 Batch 486 Loss 4.9489\n",
            "Epoch 5 Batch 489 Loss 4.9486\n",
            "Epoch 5 Batch 492 Loss 4.9483\n",
            "Epoch 5 Batch 495 Loss 4.9474\n",
            "Epoch 5 Batch 498 Loss 4.9461\n",
            "Epoch 5 Batch 501 Loss 4.9452\n",
            "Epoch 5 Batch 504 Loss 4.9446\n",
            "Epoch 5 Batch 507 Loss 4.9443\n",
            "Epoch 5 Batch 510 Loss 4.9430\n",
            "Epoch 5 Batch 513 Loss 4.9421\n",
            "Epoch 5 Batch 516 Loss 4.9419\n",
            "Epoch 5 Batch 519 Loss 4.9407\n",
            "Epoch 5 Batch 522 Loss 4.9398\n",
            "Epoch 5 Batch 525 Loss 4.9394\n",
            "Epoch 5 Batch 528 Loss 4.9389\n",
            "Epoch 5 Batch 531 Loss 4.9380\n",
            "Epoch 5 Batch 534 Loss 4.9368\n",
            "Epoch 5 Batch 537 Loss 4.9364\n",
            "Epoch 5 Batch 540 Loss 4.9363\n",
            "Epoch 5 Batch 543 Loss 4.9357\n",
            "Epoch 5 Batch 546 Loss 4.9349\n",
            "Epoch 5 Batch 549 Loss 4.9344\n",
            "Epoch 5 Batch 552 Loss 4.9341\n",
            "Epoch 5 Batch 555 Loss 4.9328\n",
            "Epoch 5 Batch 558 Loss 4.9328\n",
            "Epoch 5 Batch 561 Loss 4.9319\n",
            "Epoch 5 Batch 564 Loss 4.9313\n",
            "Epoch 5 Batch 567 Loss 4.9302\n",
            "Epoch 5 Batch 570 Loss 4.9299\n",
            "Epoch 5 Batch 573 Loss 4.9289\n",
            "Epoch 5 Batch 576 Loss 4.9284\n",
            "Epoch 5 Batch 579 Loss 4.9275\n",
            "Epoch 5 Batch 582 Loss 4.9267\n",
            "Epoch 5 Batch 585 Loss 4.9260\n",
            "Epoch 5 Batch 588 Loss 4.9250\n",
            "Epoch 5 Batch 591 Loss 4.9246\n",
            "Epoch 5 Batch 594 Loss 4.9240\n",
            "Epoch 5 Batch 597 Loss 4.9234\n",
            "Epoch 5 Batch 600 Loss 4.9228\n",
            "Epoch 5 Batch 603 Loss 4.9220\n",
            "Epoch 5 Batch 606 Loss 4.9214\n",
            "Epoch 5 Batch 609 Loss 4.9208\n",
            "Epoch 5 Batch 612 Loss 4.9212\n",
            "Epoch 5 Batch 615 Loss 4.9203\n",
            "Epoch 5 Batch 618 Loss 4.9196\n",
            "Epoch 5 Batch 621 Loss 4.9188\n",
            "Epoch 5 Batch 624 Loss 4.9171\n",
            "Epoch 5 Batch 627 Loss 4.9167\n",
            "Epoch 5 Batch 630 Loss 4.9166\n",
            "Epoch 5 Batch 633 Loss 4.9161\n",
            "Epoch 5 Batch 636 Loss 4.9151\n",
            "Epoch 5 Batch 639 Loss 4.9140\n",
            "Epoch 5 Batch 642 Loss 4.9133\n",
            "Epoch 5 Batch 645 Loss 4.9119\n",
            "Epoch 5 Batch 648 Loss 4.9116\n",
            "Epoch 5 Batch 651 Loss 4.9112\n",
            "Epoch 5 Batch 654 Loss 4.9108\n",
            "Epoch 5 Batch 657 Loss 4.9101\n",
            "Epoch 5 Batch 660 Loss 4.9092\n",
            "Epoch 5 Batch 663 Loss 4.9090\n",
            "Epoch 5 Batch 666 Loss 4.9084\n",
            "Epoch 5 Batch 669 Loss 4.9076\n",
            "Epoch 5 Batch 672 Loss 4.9072\n",
            "Epoch 5 Batch 675 Loss 4.9068\n",
            "Epoch 5 Batch 678 Loss 4.9063\n",
            "Epoch 5 Batch 681 Loss 4.9060\n",
            "Epoch 5 Batch 684 Loss 4.9050\n",
            "Epoch 5 Batch 687 Loss 4.9049\n",
            "Epoch 5 Batch 690 Loss 4.9046\n",
            "Epoch 5 Batch 693 Loss 4.9042\n",
            "Epoch 5 Batch 696 Loss 4.9034\n",
            "Epoch 5 Batch 699 Loss 4.9028\n",
            "Epoch 5 Batch 702 Loss 4.9018\n",
            "Epoch 5 Batch 705 Loss 4.9012\n",
            "Epoch 5 Batch 708 Loss 4.9011\n",
            "Epoch 5 Batch 711 Loss 4.9005\n",
            "Epoch 5 Batch 714 Loss 4.9004\n",
            "Epoch 5 Batch 717 Loss 4.8995\n",
            "Epoch 5 Batch 720 Loss 4.8985\n",
            "Epoch 5 Batch 723 Loss 4.8985\n",
            "Epoch 5 Batch 726 Loss 4.8984\n",
            "Epoch 5 Batch 729 Loss 4.8984\n",
            "Epoch 5 Batch 732 Loss 4.8978\n",
            "Epoch 5 Batch 735 Loss 4.8974\n",
            "Epoch 5 Batch 738 Loss 4.8968\n",
            "Epoch 5 Batch 741 Loss 4.8967\n",
            "Epoch 5 Batch 744 Loss 4.8967\n",
            "Epoch 5 Batch 747 Loss 4.8960\n",
            "Epoch 5 Batch 750 Loss 4.8961\n",
            "Epoch 5 Batch 753 Loss 4.8955\n",
            "Epoch 5 Batch 756 Loss 4.8953\n",
            "Epoch 5 Batch 759 Loss 4.8944\n",
            "Epoch 5 Batch 762 Loss 4.8933\n",
            "Epoch 5 Batch 765 Loss 4.8929\n",
            "Epoch 5 Batch 768 Loss 4.8929\n",
            "Epoch 5 Batch 771 Loss 4.8928\n",
            "Epoch 5 Batch 774 Loss 4.8923\n",
            "Epoch 5 Batch 777 Loss 4.8922\n",
            "Epoch 5 Batch 780 Loss 4.8915\n",
            "Epoch 5 Batch 783 Loss 4.8906\n",
            "Epoch 5 Batch 786 Loss 4.8904\n",
            "Epoch 5 Batch 789 Loss 4.8899\n",
            "Epoch 5 Batch 792 Loss 4.8897\n",
            "Epoch 5 Batch 795 Loss 4.8895\n",
            "Epoch 5 Batch 798 Loss 4.8894\n",
            "Epoch 5 Batch 801 Loss 4.8889\n",
            "Epoch 5 Batch 804 Loss 4.8885\n",
            "Epoch 5 Batch 807 Loss 4.8882\n",
            "Epoch 5 Batch 810 Loss 4.8872\n",
            "Epoch 5 Batch 813 Loss 4.8860\n",
            "Epoch 5 Batch 816 Loss 4.8858\n",
            "Epoch 5 Batch 819 Loss 4.8851\n",
            "Epoch 5 Batch 822 Loss 4.8847\n",
            "Epoch 5 Batch 825 Loss 4.8845\n",
            "Epoch 5 Batch 828 Loss 4.8844\n",
            "Epoch 5 Batch 831 Loss 4.8842\n",
            "Epoch 5 Batch 834 Loss 4.8840\n",
            "Epoch 5 Batch 837 Loss 4.8837\n",
            "Epoch 5 Batch 840 Loss 4.8827\n",
            "Epoch 5 Batch 843 Loss 4.8821\n",
            "Epoch 5 Batch 846 Loss 4.8821\n",
            "Epoch 5 Batch 849 Loss 4.8820\n",
            "Epoch 5 Batch 852 Loss 4.8815\n",
            "Epoch 5 Batch 855 Loss 4.8814\n",
            "Epoch 5 Batch 858 Loss 4.8810\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 4.8801\n",
            "Time taken for 1 epoch: 330.8681490421295 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.6986\n",
            "Epoch 6 Batch 3 Loss 4.7025\n",
            "Epoch 6 Batch 6 Loss 4.6942\n",
            "Epoch 6 Batch 9 Loss 4.6807\n",
            "Epoch 6 Batch 12 Loss 4.6510\n",
            "Epoch 6 Batch 15 Loss 4.6498\n",
            "Epoch 6 Batch 18 Loss 4.6617\n",
            "Epoch 6 Batch 21 Loss 4.6552\n",
            "Epoch 6 Batch 24 Loss 4.6557\n",
            "Epoch 6 Batch 27 Loss 4.6369\n",
            "Epoch 6 Batch 30 Loss 4.6268\n",
            "Epoch 6 Batch 33 Loss 4.6193\n",
            "Epoch 6 Batch 36 Loss 4.6170\n",
            "Epoch 6 Batch 39 Loss 4.6136\n",
            "Epoch 6 Batch 42 Loss 4.6123\n",
            "Epoch 6 Batch 45 Loss 4.6225\n",
            "Epoch 6 Batch 48 Loss 4.6194\n",
            "Epoch 6 Batch 51 Loss 4.6148\n",
            "Epoch 6 Batch 54 Loss 4.6110\n",
            "Epoch 6 Batch 57 Loss 4.6135\n",
            "Epoch 6 Batch 60 Loss 4.6133\n",
            "Epoch 6 Batch 63 Loss 4.6112\n",
            "Epoch 6 Batch 66 Loss 4.6092\n",
            "Epoch 6 Batch 69 Loss 4.6081\n",
            "Epoch 6 Batch 72 Loss 4.6064\n",
            "Epoch 6 Batch 75 Loss 4.6037\n",
            "Epoch 6 Batch 78 Loss 4.6034\n",
            "Epoch 6 Batch 81 Loss 4.6035\n",
            "Epoch 6 Batch 84 Loss 4.6034\n",
            "Epoch 6 Batch 87 Loss 4.6026\n",
            "Epoch 6 Batch 90 Loss 4.6010\n",
            "Epoch 6 Batch 93 Loss 4.6003\n",
            "Epoch 6 Batch 96 Loss 4.6003\n",
            "Epoch 6 Batch 99 Loss 4.5986\n",
            "Epoch 6 Batch 102 Loss 4.5976\n",
            "Epoch 6 Batch 105 Loss 4.5961\n",
            "Epoch 6 Batch 108 Loss 4.5962\n",
            "Epoch 6 Batch 111 Loss 4.5961\n",
            "Epoch 6 Batch 114 Loss 4.5960\n",
            "Epoch 6 Batch 117 Loss 4.5950\n",
            "Epoch 6 Batch 120 Loss 4.5949\n",
            "Epoch 6 Batch 123 Loss 4.5957\n",
            "Epoch 6 Batch 126 Loss 4.5936\n",
            "Epoch 6 Batch 129 Loss 4.5930\n",
            "Epoch 6 Batch 132 Loss 4.5935\n",
            "Epoch 6 Batch 135 Loss 4.5950\n",
            "Epoch 6 Batch 138 Loss 4.5963\n",
            "Epoch 6 Batch 141 Loss 4.5951\n",
            "Epoch 6 Batch 144 Loss 4.5963\n",
            "Epoch 6 Batch 147 Loss 4.5968\n",
            "Epoch 6 Batch 150 Loss 4.5944\n",
            "Epoch 6 Batch 153 Loss 4.5928\n",
            "Epoch 6 Batch 156 Loss 4.5908\n",
            "Epoch 6 Batch 159 Loss 4.5887\n",
            "Epoch 6 Batch 162 Loss 4.5888\n",
            "Epoch 6 Batch 165 Loss 4.5875\n",
            "Epoch 6 Batch 168 Loss 4.5886\n",
            "Epoch 6 Batch 171 Loss 4.5896\n",
            "Epoch 6 Batch 174 Loss 4.5902\n",
            "Epoch 6 Batch 177 Loss 4.5893\n",
            "Epoch 6 Batch 180 Loss 4.5893\n",
            "Epoch 6 Batch 183 Loss 4.5915\n",
            "Epoch 6 Batch 186 Loss 4.5908\n",
            "Epoch 6 Batch 189 Loss 4.5915\n",
            "Epoch 6 Batch 192 Loss 4.5918\n",
            "Epoch 6 Batch 195 Loss 4.5909\n",
            "Epoch 6 Batch 198 Loss 4.5905\n",
            "Epoch 6 Batch 201 Loss 4.5904\n",
            "Epoch 6 Batch 204 Loss 4.5907\n",
            "Epoch 6 Batch 207 Loss 4.5913\n",
            "Epoch 6 Batch 210 Loss 4.5910\n",
            "Epoch 6 Batch 213 Loss 4.5923\n",
            "Epoch 6 Batch 216 Loss 4.5928\n",
            "Epoch 6 Batch 219 Loss 4.5944\n",
            "Epoch 6 Batch 222 Loss 4.5954\n",
            "Epoch 6 Batch 225 Loss 4.5958\n",
            "Epoch 6 Batch 228 Loss 4.5955\n",
            "Epoch 6 Batch 231 Loss 4.5947\n",
            "Epoch 6 Batch 234 Loss 4.5953\n",
            "Epoch 6 Batch 237 Loss 4.5970\n",
            "Epoch 6 Batch 240 Loss 4.5973\n",
            "Epoch 6 Batch 243 Loss 4.5974\n",
            "Epoch 6 Batch 246 Loss 4.5962\n",
            "Epoch 6 Batch 249 Loss 4.5953\n",
            "Epoch 6 Batch 252 Loss 4.5940\n",
            "Epoch 6 Batch 255 Loss 4.5928\n",
            "Epoch 6 Batch 258 Loss 4.5926\n",
            "Epoch 6 Batch 261 Loss 4.5911\n",
            "Epoch 6 Batch 264 Loss 4.5911\n",
            "Epoch 6 Batch 267 Loss 4.5922\n",
            "Epoch 6 Batch 270 Loss 4.5916\n",
            "Epoch 6 Batch 273 Loss 4.5910\n",
            "Epoch 6 Batch 276 Loss 4.5895\n",
            "Epoch 6 Batch 279 Loss 4.5916\n",
            "Epoch 6 Batch 282 Loss 4.5907\n",
            "Epoch 6 Batch 285 Loss 4.5905\n",
            "Epoch 6 Batch 288 Loss 4.5892\n",
            "Epoch 6 Batch 291 Loss 4.5878\n",
            "Epoch 6 Batch 294 Loss 4.5872\n",
            "Epoch 6 Batch 297 Loss 4.5868\n",
            "Epoch 6 Batch 300 Loss 4.5855\n",
            "Epoch 6 Batch 303 Loss 4.5860\n",
            "Epoch 6 Batch 306 Loss 4.5856\n",
            "Epoch 6 Batch 309 Loss 4.5854\n",
            "Epoch 6 Batch 312 Loss 4.5842\n",
            "Epoch 6 Batch 315 Loss 4.5844\n",
            "Epoch 6 Batch 318 Loss 4.5848\n",
            "Epoch 6 Batch 321 Loss 4.5844\n",
            "Epoch 6 Batch 324 Loss 4.5831\n",
            "Epoch 6 Batch 327 Loss 4.5816\n",
            "Epoch 6 Batch 330 Loss 4.5812\n",
            "Epoch 6 Batch 333 Loss 4.5806\n",
            "Epoch 6 Batch 336 Loss 4.5790\n",
            "Epoch 6 Batch 339 Loss 4.5776\n",
            "Epoch 6 Batch 342 Loss 4.5777\n",
            "Epoch 6 Batch 345 Loss 4.5767\n",
            "Epoch 6 Batch 348 Loss 4.5758\n",
            "Epoch 6 Batch 351 Loss 4.5750\n",
            "Epoch 6 Batch 354 Loss 4.5750\n",
            "Epoch 6 Batch 357 Loss 4.5757\n",
            "Epoch 6 Batch 360 Loss 4.5762\n",
            "Epoch 6 Batch 363 Loss 4.5757\n",
            "Epoch 6 Batch 366 Loss 4.5761\n",
            "Epoch 6 Batch 369 Loss 4.5749\n",
            "Epoch 6 Batch 372 Loss 4.5747\n",
            "Epoch 6 Batch 375 Loss 4.5747\n",
            "Epoch 6 Batch 378 Loss 4.5745\n",
            "Epoch 6 Batch 381 Loss 4.5751\n",
            "Epoch 6 Batch 384 Loss 4.5752\n",
            "Epoch 6 Batch 387 Loss 4.5753\n",
            "Epoch 6 Batch 390 Loss 4.5750\n",
            "Epoch 6 Batch 393 Loss 4.5745\n",
            "Epoch 6 Batch 396 Loss 4.5738\n",
            "Epoch 6 Batch 399 Loss 4.5723\n",
            "Epoch 6 Batch 402 Loss 4.5720\n",
            "Epoch 6 Batch 405 Loss 4.5709\n",
            "Epoch 6 Batch 408 Loss 4.5708\n",
            "Epoch 6 Batch 411 Loss 4.5697\n",
            "Epoch 6 Batch 414 Loss 4.5700\n",
            "Epoch 6 Batch 417 Loss 4.5690\n",
            "Epoch 6 Batch 420 Loss 4.5688\n",
            "Epoch 6 Batch 423 Loss 4.5681\n",
            "Epoch 6 Batch 426 Loss 4.5668\n",
            "Epoch 6 Batch 429 Loss 4.5650\n",
            "Epoch 6 Batch 432 Loss 4.5642\n",
            "Epoch 6 Batch 435 Loss 4.5629\n",
            "Epoch 6 Batch 438 Loss 4.5622\n",
            "Epoch 6 Batch 441 Loss 4.5617\n",
            "Epoch 6 Batch 444 Loss 4.5602\n",
            "Epoch 6 Batch 447 Loss 4.5602\n",
            "Epoch 6 Batch 450 Loss 4.5596\n",
            "Epoch 6 Batch 453 Loss 4.5590\n",
            "Epoch 6 Batch 456 Loss 4.5586\n",
            "Epoch 6 Batch 459 Loss 4.5585\n",
            "Epoch 6 Batch 462 Loss 4.5582\n",
            "Epoch 6 Batch 465 Loss 4.5571\n",
            "Epoch 6 Batch 468 Loss 4.5565\n",
            "Epoch 6 Batch 471 Loss 4.5562\n",
            "Epoch 6 Batch 474 Loss 4.5564\n",
            "Epoch 6 Batch 477 Loss 4.5554\n",
            "Epoch 6 Batch 480 Loss 4.5552\n",
            "Epoch 6 Batch 483 Loss 4.5551\n",
            "Epoch 6 Batch 486 Loss 4.5539\n",
            "Epoch 6 Batch 489 Loss 4.5539\n",
            "Epoch 6 Batch 492 Loss 4.5539\n",
            "Epoch 6 Batch 495 Loss 4.5533\n",
            "Epoch 6 Batch 498 Loss 4.5527\n",
            "Epoch 6 Batch 501 Loss 4.5535\n",
            "Epoch 6 Batch 504 Loss 4.5517\n",
            "Epoch 6 Batch 507 Loss 4.5510\n",
            "Epoch 6 Batch 510 Loss 4.5507\n",
            "Epoch 6 Batch 513 Loss 4.5498\n",
            "Epoch 6 Batch 516 Loss 4.5492\n",
            "Epoch 6 Batch 519 Loss 4.5497\n",
            "Epoch 6 Batch 522 Loss 4.5493\n",
            "Epoch 6 Batch 525 Loss 4.5493\n",
            "Epoch 6 Batch 528 Loss 4.5493\n",
            "Epoch 6 Batch 531 Loss 4.5490\n",
            "Epoch 6 Batch 534 Loss 4.5476\n",
            "Epoch 6 Batch 537 Loss 4.5470\n",
            "Epoch 6 Batch 540 Loss 4.5459\n",
            "Epoch 6 Batch 543 Loss 4.5449\n",
            "Epoch 6 Batch 546 Loss 4.5449\n",
            "Epoch 6 Batch 549 Loss 4.5440\n",
            "Epoch 6 Batch 552 Loss 4.5441\n",
            "Epoch 6 Batch 555 Loss 4.5432\n",
            "Epoch 6 Batch 558 Loss 4.5423\n",
            "Epoch 6 Batch 561 Loss 4.5416\n",
            "Epoch 6 Batch 564 Loss 4.5411\n",
            "Epoch 6 Batch 567 Loss 4.5403\n",
            "Epoch 6 Batch 570 Loss 4.5393\n",
            "Epoch 6 Batch 573 Loss 4.5393\n",
            "Epoch 6 Batch 576 Loss 4.5388\n",
            "Epoch 6 Batch 579 Loss 4.5386\n",
            "Epoch 6 Batch 582 Loss 4.5377\n",
            "Epoch 6 Batch 585 Loss 4.5366\n",
            "Epoch 6 Batch 588 Loss 4.5364\n",
            "Epoch 6 Batch 591 Loss 4.5356\n",
            "Epoch 6 Batch 594 Loss 4.5350\n",
            "Epoch 6 Batch 597 Loss 4.5340\n",
            "Epoch 6 Batch 600 Loss 4.5331\n",
            "Epoch 6 Batch 603 Loss 4.5327\n",
            "Epoch 6 Batch 606 Loss 4.5328\n",
            "Epoch 6 Batch 609 Loss 4.5326\n",
            "Epoch 6 Batch 612 Loss 4.5335\n",
            "Epoch 6 Batch 615 Loss 4.5332\n",
            "Epoch 6 Batch 618 Loss 4.5325\n",
            "Epoch 6 Batch 621 Loss 4.5320\n",
            "Epoch 6 Batch 624 Loss 4.5325\n",
            "Epoch 6 Batch 627 Loss 4.5313\n",
            "Epoch 6 Batch 630 Loss 4.5317\n",
            "Epoch 6 Batch 633 Loss 4.5310\n",
            "Epoch 6 Batch 636 Loss 4.5302\n",
            "Epoch 6 Batch 639 Loss 4.5296\n",
            "Epoch 6 Batch 642 Loss 4.5292\n",
            "Epoch 6 Batch 645 Loss 4.5292\n",
            "Epoch 6 Batch 648 Loss 4.5291\n",
            "Epoch 6 Batch 651 Loss 4.5286\n",
            "Epoch 6 Batch 654 Loss 4.5284\n",
            "Epoch 6 Batch 657 Loss 4.5282\n",
            "Epoch 6 Batch 660 Loss 4.5280\n",
            "Epoch 6 Batch 663 Loss 4.5276\n",
            "Epoch 6 Batch 666 Loss 4.5268\n",
            "Epoch 6 Batch 669 Loss 4.5269\n",
            "Epoch 6 Batch 672 Loss 4.5267\n",
            "Epoch 6 Batch 675 Loss 4.5262\n",
            "Epoch 6 Batch 678 Loss 4.5252\n",
            "Epoch 6 Batch 681 Loss 4.5252\n",
            "Epoch 6 Batch 684 Loss 4.5245\n",
            "Epoch 6 Batch 687 Loss 4.5239\n",
            "Epoch 6 Batch 690 Loss 4.5235\n",
            "Epoch 6 Batch 693 Loss 4.5226\n",
            "Epoch 6 Batch 696 Loss 4.5226\n",
            "Epoch 6 Batch 699 Loss 4.5221\n",
            "Epoch 6 Batch 702 Loss 4.5218\n",
            "Epoch 6 Batch 705 Loss 4.5218\n",
            "Epoch 6 Batch 708 Loss 4.5214\n",
            "Epoch 6 Batch 711 Loss 4.5210\n",
            "Epoch 6 Batch 714 Loss 4.5210\n",
            "Epoch 6 Batch 717 Loss 4.5209\n",
            "Epoch 6 Batch 720 Loss 4.5209\n",
            "Epoch 6 Batch 723 Loss 4.5207\n",
            "Epoch 6 Batch 726 Loss 4.5205\n",
            "Epoch 6 Batch 729 Loss 4.5200\n",
            "Epoch 6 Batch 732 Loss 4.5191\n",
            "Epoch 6 Batch 735 Loss 4.5192\n",
            "Epoch 6 Batch 738 Loss 4.5195\n",
            "Epoch 6 Batch 741 Loss 4.5194\n",
            "Epoch 6 Batch 744 Loss 4.5191\n",
            "Epoch 6 Batch 747 Loss 4.5190\n",
            "Epoch 6 Batch 750 Loss 4.5186\n",
            "Epoch 6 Batch 753 Loss 4.5178\n",
            "Epoch 6 Batch 756 Loss 4.5175\n",
            "Epoch 6 Batch 759 Loss 4.5173\n",
            "Epoch 6 Batch 762 Loss 4.5170\n",
            "Epoch 6 Batch 765 Loss 4.5171\n",
            "Epoch 6 Batch 768 Loss 4.5170\n",
            "Epoch 6 Batch 771 Loss 4.5160\n",
            "Epoch 6 Batch 774 Loss 4.5161\n",
            "Epoch 6 Batch 777 Loss 4.5162\n",
            "Epoch 6 Batch 780 Loss 4.5152\n",
            "Epoch 6 Batch 783 Loss 4.5147\n",
            "Epoch 6 Batch 786 Loss 4.5144\n",
            "Epoch 6 Batch 789 Loss 4.5140\n",
            "Epoch 6 Batch 792 Loss 4.5139\n",
            "Epoch 6 Batch 795 Loss 4.5137\n",
            "Epoch 6 Batch 798 Loss 4.5134\n",
            "Epoch 6 Batch 801 Loss 4.5131\n",
            "Epoch 6 Batch 804 Loss 4.5128\n",
            "Epoch 6 Batch 807 Loss 4.5119\n",
            "Epoch 6 Batch 810 Loss 4.5118\n",
            "Epoch 6 Batch 813 Loss 4.5117\n",
            "Epoch 6 Batch 816 Loss 4.5115\n",
            "Epoch 6 Batch 819 Loss 4.5110\n",
            "Epoch 6 Batch 822 Loss 4.5109\n",
            "Epoch 6 Batch 825 Loss 4.5105\n",
            "Epoch 6 Batch 828 Loss 4.5097\n",
            "Epoch 6 Batch 831 Loss 4.5094\n",
            "Epoch 6 Batch 834 Loss 4.5088\n",
            "Epoch 6 Batch 837 Loss 4.5087\n",
            "Epoch 6 Batch 840 Loss 4.5086\n",
            "Epoch 6 Batch 843 Loss 4.5078\n",
            "Epoch 6 Batch 846 Loss 4.5075\n",
            "Epoch 6 Batch 849 Loss 4.5070\n",
            "Epoch 6 Batch 852 Loss 4.5069\n",
            "Epoch 6 Batch 855 Loss 4.5066\n",
            "Epoch 6 Batch 858 Loss 4.5066\n",
            "Epoch 6 Loss 4.5069\n",
            "Time taken for 1 epoch: 329.5079200267792 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.1253\n",
            "Epoch 7 Batch 3 Loss 4.2448\n",
            "Epoch 7 Batch 6 Loss 4.3436\n",
            "Epoch 7 Batch 9 Loss 4.3533\n",
            "Epoch 7 Batch 12 Loss 4.3439\n",
            "Epoch 7 Batch 15 Loss 4.3585\n",
            "Epoch 7 Batch 18 Loss 4.3423\n",
            "Epoch 7 Batch 21 Loss 4.3374\n",
            "Epoch 7 Batch 24 Loss 4.3358\n",
            "Epoch 7 Batch 27 Loss 4.3293\n",
            "Epoch 7 Batch 30 Loss 4.3277\n",
            "Epoch 7 Batch 33 Loss 4.3217\n",
            "Epoch 7 Batch 36 Loss 4.3211\n",
            "Epoch 7 Batch 39 Loss 4.2990\n",
            "Epoch 7 Batch 42 Loss 4.2972\n",
            "Epoch 7 Batch 45 Loss 4.2947\n",
            "Epoch 7 Batch 48 Loss 4.2899\n",
            "Epoch 7 Batch 51 Loss 4.2844\n",
            "Epoch 7 Batch 54 Loss 4.2750\n",
            "Epoch 7 Batch 57 Loss 4.2674\n",
            "Epoch 7 Batch 60 Loss 4.2641\n",
            "Epoch 7 Batch 63 Loss 4.2599\n",
            "Epoch 7 Batch 66 Loss 4.2550\n",
            "Epoch 7 Batch 69 Loss 4.2502\n",
            "Epoch 7 Batch 72 Loss 4.2556\n",
            "Epoch 7 Batch 75 Loss 4.2506\n",
            "Epoch 7 Batch 78 Loss 4.2477\n",
            "Epoch 7 Batch 81 Loss 4.2505\n",
            "Epoch 7 Batch 84 Loss 4.2480\n",
            "Epoch 7 Batch 87 Loss 4.2479\n",
            "Epoch 7 Batch 90 Loss 4.2476\n",
            "Epoch 7 Batch 93 Loss 4.2484\n",
            "Epoch 7 Batch 96 Loss 4.2478\n",
            "Epoch 7 Batch 99 Loss 4.2483\n",
            "Epoch 7 Batch 102 Loss 4.2480\n",
            "Epoch 7 Batch 105 Loss 4.2498\n",
            "Epoch 7 Batch 108 Loss 4.2510\n",
            "Epoch 7 Batch 111 Loss 4.2529\n",
            "Epoch 7 Batch 114 Loss 4.2529\n",
            "Epoch 7 Batch 117 Loss 4.2517\n",
            "Epoch 7 Batch 120 Loss 4.2514\n",
            "Epoch 7 Batch 123 Loss 4.2508\n",
            "Epoch 7 Batch 126 Loss 4.2507\n",
            "Epoch 7 Batch 129 Loss 4.2491\n",
            "Epoch 7 Batch 132 Loss 4.2517\n",
            "Epoch 7 Batch 135 Loss 4.2517\n",
            "Epoch 7 Batch 138 Loss 4.2497\n",
            "Epoch 7 Batch 141 Loss 4.2484\n",
            "Epoch 7 Batch 144 Loss 4.2466\n",
            "Epoch 7 Batch 147 Loss 4.2454\n",
            "Epoch 7 Batch 150 Loss 4.2429\n",
            "Epoch 7 Batch 153 Loss 4.2434\n",
            "Epoch 7 Batch 156 Loss 4.2417\n",
            "Epoch 7 Batch 159 Loss 4.2416\n",
            "Epoch 7 Batch 162 Loss 4.2435\n",
            "Epoch 7 Batch 165 Loss 4.2422\n",
            "Epoch 7 Batch 168 Loss 4.2418\n",
            "Epoch 7 Batch 171 Loss 4.2409\n",
            "Epoch 7 Batch 174 Loss 4.2424\n",
            "Epoch 7 Batch 177 Loss 4.2423\n",
            "Epoch 7 Batch 180 Loss 4.2414\n",
            "Epoch 7 Batch 183 Loss 4.2430\n",
            "Epoch 7 Batch 186 Loss 4.2455\n",
            "Epoch 7 Batch 189 Loss 4.2460\n",
            "Epoch 7 Batch 192 Loss 4.2472\n",
            "Epoch 7 Batch 195 Loss 4.2483\n",
            "Epoch 7 Batch 198 Loss 4.2465\n",
            "Epoch 7 Batch 201 Loss 4.2468\n",
            "Epoch 7 Batch 204 Loss 4.2492\n",
            "Epoch 7 Batch 207 Loss 4.2498\n",
            "Epoch 7 Batch 210 Loss 4.2492\n",
            "Epoch 7 Batch 213 Loss 4.2485\n",
            "Epoch 7 Batch 216 Loss 4.2485\n",
            "Epoch 7 Batch 219 Loss 4.2498\n",
            "Epoch 7 Batch 222 Loss 4.2518\n",
            "Epoch 7 Batch 225 Loss 4.2499\n",
            "Epoch 7 Batch 228 Loss 4.2488\n",
            "Epoch 7 Batch 231 Loss 4.2507\n",
            "Epoch 7 Batch 234 Loss 4.2504\n",
            "Epoch 7 Batch 237 Loss 4.2519\n",
            "Epoch 7 Batch 240 Loss 4.2523\n",
            "Epoch 7 Batch 243 Loss 4.2515\n",
            "Epoch 7 Batch 246 Loss 4.2518\n",
            "Epoch 7 Batch 249 Loss 4.2503\n",
            "Epoch 7 Batch 252 Loss 4.2511\n",
            "Epoch 7 Batch 255 Loss 4.2521\n",
            "Epoch 7 Batch 258 Loss 4.2522\n",
            "Epoch 7 Batch 261 Loss 4.2537\n",
            "Epoch 7 Batch 264 Loss 4.2541\n",
            "Epoch 7 Batch 267 Loss 4.2543\n",
            "Epoch 7 Batch 270 Loss 4.2543\n",
            "Epoch 7 Batch 273 Loss 4.2542\n",
            "Epoch 7 Batch 276 Loss 4.2528\n",
            "Epoch 7 Batch 279 Loss 4.2529\n",
            "Epoch 7 Batch 282 Loss 4.2526\n",
            "Epoch 7 Batch 285 Loss 4.2510\n",
            "Epoch 7 Batch 288 Loss 4.2517\n",
            "Epoch 7 Batch 291 Loss 4.2511\n",
            "Epoch 7 Batch 294 Loss 4.2497\n",
            "Epoch 7 Batch 297 Loss 4.2483\n",
            "Epoch 7 Batch 300 Loss 4.2486\n",
            "Epoch 7 Batch 303 Loss 4.2488\n",
            "Epoch 7 Batch 306 Loss 4.2502\n",
            "Epoch 7 Batch 309 Loss 4.2496\n",
            "Epoch 7 Batch 312 Loss 4.2495\n",
            "Epoch 7 Batch 315 Loss 4.2485\n",
            "Epoch 7 Batch 318 Loss 4.2481\n",
            "Epoch 7 Batch 321 Loss 4.2472\n",
            "Epoch 7 Batch 324 Loss 4.2473\n",
            "Epoch 7 Batch 327 Loss 4.2458\n",
            "Epoch 7 Batch 330 Loss 4.2460\n",
            "Epoch 7 Batch 333 Loss 4.2460\n",
            "Epoch 7 Batch 336 Loss 4.2465\n",
            "Epoch 7 Batch 339 Loss 4.2463\n",
            "Epoch 7 Batch 342 Loss 4.2459\n",
            "Epoch 7 Batch 345 Loss 4.2456\n",
            "Epoch 7 Batch 348 Loss 4.2452\n",
            "Epoch 7 Batch 351 Loss 4.2461\n",
            "Epoch 7 Batch 354 Loss 4.2469\n",
            "Epoch 7 Batch 357 Loss 4.2478\n",
            "Epoch 7 Batch 360 Loss 4.2467\n",
            "Epoch 7 Batch 363 Loss 4.2461\n",
            "Epoch 7 Batch 366 Loss 4.2446\n",
            "Epoch 7 Batch 369 Loss 4.2446\n",
            "Epoch 7 Batch 372 Loss 4.2427\n",
            "Epoch 7 Batch 375 Loss 4.2409\n",
            "Epoch 7 Batch 378 Loss 4.2415\n",
            "Epoch 7 Batch 381 Loss 4.2402\n",
            "Epoch 7 Batch 384 Loss 4.2395\n",
            "Epoch 7 Batch 387 Loss 4.2395\n",
            "Epoch 7 Batch 390 Loss 4.2389\n",
            "Epoch 7 Batch 393 Loss 4.2384\n",
            "Epoch 7 Batch 396 Loss 4.2384\n",
            "Epoch 7 Batch 399 Loss 4.2382\n",
            "Epoch 7 Batch 402 Loss 4.2378\n",
            "Epoch 7 Batch 405 Loss 4.2379\n",
            "Epoch 7 Batch 408 Loss 4.2384\n",
            "Epoch 7 Batch 411 Loss 4.2384\n",
            "Epoch 7 Batch 414 Loss 4.2384\n",
            "Epoch 7 Batch 417 Loss 4.2378\n",
            "Epoch 7 Batch 420 Loss 4.2376\n",
            "Epoch 7 Batch 423 Loss 4.2376\n",
            "Epoch 7 Batch 426 Loss 4.2368\n",
            "Epoch 7 Batch 429 Loss 4.2365\n",
            "Epoch 7 Batch 432 Loss 4.2366\n",
            "Epoch 7 Batch 435 Loss 4.2357\n",
            "Epoch 7 Batch 438 Loss 4.2340\n",
            "Epoch 7 Batch 441 Loss 4.2344\n",
            "Epoch 7 Batch 444 Loss 4.2338\n",
            "Epoch 7 Batch 447 Loss 4.2323\n",
            "Epoch 7 Batch 450 Loss 4.2317\n",
            "Epoch 7 Batch 453 Loss 4.2314\n",
            "Epoch 7 Batch 456 Loss 4.2322\n",
            "Epoch 7 Batch 459 Loss 4.2316\n",
            "Epoch 7 Batch 462 Loss 4.2309\n",
            "Epoch 7 Batch 465 Loss 4.2312\n",
            "Epoch 7 Batch 468 Loss 4.2305\n",
            "Epoch 7 Batch 471 Loss 4.2302\n",
            "Epoch 7 Batch 474 Loss 4.2301\n",
            "Epoch 7 Batch 477 Loss 4.2302\n",
            "Epoch 7 Batch 480 Loss 4.2293\n",
            "Epoch 7 Batch 483 Loss 4.2286\n",
            "Epoch 7 Batch 486 Loss 4.2279\n",
            "Epoch 7 Batch 489 Loss 4.2275\n",
            "Epoch 7 Batch 492 Loss 4.2276\n",
            "Epoch 7 Batch 495 Loss 4.2268\n",
            "Epoch 7 Batch 498 Loss 4.2262\n",
            "Epoch 7 Batch 501 Loss 4.2254\n",
            "Epoch 7 Batch 504 Loss 4.2257\n",
            "Epoch 7 Batch 507 Loss 4.2255\n",
            "Epoch 7 Batch 510 Loss 4.2258\n",
            "Epoch 7 Batch 513 Loss 4.2251\n",
            "Epoch 7 Batch 516 Loss 4.2246\n",
            "Epoch 7 Batch 519 Loss 4.2238\n",
            "Epoch 7 Batch 522 Loss 4.2225\n",
            "Epoch 7 Batch 525 Loss 4.2223\n",
            "Epoch 7 Batch 528 Loss 4.2222\n",
            "Epoch 7 Batch 531 Loss 4.2224\n",
            "Epoch 7 Batch 534 Loss 4.2219\n",
            "Epoch 7 Batch 537 Loss 4.2216\n",
            "Epoch 7 Batch 540 Loss 4.2213\n",
            "Epoch 7 Batch 543 Loss 4.2216\n",
            "Epoch 7 Batch 546 Loss 4.2201\n",
            "Epoch 7 Batch 549 Loss 4.2193\n",
            "Epoch 7 Batch 552 Loss 4.2191\n",
            "Epoch 7 Batch 555 Loss 4.2194\n",
            "Epoch 7 Batch 558 Loss 4.2190\n",
            "Epoch 7 Batch 561 Loss 4.2181\n",
            "Epoch 7 Batch 564 Loss 4.2175\n",
            "Epoch 7 Batch 567 Loss 4.2178\n",
            "Epoch 7 Batch 570 Loss 4.2169\n",
            "Epoch 7 Batch 573 Loss 4.2159\n",
            "Epoch 7 Batch 576 Loss 4.2155\n",
            "Epoch 7 Batch 579 Loss 4.2156\n",
            "Epoch 7 Batch 582 Loss 4.2146\n",
            "Epoch 7 Batch 585 Loss 4.2143\n",
            "Epoch 7 Batch 588 Loss 4.2133\n",
            "Epoch 7 Batch 591 Loss 4.2122\n",
            "Epoch 7 Batch 594 Loss 4.2113\n",
            "Epoch 7 Batch 597 Loss 4.2107\n",
            "Epoch 7 Batch 600 Loss 4.2105\n",
            "Epoch 7 Batch 603 Loss 4.2101\n",
            "Epoch 7 Batch 606 Loss 4.2100\n",
            "Epoch 7 Batch 609 Loss 4.2098\n",
            "Epoch 7 Batch 612 Loss 4.2100\n",
            "Epoch 7 Batch 615 Loss 4.2101\n",
            "Epoch 7 Batch 618 Loss 4.2106\n",
            "Epoch 7 Batch 621 Loss 4.2099\n",
            "Epoch 7 Batch 624 Loss 4.2095\n",
            "Epoch 7 Batch 627 Loss 4.2093\n",
            "Epoch 7 Batch 630 Loss 4.2098\n",
            "Epoch 7 Batch 633 Loss 4.2089\n",
            "Epoch 7 Batch 636 Loss 4.2084\n",
            "Epoch 7 Batch 639 Loss 4.2081\n",
            "Epoch 7 Batch 642 Loss 4.2069\n",
            "Epoch 7 Batch 645 Loss 4.2066\n",
            "Epoch 7 Batch 648 Loss 4.2071\n",
            "Epoch 7 Batch 651 Loss 4.2070\n",
            "Epoch 7 Batch 654 Loss 4.2070\n",
            "Epoch 7 Batch 657 Loss 4.2065\n",
            "Epoch 7 Batch 660 Loss 4.2062\n",
            "Epoch 7 Batch 663 Loss 4.2056\n",
            "Epoch 7 Batch 666 Loss 4.2058\n",
            "Epoch 7 Batch 669 Loss 4.2051\n",
            "Epoch 7 Batch 672 Loss 4.2053\n",
            "Epoch 7 Batch 675 Loss 4.2048\n",
            "Epoch 7 Batch 678 Loss 4.2047\n",
            "Epoch 7 Batch 681 Loss 4.2038\n",
            "Epoch 7 Batch 684 Loss 4.2031\n",
            "Epoch 7 Batch 687 Loss 4.2026\n",
            "Epoch 7 Batch 690 Loss 4.2026\n",
            "Epoch 7 Batch 693 Loss 4.2023\n",
            "Epoch 7 Batch 696 Loss 4.2022\n",
            "Epoch 7 Batch 699 Loss 4.2017\n",
            "Epoch 7 Batch 702 Loss 4.2017\n",
            "Epoch 7 Batch 705 Loss 4.2015\n",
            "Epoch 7 Batch 708 Loss 4.2014\n",
            "Epoch 7 Batch 711 Loss 4.2004\n",
            "Epoch 7 Batch 714 Loss 4.2008\n",
            "Epoch 7 Batch 717 Loss 4.2001\n",
            "Epoch 7 Batch 720 Loss 4.1996\n",
            "Epoch 7 Batch 723 Loss 4.1997\n",
            "Epoch 7 Batch 726 Loss 4.1998\n",
            "Epoch 7 Batch 729 Loss 4.1992\n",
            "Epoch 7 Batch 732 Loss 4.1991\n",
            "Epoch 7 Batch 735 Loss 4.1998\n",
            "Epoch 7 Batch 738 Loss 4.1998\n",
            "Epoch 7 Batch 741 Loss 4.1994\n",
            "Epoch 7 Batch 744 Loss 4.1991\n",
            "Epoch 7 Batch 747 Loss 4.1992\n",
            "Epoch 7 Batch 750 Loss 4.1994\n",
            "Epoch 7 Batch 753 Loss 4.1992\n",
            "Epoch 7 Batch 756 Loss 4.1994\n",
            "Epoch 7 Batch 759 Loss 4.1992\n",
            "Epoch 7 Batch 762 Loss 4.1986\n",
            "Epoch 7 Batch 765 Loss 4.1986\n",
            "Epoch 7 Batch 768 Loss 4.1985\n",
            "Epoch 7 Batch 771 Loss 4.1984\n",
            "Epoch 7 Batch 774 Loss 4.1988\n",
            "Epoch 7 Batch 777 Loss 4.1986\n",
            "Epoch 7 Batch 780 Loss 4.1987\n",
            "Epoch 7 Batch 783 Loss 4.1987\n",
            "Epoch 7 Batch 786 Loss 4.1989\n",
            "Epoch 7 Batch 789 Loss 4.1987\n",
            "Epoch 7 Batch 792 Loss 4.1983\n",
            "Epoch 7 Batch 795 Loss 4.1978\n",
            "Epoch 7 Batch 798 Loss 4.1974\n",
            "Epoch 7 Batch 801 Loss 4.1972\n",
            "Epoch 7 Batch 804 Loss 4.1973\n",
            "Epoch 7 Batch 807 Loss 4.1974\n",
            "Epoch 7 Batch 810 Loss 4.1976\n",
            "Epoch 7 Batch 813 Loss 4.1976\n",
            "Epoch 7 Batch 816 Loss 4.1978\n",
            "Epoch 7 Batch 819 Loss 4.1973\n",
            "Epoch 7 Batch 822 Loss 4.1970\n",
            "Epoch 7 Batch 825 Loss 4.1971\n",
            "Epoch 7 Batch 828 Loss 4.1974\n",
            "Epoch 7 Batch 831 Loss 4.1974\n",
            "Epoch 7 Batch 834 Loss 4.1977\n",
            "Epoch 7 Batch 837 Loss 4.1979\n",
            "Epoch 7 Batch 840 Loss 4.1978\n",
            "Epoch 7 Batch 843 Loss 4.1973\n",
            "Epoch 7 Batch 846 Loss 4.1975\n",
            "Epoch 7 Batch 849 Loss 4.1975\n",
            "Epoch 7 Batch 852 Loss 4.1973\n",
            "Epoch 7 Batch 855 Loss 4.1968\n",
            "Epoch 7 Batch 858 Loss 4.1971\n",
            "Epoch 7 Loss 4.1971\n",
            "Time taken for 1 epoch: 329.76534605026245 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.0490\n",
            "Epoch 8 Batch 3 Loss 4.0211\n",
            "Epoch 8 Batch 6 Loss 4.0301\n",
            "Epoch 8 Batch 9 Loss 4.0038\n",
            "Epoch 8 Batch 12 Loss 3.9981\n",
            "Epoch 8 Batch 15 Loss 3.9919\n",
            "Epoch 8 Batch 18 Loss 3.9914\n",
            "Epoch 8 Batch 21 Loss 3.9845\n",
            "Epoch 8 Batch 24 Loss 3.9907\n",
            "Epoch 8 Batch 27 Loss 3.9857\n",
            "Epoch 8 Batch 30 Loss 3.9868\n",
            "Epoch 8 Batch 33 Loss 3.9900\n",
            "Epoch 8 Batch 36 Loss 3.9907\n",
            "Epoch 8 Batch 39 Loss 3.9800\n",
            "Epoch 8 Batch 42 Loss 3.9814\n",
            "Epoch 8 Batch 45 Loss 3.9866\n",
            "Epoch 8 Batch 48 Loss 3.9777\n",
            "Epoch 8 Batch 51 Loss 3.9786\n",
            "Epoch 8 Batch 54 Loss 3.9796\n",
            "Epoch 8 Batch 57 Loss 3.9735\n",
            "Epoch 8 Batch 60 Loss 3.9632\n",
            "Epoch 8 Batch 63 Loss 3.9658\n",
            "Epoch 8 Batch 66 Loss 3.9680\n",
            "Epoch 8 Batch 69 Loss 3.9705\n",
            "Epoch 8 Batch 72 Loss 3.9679\n",
            "Epoch 8 Batch 75 Loss 3.9678\n",
            "Epoch 8 Batch 78 Loss 3.9666\n",
            "Epoch 8 Batch 81 Loss 3.9676\n",
            "Epoch 8 Batch 84 Loss 3.9655\n",
            "Epoch 8 Batch 87 Loss 3.9667\n",
            "Epoch 8 Batch 90 Loss 3.9667\n",
            "Epoch 8 Batch 93 Loss 3.9627\n",
            "Epoch 8 Batch 96 Loss 3.9639\n",
            "Epoch 8 Batch 99 Loss 3.9657\n",
            "Epoch 8 Batch 102 Loss 3.9652\n",
            "Epoch 8 Batch 105 Loss 3.9630\n",
            "Epoch 8 Batch 108 Loss 3.9617\n",
            "Epoch 8 Batch 111 Loss 3.9623\n",
            "Epoch 8 Batch 114 Loss 3.9666\n",
            "Epoch 8 Batch 117 Loss 3.9642\n",
            "Epoch 8 Batch 120 Loss 3.9655\n",
            "Epoch 8 Batch 123 Loss 3.9616\n",
            "Epoch 8 Batch 126 Loss 3.9614\n",
            "Epoch 8 Batch 129 Loss 3.9642\n",
            "Epoch 8 Batch 132 Loss 3.9609\n",
            "Epoch 8 Batch 135 Loss 3.9648\n",
            "Epoch 8 Batch 138 Loss 3.9647\n",
            "Epoch 8 Batch 141 Loss 3.9646\n",
            "Epoch 8 Batch 144 Loss 3.9650\n",
            "Epoch 8 Batch 147 Loss 3.9637\n",
            "Epoch 8 Batch 150 Loss 3.9627\n",
            "Epoch 8 Batch 153 Loss 3.9626\n",
            "Epoch 8 Batch 156 Loss 3.9628\n",
            "Epoch 8 Batch 159 Loss 3.9636\n",
            "Epoch 8 Batch 162 Loss 3.9646\n",
            "Epoch 8 Batch 165 Loss 3.9644\n",
            "Epoch 8 Batch 168 Loss 3.9661\n",
            "Epoch 8 Batch 171 Loss 3.9657\n",
            "Epoch 8 Batch 174 Loss 3.9670\n",
            "Epoch 8 Batch 177 Loss 3.9680\n",
            "Epoch 8 Batch 180 Loss 3.9697\n",
            "Epoch 8 Batch 183 Loss 3.9681\n",
            "Epoch 8 Batch 186 Loss 3.9699\n",
            "Epoch 8 Batch 189 Loss 3.9713\n",
            "Epoch 8 Batch 192 Loss 3.9737\n",
            "Epoch 8 Batch 195 Loss 3.9743\n",
            "Epoch 8 Batch 198 Loss 3.9733\n",
            "Epoch 8 Batch 201 Loss 3.9713\n",
            "Epoch 8 Batch 204 Loss 3.9710\n",
            "Epoch 8 Batch 207 Loss 3.9714\n",
            "Epoch 8 Batch 210 Loss 3.9719\n",
            "Epoch 8 Batch 213 Loss 3.9713\n",
            "Epoch 8 Batch 216 Loss 3.9714\n",
            "Epoch 8 Batch 219 Loss 3.9700\n",
            "Epoch 8 Batch 222 Loss 3.9704\n",
            "Epoch 8 Batch 225 Loss 3.9707\n",
            "Epoch 8 Batch 228 Loss 3.9711\n",
            "Epoch 8 Batch 231 Loss 3.9731\n",
            "Epoch 8 Batch 234 Loss 3.9729\n",
            "Epoch 8 Batch 237 Loss 3.9751\n",
            "Epoch 8 Batch 240 Loss 3.9750\n",
            "Epoch 8 Batch 243 Loss 3.9749\n",
            "Epoch 8 Batch 246 Loss 3.9738\n",
            "Epoch 8 Batch 249 Loss 3.9734\n",
            "Epoch 8 Batch 252 Loss 3.9731\n",
            "Epoch 8 Batch 255 Loss 3.9725\n",
            "Epoch 8 Batch 258 Loss 3.9704\n",
            "Epoch 8 Batch 261 Loss 3.9705\n",
            "Epoch 8 Batch 264 Loss 3.9710\n",
            "Epoch 8 Batch 267 Loss 3.9709\n",
            "Epoch 8 Batch 270 Loss 3.9711\n",
            "Epoch 8 Batch 273 Loss 3.9700\n",
            "Epoch 8 Batch 276 Loss 3.9701\n",
            "Epoch 8 Batch 279 Loss 3.9701\n",
            "Epoch 8 Batch 282 Loss 3.9711\n",
            "Epoch 8 Batch 285 Loss 3.9714\n",
            "Epoch 8 Batch 288 Loss 3.9720\n",
            "Epoch 8 Batch 291 Loss 3.9711\n",
            "Epoch 8 Batch 294 Loss 3.9712\n",
            "Epoch 8 Batch 297 Loss 3.9727\n",
            "Epoch 8 Batch 300 Loss 3.9716\n",
            "Epoch 8 Batch 303 Loss 3.9701\n",
            "Epoch 8 Batch 306 Loss 3.9705\n",
            "Epoch 8 Batch 309 Loss 3.9702\n",
            "Epoch 8 Batch 312 Loss 3.9708\n",
            "Epoch 8 Batch 315 Loss 3.9704\n",
            "Epoch 8 Batch 318 Loss 3.9698\n",
            "Epoch 8 Batch 321 Loss 3.9706\n",
            "Epoch 8 Batch 324 Loss 3.9699\n",
            "Epoch 8 Batch 327 Loss 3.9695\n",
            "Epoch 8 Batch 330 Loss 3.9689\n",
            "Epoch 8 Batch 333 Loss 3.9680\n",
            "Epoch 8 Batch 336 Loss 3.9677\n",
            "Epoch 8 Batch 339 Loss 3.9677\n",
            "Epoch 8 Batch 342 Loss 3.9678\n",
            "Epoch 8 Batch 345 Loss 3.9676\n",
            "Epoch 8 Batch 348 Loss 3.9684\n",
            "Epoch 8 Batch 351 Loss 3.9682\n",
            "Epoch 8 Batch 354 Loss 3.9666\n",
            "Epoch 8 Batch 357 Loss 3.9657\n",
            "Epoch 8 Batch 360 Loss 3.9657\n",
            "Epoch 8 Batch 363 Loss 3.9641\n",
            "Epoch 8 Batch 366 Loss 3.9647\n",
            "Epoch 8 Batch 369 Loss 3.9647\n",
            "Epoch 8 Batch 372 Loss 3.9648\n",
            "Epoch 8 Batch 375 Loss 3.9659\n",
            "Epoch 8 Batch 378 Loss 3.9668\n",
            "Epoch 8 Batch 381 Loss 3.9662\n",
            "Epoch 8 Batch 384 Loss 3.9659\n",
            "Epoch 8 Batch 387 Loss 3.9670\n",
            "Epoch 8 Batch 390 Loss 3.9669\n",
            "Epoch 8 Batch 393 Loss 3.9659\n",
            "Epoch 8 Batch 396 Loss 3.9658\n",
            "Epoch 8 Batch 399 Loss 3.9667\n",
            "Epoch 8 Batch 402 Loss 3.9675\n",
            "Epoch 8 Batch 405 Loss 3.9666\n",
            "Epoch 8 Batch 408 Loss 3.9669\n",
            "Epoch 8 Batch 411 Loss 3.9673\n",
            "Epoch 8 Batch 414 Loss 3.9667\n",
            "Epoch 8 Batch 417 Loss 3.9666\n",
            "Epoch 8 Batch 420 Loss 3.9657\n",
            "Epoch 8 Batch 423 Loss 3.9645\n",
            "Epoch 8 Batch 426 Loss 3.9649\n",
            "Epoch 8 Batch 429 Loss 3.9659\n",
            "Epoch 8 Batch 432 Loss 3.9659\n",
            "Epoch 8 Batch 435 Loss 3.9660\n",
            "Epoch 8 Batch 438 Loss 3.9650\n",
            "Epoch 8 Batch 441 Loss 3.9652\n",
            "Epoch 8 Batch 444 Loss 3.9653\n",
            "Epoch 8 Batch 447 Loss 3.9647\n",
            "Epoch 8 Batch 450 Loss 3.9638\n",
            "Epoch 8 Batch 453 Loss 3.9630\n",
            "Epoch 8 Batch 456 Loss 3.9633\n",
            "Epoch 8 Batch 459 Loss 3.9633\n",
            "Epoch 8 Batch 462 Loss 3.9631\n",
            "Epoch 8 Batch 465 Loss 3.9624\n",
            "Epoch 8 Batch 468 Loss 3.9613\n",
            "Epoch 8 Batch 471 Loss 3.9604\n",
            "Epoch 8 Batch 474 Loss 3.9608\n",
            "Epoch 8 Batch 477 Loss 3.9603\n",
            "Epoch 8 Batch 480 Loss 3.9595\n",
            "Epoch 8 Batch 483 Loss 3.9594\n",
            "Epoch 8 Batch 486 Loss 3.9596\n",
            "Epoch 8 Batch 489 Loss 3.9586\n",
            "Epoch 8 Batch 492 Loss 3.9586\n",
            "Epoch 8 Batch 495 Loss 3.9589\n",
            "Epoch 8 Batch 498 Loss 3.9581\n",
            "Epoch 8 Batch 501 Loss 3.9585\n",
            "Epoch 8 Batch 504 Loss 3.9582\n",
            "Epoch 8 Batch 507 Loss 3.9575\n",
            "Epoch 8 Batch 510 Loss 3.9565\n",
            "Epoch 8 Batch 513 Loss 3.9559\n",
            "Epoch 8 Batch 516 Loss 3.9561\n",
            "Epoch 8 Batch 519 Loss 3.9545\n",
            "Epoch 8 Batch 522 Loss 3.9541\n",
            "Epoch 8 Batch 525 Loss 3.9532\n",
            "Epoch 8 Batch 528 Loss 3.9526\n",
            "Epoch 8 Batch 531 Loss 3.9524\n",
            "Epoch 8 Batch 534 Loss 3.9520\n",
            "Epoch 8 Batch 537 Loss 3.9519\n",
            "Epoch 8 Batch 540 Loss 3.9519\n",
            "Epoch 8 Batch 543 Loss 3.9514\n",
            "Epoch 8 Batch 546 Loss 3.9509\n",
            "Epoch 8 Batch 549 Loss 3.9514\n",
            "Epoch 8 Batch 552 Loss 3.9512\n",
            "Epoch 8 Batch 555 Loss 3.9511\n",
            "Epoch 8 Batch 558 Loss 3.9506\n",
            "Epoch 8 Batch 561 Loss 3.9503\n",
            "Epoch 8 Batch 564 Loss 3.9499\n",
            "Epoch 8 Batch 567 Loss 3.9502\n",
            "Epoch 8 Batch 570 Loss 3.9494\n",
            "Epoch 8 Batch 573 Loss 3.9489\n",
            "Epoch 8 Batch 576 Loss 3.9477\n",
            "Epoch 8 Batch 579 Loss 3.9478\n",
            "Epoch 8 Batch 582 Loss 3.9481\n",
            "Epoch 8 Batch 585 Loss 3.9481\n",
            "Epoch 8 Batch 588 Loss 3.9478\n",
            "Epoch 8 Batch 591 Loss 3.9470\n",
            "Epoch 8 Batch 594 Loss 3.9465\n",
            "Epoch 8 Batch 597 Loss 3.9464\n",
            "Epoch 8 Batch 600 Loss 3.9459\n",
            "Epoch 8 Batch 603 Loss 3.9463\n",
            "Epoch 8 Batch 606 Loss 3.9459\n",
            "Epoch 8 Batch 609 Loss 3.9449\n",
            "Epoch 8 Batch 612 Loss 3.9451\n",
            "Epoch 8 Batch 615 Loss 3.9445\n",
            "Epoch 8 Batch 618 Loss 3.9447\n",
            "Epoch 8 Batch 621 Loss 3.9444\n",
            "Epoch 8 Batch 624 Loss 3.9444\n",
            "Epoch 8 Batch 627 Loss 3.9440\n",
            "Epoch 8 Batch 630 Loss 3.9440\n",
            "Epoch 8 Batch 633 Loss 3.9436\n",
            "Epoch 8 Batch 636 Loss 3.9434\n",
            "Epoch 8 Batch 639 Loss 3.9437\n",
            "Epoch 8 Batch 642 Loss 3.9432\n",
            "Epoch 8 Batch 645 Loss 3.9428\n",
            "Epoch 8 Batch 648 Loss 3.9423\n",
            "Epoch 8 Batch 651 Loss 3.9417\n",
            "Epoch 8 Batch 654 Loss 3.9420\n",
            "Epoch 8 Batch 657 Loss 3.9420\n",
            "Epoch 8 Batch 660 Loss 3.9421\n",
            "Epoch 8 Batch 663 Loss 3.9424\n",
            "Epoch 8 Batch 666 Loss 3.9432\n",
            "Epoch 8 Batch 669 Loss 3.9425\n",
            "Epoch 8 Batch 672 Loss 3.9423\n",
            "Epoch 8 Batch 675 Loss 3.9428\n",
            "Epoch 8 Batch 678 Loss 3.9429\n",
            "Epoch 8 Batch 681 Loss 3.9427\n",
            "Epoch 8 Batch 684 Loss 3.9429\n",
            "Epoch 8 Batch 687 Loss 3.9430\n",
            "Epoch 8 Batch 690 Loss 3.9433\n",
            "Epoch 8 Batch 693 Loss 3.9434\n",
            "Epoch 8 Batch 696 Loss 3.9436\n",
            "Epoch 8 Batch 699 Loss 3.9435\n",
            "Epoch 8 Batch 702 Loss 3.9439\n",
            "Epoch 8 Batch 705 Loss 3.9436\n",
            "Epoch 8 Batch 708 Loss 3.9439\n",
            "Epoch 8 Batch 711 Loss 3.9443\n",
            "Epoch 8 Batch 714 Loss 3.9445\n",
            "Epoch 8 Batch 717 Loss 3.9441\n",
            "Epoch 8 Batch 720 Loss 3.9447\n",
            "Epoch 8 Batch 723 Loss 3.9452\n",
            "Epoch 8 Batch 726 Loss 3.9448\n",
            "Epoch 8 Batch 729 Loss 3.9449\n",
            "Epoch 8 Batch 732 Loss 3.9451\n",
            "Epoch 8 Batch 735 Loss 3.9458\n",
            "Epoch 8 Batch 738 Loss 3.9460\n",
            "Epoch 8 Batch 741 Loss 3.9461\n",
            "Epoch 8 Batch 744 Loss 3.9462\n",
            "Epoch 8 Batch 747 Loss 3.9464\n",
            "Epoch 8 Batch 750 Loss 3.9468\n",
            "Epoch 8 Batch 753 Loss 3.9466\n",
            "Epoch 8 Batch 756 Loss 3.9468\n",
            "Epoch 8 Batch 759 Loss 3.9475\n",
            "Epoch 8 Batch 762 Loss 3.9475\n",
            "Epoch 8 Batch 765 Loss 3.9481\n",
            "Epoch 8 Batch 768 Loss 3.9482\n",
            "Epoch 8 Batch 771 Loss 3.9487\n",
            "Epoch 8 Batch 774 Loss 3.9485\n",
            "Epoch 8 Batch 777 Loss 3.9483\n",
            "Epoch 8 Batch 780 Loss 3.9484\n",
            "Epoch 8 Batch 783 Loss 3.9489\n",
            "Epoch 8 Batch 786 Loss 3.9495\n",
            "Epoch 8 Batch 789 Loss 3.9493\n",
            "Epoch 8 Batch 792 Loss 3.9492\n",
            "Epoch 8 Batch 795 Loss 3.9489\n",
            "Epoch 8 Batch 798 Loss 3.9488\n",
            "Epoch 8 Batch 801 Loss 3.9489\n",
            "Epoch 8 Batch 804 Loss 3.9483\n",
            "Epoch 8 Batch 807 Loss 3.9483\n",
            "Epoch 8 Batch 810 Loss 3.9486\n",
            "Epoch 8 Batch 813 Loss 3.9489\n",
            "Epoch 8 Batch 816 Loss 3.9493\n",
            "Epoch 8 Batch 819 Loss 3.9492\n",
            "Epoch 8 Batch 822 Loss 3.9492\n",
            "Epoch 8 Batch 825 Loss 3.9494\n",
            "Epoch 8 Batch 828 Loss 3.9495\n",
            "Epoch 8 Batch 831 Loss 3.9495\n",
            "Epoch 8 Batch 834 Loss 3.9494\n",
            "Epoch 8 Batch 837 Loss 3.9492\n",
            "Epoch 8 Batch 840 Loss 3.9496\n",
            "Epoch 8 Batch 843 Loss 3.9497\n",
            "Epoch 8 Batch 846 Loss 3.9494\n",
            "Epoch 8 Batch 849 Loss 3.9495\n",
            "Epoch 8 Batch 852 Loss 3.9491\n",
            "Epoch 8 Batch 855 Loss 3.9489\n",
            "Epoch 8 Batch 858 Loss 3.9490\n",
            "Epoch 8 Loss 3.9492\n",
            "Time taken for 1 epoch: 329.56173038482666 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.6624\n",
            "Epoch 9 Batch 3 Loss 3.8145\n",
            "Epoch 9 Batch 6 Loss 3.7802\n",
            "Epoch 9 Batch 9 Loss 3.7504\n",
            "Epoch 9 Batch 12 Loss 3.7608\n",
            "Epoch 9 Batch 15 Loss 3.7636\n",
            "Epoch 9 Batch 18 Loss 3.7505\n",
            "Epoch 9 Batch 21 Loss 3.7410\n",
            "Epoch 9 Batch 24 Loss 3.7622\n",
            "Epoch 9 Batch 27 Loss 3.7552\n",
            "Epoch 9 Batch 30 Loss 3.7582\n",
            "Epoch 9 Batch 33 Loss 3.7524\n",
            "Epoch 9 Batch 36 Loss 3.7471\n",
            "Epoch 9 Batch 39 Loss 3.7470\n",
            "Epoch 9 Batch 42 Loss 3.7449\n",
            "Epoch 9 Batch 45 Loss 3.7500\n",
            "Epoch 9 Batch 48 Loss 3.7503\n",
            "Epoch 9 Batch 51 Loss 3.7500\n",
            "Epoch 9 Batch 54 Loss 3.7545\n",
            "Epoch 9 Batch 57 Loss 3.7666\n",
            "Epoch 9 Batch 60 Loss 3.7649\n",
            "Epoch 9 Batch 63 Loss 3.7650\n",
            "Epoch 9 Batch 66 Loss 3.7686\n",
            "Epoch 9 Batch 69 Loss 3.7720\n",
            "Epoch 9 Batch 72 Loss 3.7650\n",
            "Epoch 9 Batch 75 Loss 3.7675\n",
            "Epoch 9 Batch 78 Loss 3.7696\n",
            "Epoch 9 Batch 81 Loss 3.7667\n",
            "Epoch 9 Batch 84 Loss 3.7679\n",
            "Epoch 9 Batch 87 Loss 3.7673\n",
            "Epoch 9 Batch 90 Loss 3.7656\n",
            "Epoch 9 Batch 93 Loss 3.7631\n",
            "Epoch 9 Batch 96 Loss 3.7645\n",
            "Epoch 9 Batch 99 Loss 3.7623\n",
            "Epoch 9 Batch 102 Loss 3.7626\n",
            "Epoch 9 Batch 105 Loss 3.7619\n",
            "Epoch 9 Batch 108 Loss 3.7619\n",
            "Epoch 9 Batch 111 Loss 3.7596\n",
            "Epoch 9 Batch 114 Loss 3.7610\n",
            "Epoch 9 Batch 117 Loss 3.7614\n",
            "Epoch 9 Batch 120 Loss 3.7605\n",
            "Epoch 9 Batch 123 Loss 3.7607\n",
            "Epoch 9 Batch 126 Loss 3.7582\n",
            "Epoch 9 Batch 129 Loss 3.7585\n",
            "Epoch 9 Batch 132 Loss 3.7598\n",
            "Epoch 9 Batch 135 Loss 3.7567\n",
            "Epoch 9 Batch 138 Loss 3.7545\n",
            "Epoch 9 Batch 141 Loss 3.7533\n",
            "Epoch 9 Batch 144 Loss 3.7544\n",
            "Epoch 9 Batch 147 Loss 3.7565\n",
            "Epoch 9 Batch 150 Loss 3.7555\n",
            "Epoch 9 Batch 153 Loss 3.7554\n",
            "Epoch 9 Batch 156 Loss 3.7526\n",
            "Epoch 9 Batch 159 Loss 3.7548\n",
            "Epoch 9 Batch 162 Loss 3.7552\n",
            "Epoch 9 Batch 165 Loss 3.7556\n",
            "Epoch 9 Batch 168 Loss 3.7563\n",
            "Epoch 9 Batch 171 Loss 3.7557\n",
            "Epoch 9 Batch 174 Loss 3.7548\n",
            "Epoch 9 Batch 177 Loss 3.7563\n",
            "Epoch 9 Batch 180 Loss 3.7574\n",
            "Epoch 9 Batch 183 Loss 3.7567\n",
            "Epoch 9 Batch 186 Loss 3.7564\n",
            "Epoch 9 Batch 189 Loss 3.7556\n",
            "Epoch 9 Batch 192 Loss 3.7555\n",
            "Epoch 9 Batch 195 Loss 3.7542\n",
            "Epoch 9 Batch 198 Loss 3.7529\n",
            "Epoch 9 Batch 201 Loss 3.7543\n",
            "Epoch 9 Batch 204 Loss 3.7570\n",
            "Epoch 9 Batch 207 Loss 3.7571\n",
            "Epoch 9 Batch 210 Loss 3.7580\n",
            "Epoch 9 Batch 213 Loss 3.7575\n",
            "Epoch 9 Batch 216 Loss 3.7589\n",
            "Epoch 9 Batch 219 Loss 3.7575\n",
            "Epoch 9 Batch 222 Loss 3.7570\n",
            "Epoch 9 Batch 225 Loss 3.7588\n",
            "Epoch 9 Batch 228 Loss 3.7580\n",
            "Epoch 9 Batch 231 Loss 3.7579\n",
            "Epoch 9 Batch 234 Loss 3.7566\n",
            "Epoch 9 Batch 237 Loss 3.7566\n",
            "Epoch 9 Batch 240 Loss 3.7566\n",
            "Epoch 9 Batch 243 Loss 3.7566\n",
            "Epoch 9 Batch 246 Loss 3.7580\n",
            "Epoch 9 Batch 249 Loss 3.7581\n",
            "Epoch 9 Batch 252 Loss 3.7573\n",
            "Epoch 9 Batch 255 Loss 3.7578\n",
            "Epoch 9 Batch 258 Loss 3.7599\n",
            "Epoch 9 Batch 261 Loss 3.7591\n",
            "Epoch 9 Batch 264 Loss 3.7589\n",
            "Epoch 9 Batch 267 Loss 3.7613\n",
            "Epoch 9 Batch 270 Loss 3.7598\n",
            "Epoch 9 Batch 273 Loss 3.7602\n",
            "Epoch 9 Batch 276 Loss 3.7596\n",
            "Epoch 9 Batch 279 Loss 3.7597\n",
            "Epoch 9 Batch 282 Loss 3.7600\n",
            "Epoch 9 Batch 285 Loss 3.7595\n",
            "Epoch 9 Batch 288 Loss 3.7587\n",
            "Epoch 9 Batch 291 Loss 3.7586\n",
            "Epoch 9 Batch 294 Loss 3.7582\n",
            "Epoch 9 Batch 297 Loss 3.7566\n",
            "Epoch 9 Batch 300 Loss 3.7573\n",
            "Epoch 9 Batch 303 Loss 3.7570\n",
            "Epoch 9 Batch 306 Loss 3.7563\n",
            "Epoch 9 Batch 309 Loss 3.7560\n",
            "Epoch 9 Batch 312 Loss 3.7562\n",
            "Epoch 9 Batch 315 Loss 3.7579\n",
            "Epoch 9 Batch 318 Loss 3.7572\n",
            "Epoch 9 Batch 321 Loss 3.7565\n",
            "Epoch 9 Batch 324 Loss 3.7568\n",
            "Epoch 9 Batch 327 Loss 3.7563\n",
            "Epoch 9 Batch 330 Loss 3.7561\n",
            "Epoch 9 Batch 333 Loss 3.7562\n",
            "Epoch 9 Batch 336 Loss 3.7560\n",
            "Epoch 9 Batch 339 Loss 3.7567\n",
            "Epoch 9 Batch 342 Loss 3.7568\n",
            "Epoch 9 Batch 345 Loss 3.7566\n",
            "Epoch 9 Batch 348 Loss 3.7564\n",
            "Epoch 9 Batch 351 Loss 3.7567\n",
            "Epoch 9 Batch 354 Loss 3.7566\n",
            "Epoch 9 Batch 357 Loss 3.7561\n",
            "Epoch 9 Batch 360 Loss 3.7568\n",
            "Epoch 9 Batch 363 Loss 3.7570\n",
            "Epoch 9 Batch 366 Loss 3.7569\n",
            "Epoch 9 Batch 369 Loss 3.7558\n",
            "Epoch 9 Batch 372 Loss 3.7562\n",
            "Epoch 9 Batch 375 Loss 3.7570\n",
            "Epoch 9 Batch 378 Loss 3.7559\n",
            "Epoch 9 Batch 381 Loss 3.7561\n",
            "Epoch 9 Batch 384 Loss 3.7568\n",
            "Epoch 9 Batch 387 Loss 3.7563\n",
            "Epoch 9 Batch 390 Loss 3.7561\n",
            "Epoch 9 Batch 393 Loss 3.7544\n",
            "Epoch 9 Batch 396 Loss 3.7538\n",
            "Epoch 9 Batch 399 Loss 3.7528\n",
            "Epoch 9 Batch 402 Loss 3.7521\n",
            "Epoch 9 Batch 405 Loss 3.7525\n",
            "Epoch 9 Batch 408 Loss 3.7518\n",
            "Epoch 9 Batch 411 Loss 3.7517\n",
            "Epoch 9 Batch 414 Loss 3.7511\n",
            "Epoch 9 Batch 417 Loss 3.7512\n",
            "Epoch 9 Batch 420 Loss 3.7506\n",
            "Epoch 9 Batch 423 Loss 3.7506\n",
            "Epoch 9 Batch 426 Loss 3.7502\n",
            "Epoch 9 Batch 429 Loss 3.7504\n",
            "Epoch 9 Batch 432 Loss 3.7496\n",
            "Epoch 9 Batch 435 Loss 3.7501\n",
            "Epoch 9 Batch 438 Loss 3.7491\n",
            "Epoch 9 Batch 441 Loss 3.7493\n",
            "Epoch 9 Batch 444 Loss 3.7493\n",
            "Epoch 9 Batch 447 Loss 3.7487\n",
            "Epoch 9 Batch 450 Loss 3.7489\n",
            "Epoch 9 Batch 453 Loss 3.7488\n",
            "Epoch 9 Batch 456 Loss 3.7482\n",
            "Epoch 9 Batch 459 Loss 3.7479\n",
            "Epoch 9 Batch 462 Loss 3.7483\n",
            "Epoch 9 Batch 465 Loss 3.7482\n",
            "Epoch 9 Batch 468 Loss 3.7476\n",
            "Epoch 9 Batch 471 Loss 3.7471\n",
            "Epoch 9 Batch 474 Loss 3.7473\n",
            "Epoch 9 Batch 477 Loss 3.7465\n",
            "Epoch 9 Batch 480 Loss 3.7458\n",
            "Epoch 9 Batch 483 Loss 3.7460\n",
            "Epoch 9 Batch 486 Loss 3.7452\n",
            "Epoch 9 Batch 489 Loss 3.7459\n",
            "Epoch 9 Batch 492 Loss 3.7460\n",
            "Epoch 9 Batch 495 Loss 3.7466\n",
            "Epoch 9 Batch 498 Loss 3.7465\n",
            "Epoch 9 Batch 501 Loss 3.7464\n",
            "Epoch 9 Batch 504 Loss 3.7477\n",
            "Epoch 9 Batch 507 Loss 3.7472\n",
            "Epoch 9 Batch 510 Loss 3.7475\n",
            "Epoch 9 Batch 513 Loss 3.7472\n",
            "Epoch 9 Batch 516 Loss 3.7472\n",
            "Epoch 9 Batch 519 Loss 3.7476\n",
            "Epoch 9 Batch 522 Loss 3.7473\n",
            "Epoch 9 Batch 525 Loss 3.7465\n",
            "Epoch 9 Batch 528 Loss 3.7466\n",
            "Epoch 9 Batch 531 Loss 3.7473\n",
            "Epoch 9 Batch 534 Loss 3.7470\n",
            "Epoch 9 Batch 537 Loss 3.7463\n",
            "Epoch 9 Batch 540 Loss 3.7466\n",
            "Epoch 9 Batch 543 Loss 3.7460\n",
            "Epoch 9 Batch 546 Loss 3.7460\n",
            "Epoch 9 Batch 549 Loss 3.7452\n",
            "Epoch 9 Batch 552 Loss 3.7455\n",
            "Epoch 9 Batch 555 Loss 3.7449\n",
            "Epoch 9 Batch 558 Loss 3.7452\n",
            "Epoch 9 Batch 561 Loss 3.7447\n",
            "Epoch 9 Batch 564 Loss 3.7444\n",
            "Epoch 9 Batch 567 Loss 3.7444\n",
            "Epoch 9 Batch 570 Loss 3.7432\n",
            "Epoch 9 Batch 573 Loss 3.7430\n",
            "Epoch 9 Batch 576 Loss 3.7427\n",
            "Epoch 9 Batch 579 Loss 3.7427\n",
            "Epoch 9 Batch 582 Loss 3.7424\n",
            "Epoch 9 Batch 585 Loss 3.7419\n",
            "Epoch 9 Batch 588 Loss 3.7419\n",
            "Epoch 9 Batch 591 Loss 3.7426\n",
            "Epoch 9 Batch 594 Loss 3.7430\n",
            "Epoch 9 Batch 597 Loss 3.7438\n",
            "Epoch 9 Batch 600 Loss 3.7435\n",
            "Epoch 9 Batch 603 Loss 3.7431\n",
            "Epoch 9 Batch 606 Loss 3.7428\n",
            "Epoch 9 Batch 609 Loss 3.7428\n",
            "Epoch 9 Batch 612 Loss 3.7428\n",
            "Epoch 9 Batch 615 Loss 3.7417\n",
            "Epoch 9 Batch 618 Loss 3.7417\n",
            "Epoch 9 Batch 621 Loss 3.7420\n",
            "Epoch 9 Batch 624 Loss 3.7423\n",
            "Epoch 9 Batch 627 Loss 3.7422\n",
            "Epoch 9 Batch 630 Loss 3.7424\n",
            "Epoch 9 Batch 633 Loss 3.7431\n",
            "Epoch 9 Batch 636 Loss 3.7436\n",
            "Epoch 9 Batch 639 Loss 3.7432\n",
            "Epoch 9 Batch 642 Loss 3.7433\n",
            "Epoch 9 Batch 645 Loss 3.7429\n",
            "Epoch 9 Batch 648 Loss 3.7425\n",
            "Epoch 9 Batch 651 Loss 3.7426\n",
            "Epoch 9 Batch 654 Loss 3.7434\n",
            "Epoch 9 Batch 657 Loss 3.7435\n",
            "Epoch 9 Batch 660 Loss 3.7440\n",
            "Epoch 9 Batch 663 Loss 3.7440\n",
            "Epoch 9 Batch 666 Loss 3.7438\n",
            "Epoch 9 Batch 669 Loss 3.7442\n",
            "Epoch 9 Batch 672 Loss 3.7447\n",
            "Epoch 9 Batch 675 Loss 3.7444\n",
            "Epoch 9 Batch 678 Loss 3.7440\n",
            "Epoch 9 Batch 681 Loss 3.7445\n",
            "Epoch 9 Batch 684 Loss 3.7439\n",
            "Epoch 9 Batch 687 Loss 3.7439\n",
            "Epoch 9 Batch 690 Loss 3.7441\n",
            "Epoch 9 Batch 693 Loss 3.7441\n",
            "Epoch 9 Batch 696 Loss 3.7441\n",
            "Epoch 9 Batch 699 Loss 3.7440\n",
            "Epoch 9 Batch 702 Loss 3.7441\n",
            "Epoch 9 Batch 705 Loss 3.7441\n",
            "Epoch 9 Batch 708 Loss 3.7448\n",
            "Epoch 9 Batch 711 Loss 3.7445\n",
            "Epoch 9 Batch 714 Loss 3.7448\n",
            "Epoch 9 Batch 717 Loss 3.7448\n",
            "Epoch 9 Batch 720 Loss 3.7448\n",
            "Epoch 9 Batch 723 Loss 3.7447\n",
            "Epoch 9 Batch 726 Loss 3.7452\n",
            "Epoch 9 Batch 729 Loss 3.7454\n",
            "Epoch 9 Batch 732 Loss 3.7454\n",
            "Epoch 9 Batch 735 Loss 3.7455\n",
            "Epoch 9 Batch 738 Loss 3.7455\n",
            "Epoch 9 Batch 741 Loss 3.7457\n",
            "Epoch 9 Batch 744 Loss 3.7462\n",
            "Epoch 9 Batch 747 Loss 3.7465\n",
            "Epoch 9 Batch 750 Loss 3.7467\n",
            "Epoch 9 Batch 753 Loss 3.7474\n",
            "Epoch 9 Batch 756 Loss 3.7476\n",
            "Epoch 9 Batch 759 Loss 3.7481\n",
            "Epoch 9 Batch 762 Loss 3.7476\n",
            "Epoch 9 Batch 765 Loss 3.7477\n",
            "Epoch 9 Batch 768 Loss 3.7481\n",
            "Epoch 9 Batch 771 Loss 3.7489\n",
            "Epoch 9 Batch 774 Loss 3.7489\n",
            "Epoch 9 Batch 777 Loss 3.7483\n",
            "Epoch 9 Batch 780 Loss 3.7484\n",
            "Epoch 9 Batch 783 Loss 3.7482\n",
            "Epoch 9 Batch 786 Loss 3.7479\n",
            "Epoch 9 Batch 789 Loss 3.7481\n",
            "Epoch 9 Batch 792 Loss 3.7487\n",
            "Epoch 9 Batch 795 Loss 3.7486\n",
            "Epoch 9 Batch 798 Loss 3.7487\n",
            "Epoch 9 Batch 801 Loss 3.7489\n",
            "Epoch 9 Batch 804 Loss 3.7493\n",
            "Epoch 9 Batch 807 Loss 3.7498\n",
            "Epoch 9 Batch 810 Loss 3.7498\n",
            "Epoch 9 Batch 813 Loss 3.7500\n",
            "Epoch 9 Batch 816 Loss 3.7503\n",
            "Epoch 9 Batch 819 Loss 3.7510\n",
            "Epoch 9 Batch 822 Loss 3.7519\n",
            "Epoch 9 Batch 825 Loss 3.7523\n",
            "Epoch 9 Batch 828 Loss 3.7525\n",
            "Epoch 9 Batch 831 Loss 3.7522\n",
            "Epoch 9 Batch 834 Loss 3.7524\n",
            "Epoch 9 Batch 837 Loss 3.7526\n",
            "Epoch 9 Batch 840 Loss 3.7527\n",
            "Epoch 9 Batch 843 Loss 3.7524\n",
            "Epoch 9 Batch 846 Loss 3.7528\n",
            "Epoch 9 Batch 849 Loss 3.7534\n",
            "Epoch 9 Batch 852 Loss 3.7543\n",
            "Epoch 9 Batch 855 Loss 3.7541\n",
            "Epoch 9 Batch 858 Loss 3.7538\n",
            "Epoch 9 Loss 3.7536\n",
            "Time taken for 1 epoch: 329.3090033531189 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.7658\n",
            "Epoch 10 Batch 3 Loss 3.7264\n",
            "Epoch 10 Batch 6 Loss 3.7002\n",
            "Epoch 10 Batch 9 Loss 3.7256\n",
            "Epoch 10 Batch 12 Loss 3.7201\n",
            "Epoch 10 Batch 15 Loss 3.6719\n",
            "Epoch 10 Batch 18 Loss 3.6468\n",
            "Epoch 10 Batch 21 Loss 3.6367\n",
            "Epoch 10 Batch 24 Loss 3.6356\n",
            "Epoch 10 Batch 27 Loss 3.6358\n",
            "Epoch 10 Batch 30 Loss 3.6199\n",
            "Epoch 10 Batch 33 Loss 3.6157\n",
            "Epoch 10 Batch 36 Loss 3.6065\n",
            "Epoch 10 Batch 39 Loss 3.6062\n",
            "Epoch 10 Batch 42 Loss 3.6113\n",
            "Epoch 10 Batch 45 Loss 3.6090\n",
            "Epoch 10 Batch 48 Loss 3.6088\n",
            "Epoch 10 Batch 51 Loss 3.6040\n",
            "Epoch 10 Batch 54 Loss 3.6056\n",
            "Epoch 10 Batch 57 Loss 3.6042\n",
            "Epoch 10 Batch 60 Loss 3.6017\n",
            "Epoch 10 Batch 63 Loss 3.5992\n",
            "Epoch 10 Batch 66 Loss 3.5991\n",
            "Epoch 10 Batch 69 Loss 3.6005\n",
            "Epoch 10 Batch 72 Loss 3.6001\n",
            "Epoch 10 Batch 75 Loss 3.6021\n",
            "Epoch 10 Batch 78 Loss 3.6013\n",
            "Epoch 10 Batch 81 Loss 3.6013\n",
            "Epoch 10 Batch 84 Loss 3.5986\n",
            "Epoch 10 Batch 87 Loss 3.5995\n",
            "Epoch 10 Batch 90 Loss 3.6029\n",
            "Epoch 10 Batch 93 Loss 3.6056\n",
            "Epoch 10 Batch 96 Loss 3.6078\n",
            "Epoch 10 Batch 99 Loss 3.6083\n",
            "Epoch 10 Batch 102 Loss 3.6062\n",
            "Epoch 10 Batch 105 Loss 3.6069\n",
            "Epoch 10 Batch 108 Loss 3.6056\n",
            "Epoch 10 Batch 111 Loss 3.6066\n",
            "Epoch 10 Batch 114 Loss 3.6078\n",
            "Epoch 10 Batch 117 Loss 3.6068\n",
            "Epoch 10 Batch 120 Loss 3.6069\n",
            "Epoch 10 Batch 123 Loss 3.6046\n",
            "Epoch 10 Batch 126 Loss 3.6000\n",
            "Epoch 10 Batch 129 Loss 3.5994\n",
            "Epoch 10 Batch 132 Loss 3.5981\n",
            "Epoch 10 Batch 135 Loss 3.6012\n",
            "Epoch 10 Batch 138 Loss 3.6003\n",
            "Epoch 10 Batch 141 Loss 3.6000\n",
            "Epoch 10 Batch 144 Loss 3.5995\n",
            "Epoch 10 Batch 147 Loss 3.6005\n",
            "Epoch 10 Batch 150 Loss 3.6017\n",
            "Epoch 10 Batch 153 Loss 3.5986\n",
            "Epoch 10 Batch 156 Loss 3.5998\n",
            "Epoch 10 Batch 159 Loss 3.5996\n",
            "Epoch 10 Batch 162 Loss 3.5998\n",
            "Epoch 10 Batch 165 Loss 3.5981\n",
            "Epoch 10 Batch 168 Loss 3.5970\n",
            "Epoch 10 Batch 171 Loss 3.5965\n",
            "Epoch 10 Batch 174 Loss 3.5983\n",
            "Epoch 10 Batch 177 Loss 3.5973\n",
            "Epoch 10 Batch 180 Loss 3.5983\n",
            "Epoch 10 Batch 183 Loss 3.5974\n",
            "Epoch 10 Batch 186 Loss 3.5973\n",
            "Epoch 10 Batch 189 Loss 3.5968\n",
            "Epoch 10 Batch 192 Loss 3.5957\n",
            "Epoch 10 Batch 195 Loss 3.5967\n",
            "Epoch 10 Batch 198 Loss 3.5966\n",
            "Epoch 10 Batch 201 Loss 3.5962\n",
            "Epoch 10 Batch 204 Loss 3.5970\n",
            "Epoch 10 Batch 207 Loss 3.5968\n",
            "Epoch 10 Batch 210 Loss 3.5964\n",
            "Epoch 10 Batch 213 Loss 3.5958\n",
            "Epoch 10 Batch 216 Loss 3.5960\n",
            "Epoch 10 Batch 219 Loss 3.5956\n",
            "Epoch 10 Batch 222 Loss 3.5962\n",
            "Epoch 10 Batch 225 Loss 3.5946\n",
            "Epoch 10 Batch 228 Loss 3.5957\n",
            "Epoch 10 Batch 231 Loss 3.5959\n",
            "Epoch 10 Batch 234 Loss 3.5942\n",
            "Epoch 10 Batch 237 Loss 3.5943\n",
            "Epoch 10 Batch 240 Loss 3.5940\n",
            "Epoch 10 Batch 243 Loss 3.5935\n",
            "Epoch 10 Batch 246 Loss 3.5925\n",
            "Epoch 10 Batch 249 Loss 3.5909\n",
            "Epoch 10 Batch 252 Loss 3.5905\n",
            "Epoch 10 Batch 255 Loss 3.5912\n",
            "Epoch 10 Batch 258 Loss 3.5910\n",
            "Epoch 10 Batch 261 Loss 3.5906\n",
            "Epoch 10 Batch 264 Loss 3.5907\n",
            "Epoch 10 Batch 267 Loss 3.5911\n",
            "Epoch 10 Batch 270 Loss 3.5916\n",
            "Epoch 10 Batch 273 Loss 3.5907\n",
            "Epoch 10 Batch 276 Loss 3.5905\n",
            "Epoch 10 Batch 279 Loss 3.5910\n",
            "Epoch 10 Batch 282 Loss 3.5905\n",
            "Epoch 10 Batch 285 Loss 3.5914\n",
            "Epoch 10 Batch 288 Loss 3.5914\n",
            "Epoch 10 Batch 291 Loss 3.5931\n",
            "Epoch 10 Batch 294 Loss 3.5933\n",
            "Epoch 10 Batch 297 Loss 3.5927\n",
            "Epoch 10 Batch 300 Loss 3.5940\n",
            "Epoch 10 Batch 303 Loss 3.5933\n",
            "Epoch 10 Batch 306 Loss 3.5942\n",
            "Epoch 10 Batch 309 Loss 3.5935\n",
            "Epoch 10 Batch 312 Loss 3.5928\n",
            "Epoch 10 Batch 315 Loss 3.5929\n",
            "Epoch 10 Batch 318 Loss 3.5929\n",
            "Epoch 10 Batch 321 Loss 3.5927\n",
            "Epoch 10 Batch 324 Loss 3.5913\n",
            "Epoch 10 Batch 327 Loss 3.5910\n",
            "Epoch 10 Batch 330 Loss 3.5915\n",
            "Epoch 10 Batch 333 Loss 3.5912\n",
            "Epoch 10 Batch 336 Loss 3.5902\n",
            "Epoch 10 Batch 339 Loss 3.5898\n",
            "Epoch 10 Batch 342 Loss 3.5898\n",
            "Epoch 10 Batch 345 Loss 3.5902\n",
            "Epoch 10 Batch 348 Loss 3.5911\n",
            "Epoch 10 Batch 351 Loss 3.5925\n",
            "Epoch 10 Batch 354 Loss 3.5923\n",
            "Epoch 10 Batch 357 Loss 3.5919\n",
            "Epoch 10 Batch 360 Loss 3.5926\n",
            "Epoch 10 Batch 363 Loss 3.5918\n",
            "Epoch 10 Batch 366 Loss 3.5909\n",
            "Epoch 10 Batch 369 Loss 3.5903\n",
            "Epoch 10 Batch 372 Loss 3.5912\n",
            "Epoch 10 Batch 375 Loss 3.5910\n",
            "Epoch 10 Batch 378 Loss 3.5905\n",
            "Epoch 10 Batch 381 Loss 3.5908\n",
            "Epoch 10 Batch 384 Loss 3.5907\n",
            "Epoch 10 Batch 387 Loss 3.5913\n",
            "Epoch 10 Batch 390 Loss 3.5921\n",
            "Epoch 10 Batch 393 Loss 3.5918\n",
            "Epoch 10 Batch 396 Loss 3.5912\n",
            "Epoch 10 Batch 399 Loss 3.5898\n",
            "Epoch 10 Batch 402 Loss 3.5900\n",
            "Epoch 10 Batch 405 Loss 3.5906\n",
            "Epoch 10 Batch 408 Loss 3.5906\n",
            "Epoch 10 Batch 411 Loss 3.5896\n",
            "Epoch 10 Batch 414 Loss 3.5884\n",
            "Epoch 10 Batch 417 Loss 3.5885\n",
            "Epoch 10 Batch 420 Loss 3.5883\n",
            "Epoch 10 Batch 423 Loss 3.5885\n",
            "Epoch 10 Batch 426 Loss 3.5890\n",
            "Epoch 10 Batch 429 Loss 3.5884\n",
            "Epoch 10 Batch 432 Loss 3.5877\n",
            "Epoch 10 Batch 435 Loss 3.5871\n",
            "Epoch 10 Batch 438 Loss 3.5870\n",
            "Epoch 10 Batch 441 Loss 3.5870\n",
            "Epoch 10 Batch 444 Loss 3.5870\n",
            "Epoch 10 Batch 447 Loss 3.5870\n",
            "Epoch 10 Batch 450 Loss 3.5871\n",
            "Epoch 10 Batch 453 Loss 3.5873\n",
            "Epoch 10 Batch 456 Loss 3.5875\n",
            "Epoch 10 Batch 459 Loss 3.5875\n",
            "Epoch 10 Batch 462 Loss 3.5868\n",
            "Epoch 10 Batch 465 Loss 3.5855\n",
            "Epoch 10 Batch 468 Loss 3.5855\n",
            "Epoch 10 Batch 471 Loss 3.5849\n",
            "Epoch 10 Batch 474 Loss 3.5852\n",
            "Epoch 10 Batch 477 Loss 3.5852\n",
            "Epoch 10 Batch 480 Loss 3.5848\n",
            "Epoch 10 Batch 483 Loss 3.5849\n",
            "Epoch 10 Batch 486 Loss 3.5850\n",
            "Epoch 10 Batch 489 Loss 3.5841\n",
            "Epoch 10 Batch 492 Loss 3.5839\n",
            "Epoch 10 Batch 495 Loss 3.5834\n",
            "Epoch 10 Batch 498 Loss 3.5832\n",
            "Epoch 10 Batch 501 Loss 3.5839\n",
            "Epoch 10 Batch 504 Loss 3.5834\n",
            "Epoch 10 Batch 507 Loss 3.5834\n",
            "Epoch 10 Batch 510 Loss 3.5831\n",
            "Epoch 10 Batch 513 Loss 3.5828\n",
            "Epoch 10 Batch 516 Loss 3.5821\n",
            "Epoch 10 Batch 519 Loss 3.5820\n",
            "Epoch 10 Batch 522 Loss 3.5818\n",
            "Epoch 10 Batch 525 Loss 3.5822\n",
            "Epoch 10 Batch 528 Loss 3.5820\n",
            "Epoch 10 Batch 531 Loss 3.5820\n",
            "Epoch 10 Batch 534 Loss 3.5825\n",
            "Epoch 10 Batch 537 Loss 3.5822\n",
            "Epoch 10 Batch 540 Loss 3.5825\n",
            "Epoch 10 Batch 543 Loss 3.5828\n",
            "Epoch 10 Batch 546 Loss 3.5821\n",
            "Epoch 10 Batch 549 Loss 3.5822\n",
            "Epoch 10 Batch 552 Loss 3.5823\n",
            "Epoch 10 Batch 555 Loss 3.5817\n",
            "Epoch 10 Batch 558 Loss 3.5821\n",
            "Epoch 10 Batch 561 Loss 3.5818\n",
            "Epoch 10 Batch 564 Loss 3.5820\n",
            "Epoch 10 Batch 567 Loss 3.5817\n",
            "Epoch 10 Batch 570 Loss 3.5818\n",
            "Epoch 10 Batch 573 Loss 3.5817\n",
            "Epoch 10 Batch 576 Loss 3.5811\n",
            "Epoch 10 Batch 579 Loss 3.5805\n",
            "Epoch 10 Batch 582 Loss 3.5809\n",
            "Epoch 10 Batch 585 Loss 3.5797\n",
            "Epoch 10 Batch 588 Loss 3.5798\n",
            "Epoch 10 Batch 591 Loss 3.5796\n",
            "Epoch 10 Batch 594 Loss 3.5791\n",
            "Epoch 10 Batch 597 Loss 3.5796\n",
            "Epoch 10 Batch 600 Loss 3.5802\n",
            "Epoch 10 Batch 603 Loss 3.5807\n",
            "Epoch 10 Batch 606 Loss 3.5812\n",
            "Epoch 10 Batch 609 Loss 3.5811\n",
            "Epoch 10 Batch 612 Loss 3.5820\n",
            "Epoch 10 Batch 615 Loss 3.5823\n",
            "Epoch 10 Batch 618 Loss 3.5813\n",
            "Epoch 10 Batch 621 Loss 3.5817\n",
            "Epoch 10 Batch 624 Loss 3.5812\n",
            "Epoch 10 Batch 627 Loss 3.5814\n",
            "Epoch 10 Batch 630 Loss 3.5818\n",
            "Epoch 10 Batch 633 Loss 3.5815\n",
            "Epoch 10 Batch 636 Loss 3.5816\n",
            "Epoch 10 Batch 639 Loss 3.5815\n",
            "Epoch 10 Batch 642 Loss 3.5815\n",
            "Epoch 10 Batch 645 Loss 3.5815\n",
            "Epoch 10 Batch 648 Loss 3.5817\n",
            "Epoch 10 Batch 651 Loss 3.5814\n",
            "Epoch 10 Batch 654 Loss 3.5809\n",
            "Epoch 10 Batch 657 Loss 3.5809\n",
            "Epoch 10 Batch 660 Loss 3.5812\n",
            "Epoch 10 Batch 663 Loss 3.5814\n",
            "Epoch 10 Batch 666 Loss 3.5814\n",
            "Epoch 10 Batch 669 Loss 3.5810\n",
            "Epoch 10 Batch 672 Loss 3.5810\n",
            "Epoch 10 Batch 675 Loss 3.5810\n",
            "Epoch 10 Batch 678 Loss 3.5812\n",
            "Epoch 10 Batch 681 Loss 3.5809\n",
            "Epoch 10 Batch 684 Loss 3.5809\n",
            "Epoch 10 Batch 687 Loss 3.5807\n",
            "Epoch 10 Batch 690 Loss 3.5811\n",
            "Epoch 10 Batch 693 Loss 3.5812\n",
            "Epoch 10 Batch 696 Loss 3.5810\n",
            "Epoch 10 Batch 699 Loss 3.5813\n",
            "Epoch 10 Batch 702 Loss 3.5813\n",
            "Epoch 10 Batch 705 Loss 3.5821\n",
            "Epoch 10 Batch 708 Loss 3.5823\n",
            "Epoch 10 Batch 711 Loss 3.5823\n",
            "Epoch 10 Batch 714 Loss 3.5828\n",
            "Epoch 10 Batch 717 Loss 3.5823\n",
            "Epoch 10 Batch 720 Loss 3.5829\n",
            "Epoch 10 Batch 723 Loss 3.5830\n",
            "Epoch 10 Batch 726 Loss 3.5832\n",
            "Epoch 10 Batch 729 Loss 3.5827\n",
            "Epoch 10 Batch 732 Loss 3.5825\n",
            "Epoch 10 Batch 735 Loss 3.5829\n",
            "Epoch 10 Batch 738 Loss 3.5827\n",
            "Epoch 10 Batch 741 Loss 3.5825\n",
            "Epoch 10 Batch 744 Loss 3.5826\n",
            "Epoch 10 Batch 747 Loss 3.5826\n",
            "Epoch 10 Batch 750 Loss 3.5827\n",
            "Epoch 10 Batch 753 Loss 3.5830\n",
            "Epoch 10 Batch 756 Loss 3.5837\n",
            "Epoch 10 Batch 759 Loss 3.5839\n",
            "Epoch 10 Batch 762 Loss 3.5838\n",
            "Epoch 10 Batch 765 Loss 3.5837\n",
            "Epoch 10 Batch 768 Loss 3.5843\n",
            "Epoch 10 Batch 771 Loss 3.5844\n",
            "Epoch 10 Batch 774 Loss 3.5845\n",
            "Epoch 10 Batch 777 Loss 3.5843\n",
            "Epoch 10 Batch 780 Loss 3.5853\n",
            "Epoch 10 Batch 783 Loss 3.5854\n",
            "Epoch 10 Batch 786 Loss 3.5860\n",
            "Epoch 10 Batch 789 Loss 3.5859\n",
            "Epoch 10 Batch 792 Loss 3.5859\n",
            "Epoch 10 Batch 795 Loss 3.5866\n",
            "Epoch 10 Batch 798 Loss 3.5868\n",
            "Epoch 10 Batch 801 Loss 3.5870\n",
            "Epoch 10 Batch 804 Loss 3.5870\n",
            "Epoch 10 Batch 807 Loss 3.5874\n",
            "Epoch 10 Batch 810 Loss 3.5879\n",
            "Epoch 10 Batch 813 Loss 3.5878\n",
            "Epoch 10 Batch 816 Loss 3.5883\n",
            "Epoch 10 Batch 819 Loss 3.5889\n",
            "Epoch 10 Batch 822 Loss 3.5896\n",
            "Epoch 10 Batch 825 Loss 3.5897\n",
            "Epoch 10 Batch 828 Loss 3.5895\n",
            "Epoch 10 Batch 831 Loss 3.5902\n",
            "Epoch 10 Batch 834 Loss 3.5900\n",
            "Epoch 10 Batch 837 Loss 3.5895\n",
            "Epoch 10 Batch 840 Loss 3.5899\n",
            "Epoch 10 Batch 843 Loss 3.5904\n",
            "Epoch 10 Batch 846 Loss 3.5904\n",
            "Epoch 10 Batch 849 Loss 3.5913\n",
            "Epoch 10 Batch 852 Loss 3.5917\n",
            "Epoch 10 Batch 855 Loss 3.5918\n",
            "Epoch 10 Batch 858 Loss 3.5924\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 3.5926\n",
            "Time taken for 1 epoch: 329.7597494125366 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.4393\n",
            "Epoch 11 Batch 3 Loss 3.5578\n",
            "Epoch 11 Batch 6 Loss 3.4834\n",
            "Epoch 11 Batch 9 Loss 3.4972\n",
            "Epoch 11 Batch 12 Loss 3.4744\n",
            "Epoch 11 Batch 15 Loss 3.4740\n",
            "Epoch 11 Batch 18 Loss 3.4664\n",
            "Epoch 11 Batch 21 Loss 3.4448\n",
            "Epoch 11 Batch 24 Loss 3.4518\n",
            "Epoch 11 Batch 27 Loss 3.4454\n",
            "Epoch 11 Batch 30 Loss 3.4459\n",
            "Epoch 11 Batch 33 Loss 3.4400\n",
            "Epoch 11 Batch 36 Loss 3.4378\n",
            "Epoch 11 Batch 39 Loss 3.4346\n",
            "Epoch 11 Batch 42 Loss 3.4255\n",
            "Epoch 11 Batch 45 Loss 3.4169\n",
            "Epoch 11 Batch 48 Loss 3.4164\n",
            "Epoch 11 Batch 51 Loss 3.4201\n",
            "Epoch 11 Batch 54 Loss 3.4188\n",
            "Epoch 11 Batch 57 Loss 3.4115\n",
            "Epoch 11 Batch 60 Loss 3.4094\n",
            "Epoch 11 Batch 63 Loss 3.4066\n",
            "Epoch 11 Batch 66 Loss 3.4091\n",
            "Epoch 11 Batch 69 Loss 3.4100\n",
            "Epoch 11 Batch 72 Loss 3.4100\n",
            "Epoch 11 Batch 75 Loss 3.4108\n",
            "Epoch 11 Batch 78 Loss 3.4121\n",
            "Epoch 11 Batch 81 Loss 3.4157\n",
            "Epoch 11 Batch 84 Loss 3.4124\n",
            "Epoch 11 Batch 87 Loss 3.4125\n",
            "Epoch 11 Batch 90 Loss 3.4144\n",
            "Epoch 11 Batch 93 Loss 3.4127\n",
            "Epoch 11 Batch 96 Loss 3.4092\n",
            "Epoch 11 Batch 99 Loss 3.4108\n",
            "Epoch 11 Batch 102 Loss 3.4114\n",
            "Epoch 11 Batch 105 Loss 3.4157\n",
            "Epoch 11 Batch 108 Loss 3.4161\n",
            "Epoch 11 Batch 111 Loss 3.4165\n",
            "Epoch 11 Batch 114 Loss 3.4174\n",
            "Epoch 11 Batch 117 Loss 3.4155\n",
            "Epoch 11 Batch 120 Loss 3.4158\n",
            "Epoch 11 Batch 123 Loss 3.4184\n",
            "Epoch 11 Batch 126 Loss 3.4194\n",
            "Epoch 11 Batch 129 Loss 3.4183\n",
            "Epoch 11 Batch 132 Loss 3.4173\n",
            "Epoch 11 Batch 135 Loss 3.4186\n",
            "Epoch 11 Batch 138 Loss 3.4190\n",
            "Epoch 11 Batch 141 Loss 3.4177\n",
            "Epoch 11 Batch 144 Loss 3.4146\n",
            "Epoch 11 Batch 147 Loss 3.4130\n",
            "Epoch 11 Batch 150 Loss 3.4141\n",
            "Epoch 11 Batch 153 Loss 3.4158\n",
            "Epoch 11 Batch 156 Loss 3.4162\n",
            "Epoch 11 Batch 159 Loss 3.4167\n",
            "Epoch 11 Batch 162 Loss 3.4176\n",
            "Epoch 11 Batch 165 Loss 3.4159\n",
            "Epoch 11 Batch 168 Loss 3.4174\n",
            "Epoch 11 Batch 171 Loss 3.4189\n",
            "Epoch 11 Batch 174 Loss 3.4209\n",
            "Epoch 11 Batch 177 Loss 3.4229\n",
            "Epoch 11 Batch 180 Loss 3.4252\n",
            "Epoch 11 Batch 183 Loss 3.4274\n",
            "Epoch 11 Batch 186 Loss 3.4286\n",
            "Epoch 11 Batch 189 Loss 3.4304\n",
            "Epoch 11 Batch 192 Loss 3.4315\n",
            "Epoch 11 Batch 195 Loss 3.4307\n",
            "Epoch 11 Batch 198 Loss 3.4332\n",
            "Epoch 11 Batch 201 Loss 3.4348\n",
            "Epoch 11 Batch 204 Loss 3.4383\n",
            "Epoch 11 Batch 207 Loss 3.4402\n",
            "Epoch 11 Batch 210 Loss 3.4400\n",
            "Epoch 11 Batch 213 Loss 3.4392\n",
            "Epoch 11 Batch 216 Loss 3.4404\n",
            "Epoch 11 Batch 219 Loss 3.4399\n",
            "Epoch 11 Batch 222 Loss 3.4392\n",
            "Epoch 11 Batch 225 Loss 3.4391\n",
            "Epoch 11 Batch 228 Loss 3.4402\n",
            "Epoch 11 Batch 231 Loss 3.4400\n",
            "Epoch 11 Batch 234 Loss 3.4397\n",
            "Epoch 11 Batch 237 Loss 3.4391\n",
            "Epoch 11 Batch 240 Loss 3.4388\n",
            "Epoch 11 Batch 243 Loss 3.4394\n",
            "Epoch 11 Batch 246 Loss 3.4407\n",
            "Epoch 11 Batch 249 Loss 3.4413\n",
            "Epoch 11 Batch 252 Loss 3.4426\n",
            "Epoch 11 Batch 255 Loss 3.4431\n",
            "Epoch 11 Batch 258 Loss 3.4430\n",
            "Epoch 11 Batch 261 Loss 3.4430\n",
            "Epoch 11 Batch 264 Loss 3.4433\n",
            "Epoch 11 Batch 267 Loss 3.4427\n",
            "Epoch 11 Batch 270 Loss 3.4424\n",
            "Epoch 11 Batch 273 Loss 3.4418\n",
            "Epoch 11 Batch 276 Loss 3.4423\n",
            "Epoch 11 Batch 279 Loss 3.4426\n",
            "Epoch 11 Batch 282 Loss 3.4432\n",
            "Epoch 11 Batch 285 Loss 3.4430\n",
            "Epoch 11 Batch 288 Loss 3.4445\n",
            "Epoch 11 Batch 291 Loss 3.4457\n",
            "Epoch 11 Batch 294 Loss 3.4458\n",
            "Epoch 11 Batch 297 Loss 3.4440\n",
            "Epoch 11 Batch 300 Loss 3.4447\n",
            "Epoch 11 Batch 303 Loss 3.4439\n",
            "Epoch 11 Batch 306 Loss 3.4447\n",
            "Epoch 11 Batch 309 Loss 3.4448\n",
            "Epoch 11 Batch 312 Loss 3.4438\n",
            "Epoch 11 Batch 315 Loss 3.4439\n",
            "Epoch 11 Batch 318 Loss 3.4444\n",
            "Epoch 11 Batch 321 Loss 3.4433\n",
            "Epoch 11 Batch 324 Loss 3.4434\n",
            "Epoch 11 Batch 327 Loss 3.4446\n",
            "Epoch 11 Batch 330 Loss 3.4438\n",
            "Epoch 11 Batch 333 Loss 3.4434\n",
            "Epoch 11 Batch 336 Loss 3.4434\n",
            "Epoch 11 Batch 339 Loss 3.4439\n",
            "Epoch 11 Batch 342 Loss 3.4443\n",
            "Epoch 11 Batch 345 Loss 3.4438\n",
            "Epoch 11 Batch 348 Loss 3.4430\n",
            "Epoch 11 Batch 351 Loss 3.4439\n",
            "Epoch 11 Batch 354 Loss 3.4447\n",
            "Epoch 11 Batch 357 Loss 3.4451\n",
            "Epoch 11 Batch 360 Loss 3.4461\n",
            "Epoch 11 Batch 363 Loss 3.4470\n",
            "Epoch 11 Batch 366 Loss 3.4477\n",
            "Epoch 11 Batch 369 Loss 3.4479\n",
            "Epoch 11 Batch 372 Loss 3.4487\n",
            "Epoch 11 Batch 375 Loss 3.4487\n",
            "Epoch 11 Batch 378 Loss 3.4478\n",
            "Epoch 11 Batch 381 Loss 3.4484\n",
            "Epoch 11 Batch 384 Loss 3.4480\n",
            "Epoch 11 Batch 387 Loss 3.4482\n",
            "Epoch 11 Batch 390 Loss 3.4479\n",
            "Epoch 11 Batch 393 Loss 3.4473\n",
            "Epoch 11 Batch 396 Loss 3.4457\n",
            "Epoch 11 Batch 399 Loss 3.4451\n",
            "Epoch 11 Batch 402 Loss 3.4450\n",
            "Epoch 11 Batch 405 Loss 3.4456\n",
            "Epoch 11 Batch 408 Loss 3.4446\n",
            "Epoch 11 Batch 411 Loss 3.4437\n",
            "Epoch 11 Batch 414 Loss 3.4441\n",
            "Epoch 11 Batch 417 Loss 3.4441\n",
            "Epoch 11 Batch 420 Loss 3.4432\n",
            "Epoch 11 Batch 423 Loss 3.4429\n",
            "Epoch 11 Batch 426 Loss 3.4424\n",
            "Epoch 11 Batch 429 Loss 3.4425\n",
            "Epoch 11 Batch 432 Loss 3.4434\n",
            "Epoch 11 Batch 435 Loss 3.4432\n",
            "Epoch 11 Batch 438 Loss 3.4434\n",
            "Epoch 11 Batch 441 Loss 3.4437\n",
            "Epoch 11 Batch 444 Loss 3.4439\n",
            "Epoch 11 Batch 447 Loss 3.4434\n",
            "Epoch 11 Batch 450 Loss 3.4440\n",
            "Epoch 11 Batch 453 Loss 3.4445\n",
            "Epoch 11 Batch 456 Loss 3.4448\n",
            "Epoch 11 Batch 459 Loss 3.4441\n",
            "Epoch 11 Batch 462 Loss 3.4432\n",
            "Epoch 11 Batch 465 Loss 3.4429\n",
            "Epoch 11 Batch 468 Loss 3.4418\n",
            "Epoch 11 Batch 471 Loss 3.4416\n",
            "Epoch 11 Batch 474 Loss 3.4415\n",
            "Epoch 11 Batch 477 Loss 3.4411\n",
            "Epoch 11 Batch 480 Loss 3.4410\n",
            "Epoch 11 Batch 483 Loss 3.4408\n",
            "Epoch 11 Batch 486 Loss 3.4406\n",
            "Epoch 11 Batch 489 Loss 3.4398\n",
            "Epoch 11 Batch 492 Loss 3.4399\n",
            "Epoch 11 Batch 495 Loss 3.4393\n",
            "Epoch 11 Batch 498 Loss 3.4392\n",
            "Epoch 11 Batch 501 Loss 3.4387\n",
            "Epoch 11 Batch 504 Loss 3.4384\n",
            "Epoch 11 Batch 507 Loss 3.4382\n",
            "Epoch 11 Batch 510 Loss 3.4383\n",
            "Epoch 11 Batch 513 Loss 3.4373\n",
            "Epoch 11 Batch 516 Loss 3.4378\n",
            "Epoch 11 Batch 519 Loss 3.4382\n",
            "Epoch 11 Batch 522 Loss 3.4385\n",
            "Epoch 11 Batch 525 Loss 3.4382\n",
            "Epoch 11 Batch 528 Loss 3.4379\n",
            "Epoch 11 Batch 531 Loss 3.4376\n",
            "Epoch 11 Batch 534 Loss 3.4379\n",
            "Epoch 11 Batch 537 Loss 3.4371\n",
            "Epoch 11 Batch 540 Loss 3.4365\n",
            "Epoch 11 Batch 543 Loss 3.4366\n",
            "Epoch 11 Batch 546 Loss 3.4362\n",
            "Epoch 11 Batch 549 Loss 3.4362\n",
            "Epoch 11 Batch 552 Loss 3.4366\n",
            "Epoch 11 Batch 555 Loss 3.4365\n",
            "Epoch 11 Batch 558 Loss 3.4361\n",
            "Epoch 11 Batch 561 Loss 3.4356\n",
            "Epoch 11 Batch 564 Loss 3.4349\n",
            "Epoch 11 Batch 567 Loss 3.4344\n",
            "Epoch 11 Batch 570 Loss 3.4338\n",
            "Epoch 11 Batch 573 Loss 3.4333\n",
            "Epoch 11 Batch 576 Loss 3.4331\n",
            "Epoch 11 Batch 579 Loss 3.4333\n",
            "Epoch 11 Batch 582 Loss 3.4321\n",
            "Epoch 11 Batch 585 Loss 3.4318\n",
            "Epoch 11 Batch 588 Loss 3.4318\n",
            "Epoch 11 Batch 591 Loss 3.4330\n",
            "Epoch 11 Batch 594 Loss 3.4324\n",
            "Epoch 11 Batch 597 Loss 3.4322\n",
            "Epoch 11 Batch 600 Loss 3.4319\n",
            "Epoch 11 Batch 603 Loss 3.4316\n",
            "Epoch 11 Batch 606 Loss 3.4320\n",
            "Epoch 11 Batch 609 Loss 3.4320\n",
            "Epoch 11 Batch 612 Loss 3.4318\n",
            "Epoch 11 Batch 615 Loss 3.4328\n",
            "Epoch 11 Batch 618 Loss 3.4330\n",
            "Epoch 11 Batch 621 Loss 3.4328\n",
            "Epoch 11 Batch 624 Loss 3.4325\n",
            "Epoch 11 Batch 627 Loss 3.4333\n",
            "Epoch 11 Batch 630 Loss 3.4337\n",
            "Epoch 11 Batch 633 Loss 3.4335\n",
            "Epoch 11 Batch 636 Loss 3.4337\n",
            "Epoch 11 Batch 639 Loss 3.4335\n",
            "Epoch 11 Batch 642 Loss 3.4334\n",
            "Epoch 11 Batch 645 Loss 3.4330\n",
            "Epoch 11 Batch 648 Loss 3.4327\n",
            "Epoch 11 Batch 651 Loss 3.4325\n",
            "Epoch 11 Batch 654 Loss 3.4326\n",
            "Epoch 11 Batch 657 Loss 3.4326\n",
            "Epoch 11 Batch 660 Loss 3.4323\n",
            "Epoch 11 Batch 663 Loss 3.4328\n",
            "Epoch 11 Batch 666 Loss 3.4328\n",
            "Epoch 11 Batch 669 Loss 3.4326\n",
            "Epoch 11 Batch 672 Loss 3.4329\n",
            "Epoch 11 Batch 675 Loss 3.4335\n",
            "Epoch 11 Batch 678 Loss 3.4337\n",
            "Epoch 11 Batch 681 Loss 3.4340\n",
            "Epoch 11 Batch 684 Loss 3.4338\n",
            "Epoch 11 Batch 687 Loss 3.4343\n",
            "Epoch 11 Batch 690 Loss 3.4345\n",
            "Epoch 11 Batch 693 Loss 3.4343\n",
            "Epoch 11 Batch 696 Loss 3.4346\n",
            "Epoch 11 Batch 699 Loss 3.4351\n",
            "Epoch 11 Batch 702 Loss 3.4353\n",
            "Epoch 11 Batch 705 Loss 3.4360\n",
            "Epoch 11 Batch 708 Loss 3.4362\n",
            "Epoch 11 Batch 711 Loss 3.4362\n",
            "Epoch 11 Batch 714 Loss 3.4357\n",
            "Epoch 11 Batch 717 Loss 3.4363\n",
            "Epoch 11 Batch 720 Loss 3.4370\n",
            "Epoch 11 Batch 723 Loss 3.4372\n",
            "Epoch 11 Batch 726 Loss 3.4368\n",
            "Epoch 11 Batch 729 Loss 3.4368\n",
            "Epoch 11 Batch 732 Loss 3.4368\n",
            "Epoch 11 Batch 735 Loss 3.4372\n",
            "Epoch 11 Batch 738 Loss 3.4375\n",
            "Epoch 11 Batch 741 Loss 3.4378\n",
            "Epoch 11 Batch 744 Loss 3.4383\n",
            "Epoch 11 Batch 747 Loss 3.4379\n",
            "Epoch 11 Batch 750 Loss 3.4376\n",
            "Epoch 11 Batch 753 Loss 3.4380\n",
            "Epoch 11 Batch 756 Loss 3.4380\n",
            "Epoch 11 Batch 759 Loss 3.4380\n",
            "Epoch 11 Batch 762 Loss 3.4380\n",
            "Epoch 11 Batch 765 Loss 3.4387\n",
            "Epoch 11 Batch 768 Loss 3.4388\n",
            "Epoch 11 Batch 771 Loss 3.4392\n",
            "Epoch 11 Batch 774 Loss 3.4391\n",
            "Epoch 11 Batch 777 Loss 3.4390\n",
            "Epoch 11 Batch 780 Loss 3.4393\n",
            "Epoch 11 Batch 783 Loss 3.4395\n",
            "Epoch 11 Batch 786 Loss 3.4403\n",
            "Epoch 11 Batch 789 Loss 3.4407\n",
            "Epoch 11 Batch 792 Loss 3.4410\n",
            "Epoch 11 Batch 795 Loss 3.4416\n",
            "Epoch 11 Batch 798 Loss 3.4417\n",
            "Epoch 11 Batch 801 Loss 3.4421\n",
            "Epoch 11 Batch 804 Loss 3.4421\n",
            "Epoch 11 Batch 807 Loss 3.4421\n",
            "Epoch 11 Batch 810 Loss 3.4428\n",
            "Epoch 11 Batch 813 Loss 3.4436\n",
            "Epoch 11 Batch 816 Loss 3.4438\n",
            "Epoch 11 Batch 819 Loss 3.4442\n",
            "Epoch 11 Batch 822 Loss 3.4443\n",
            "Epoch 11 Batch 825 Loss 3.4448\n",
            "Epoch 11 Batch 828 Loss 3.4449\n",
            "Epoch 11 Batch 831 Loss 3.4452\n",
            "Epoch 11 Batch 834 Loss 3.4452\n",
            "Epoch 11 Batch 837 Loss 3.4453\n",
            "Epoch 11 Batch 840 Loss 3.4456\n",
            "Epoch 11 Batch 843 Loss 3.4459\n",
            "Epoch 11 Batch 846 Loss 3.4456\n",
            "Epoch 11 Batch 849 Loss 3.4459\n",
            "Epoch 11 Batch 852 Loss 3.4462\n",
            "Epoch 11 Batch 855 Loss 3.4467\n",
            "Epoch 11 Batch 858 Loss 3.4470\n",
            "Epoch 11 Loss 3.4468\n",
            "Time taken for 1 epoch: 329.325875043869 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.1970\n",
            "Epoch 12 Batch 3 Loss 3.4011\n",
            "Epoch 12 Batch 6 Loss 3.3610\n",
            "Epoch 12 Batch 9 Loss 3.3364\n",
            "Epoch 12 Batch 12 Loss 3.3358\n",
            "Epoch 12 Batch 15 Loss 3.3084\n",
            "Epoch 12 Batch 18 Loss 3.2920\n",
            "Epoch 12 Batch 21 Loss 3.2942\n",
            "Epoch 12 Batch 24 Loss 3.2903\n",
            "Epoch 12 Batch 27 Loss 3.2870\n",
            "Epoch 12 Batch 30 Loss 3.2841\n",
            "Epoch 12 Batch 33 Loss 3.2864\n",
            "Epoch 12 Batch 36 Loss 3.2856\n",
            "Epoch 12 Batch 39 Loss 3.2811\n",
            "Epoch 12 Batch 42 Loss 3.2852\n",
            "Epoch 12 Batch 45 Loss 3.2908\n",
            "Epoch 12 Batch 48 Loss 3.2906\n",
            "Epoch 12 Batch 51 Loss 3.2928\n",
            "Epoch 12 Batch 54 Loss 3.2943\n",
            "Epoch 12 Batch 57 Loss 3.2872\n",
            "Epoch 12 Batch 60 Loss 3.2835\n",
            "Epoch 12 Batch 63 Loss 3.2839\n",
            "Epoch 12 Batch 66 Loss 3.2830\n",
            "Epoch 12 Batch 69 Loss 3.2814\n",
            "Epoch 12 Batch 72 Loss 3.2794\n",
            "Epoch 12 Batch 75 Loss 3.2752\n",
            "Epoch 12 Batch 78 Loss 3.2735\n",
            "Epoch 12 Batch 81 Loss 3.2712\n",
            "Epoch 12 Batch 84 Loss 3.2737\n",
            "Epoch 12 Batch 87 Loss 3.2737\n",
            "Epoch 12 Batch 90 Loss 3.2742\n",
            "Epoch 12 Batch 93 Loss 3.2754\n",
            "Epoch 12 Batch 96 Loss 3.2789\n",
            "Epoch 12 Batch 99 Loss 3.2769\n",
            "Epoch 12 Batch 102 Loss 3.2759\n",
            "Epoch 12 Batch 105 Loss 3.2780\n",
            "Epoch 12 Batch 108 Loss 3.2790\n",
            "Epoch 12 Batch 111 Loss 3.2760\n",
            "Epoch 12 Batch 114 Loss 3.2779\n",
            "Epoch 12 Batch 117 Loss 3.2781\n",
            "Epoch 12 Batch 120 Loss 3.2792\n",
            "Epoch 12 Batch 123 Loss 3.2799\n",
            "Epoch 12 Batch 126 Loss 3.2795\n",
            "Epoch 12 Batch 129 Loss 3.2815\n",
            "Epoch 12 Batch 132 Loss 3.2812\n",
            "Epoch 12 Batch 135 Loss 3.2812\n",
            "Epoch 12 Batch 138 Loss 3.2789\n",
            "Epoch 12 Batch 141 Loss 3.2808\n",
            "Epoch 12 Batch 144 Loss 3.2840\n",
            "Epoch 12 Batch 147 Loss 3.2846\n",
            "Epoch 12 Batch 150 Loss 3.2875\n",
            "Epoch 12 Batch 153 Loss 3.2894\n",
            "Epoch 12 Batch 156 Loss 3.2894\n",
            "Epoch 12 Batch 159 Loss 3.2921\n",
            "Epoch 12 Batch 162 Loss 3.2923\n",
            "Epoch 12 Batch 165 Loss 3.2929\n",
            "Epoch 12 Batch 168 Loss 3.2921\n",
            "Epoch 12 Batch 171 Loss 3.2908\n",
            "Epoch 12 Batch 174 Loss 3.2897\n",
            "Epoch 12 Batch 177 Loss 3.2896\n",
            "Epoch 12 Batch 180 Loss 3.2909\n",
            "Epoch 12 Batch 183 Loss 3.2900\n",
            "Epoch 12 Batch 186 Loss 3.2889\n",
            "Epoch 12 Batch 189 Loss 3.2905\n",
            "Epoch 12 Batch 192 Loss 3.2929\n",
            "Epoch 12 Batch 195 Loss 3.2942\n",
            "Epoch 12 Batch 198 Loss 3.2931\n",
            "Epoch 12 Batch 201 Loss 3.2932\n",
            "Epoch 12 Batch 204 Loss 3.2933\n",
            "Epoch 12 Batch 207 Loss 3.2916\n",
            "Epoch 12 Batch 210 Loss 3.2919\n",
            "Epoch 12 Batch 213 Loss 3.2919\n",
            "Epoch 12 Batch 216 Loss 3.2919\n",
            "Epoch 12 Batch 219 Loss 3.2913\n",
            "Epoch 12 Batch 222 Loss 3.2917\n",
            "Epoch 12 Batch 225 Loss 3.2926\n",
            "Epoch 12 Batch 228 Loss 3.2928\n",
            "Epoch 12 Batch 231 Loss 3.2931\n",
            "Epoch 12 Batch 234 Loss 3.2919\n",
            "Epoch 12 Batch 237 Loss 3.2916\n",
            "Epoch 12 Batch 240 Loss 3.2923\n",
            "Epoch 12 Batch 243 Loss 3.2929\n",
            "Epoch 12 Batch 246 Loss 3.2926\n",
            "Epoch 12 Batch 249 Loss 3.2922\n",
            "Epoch 12 Batch 252 Loss 3.2917\n",
            "Epoch 12 Batch 255 Loss 3.2926\n",
            "Epoch 12 Batch 258 Loss 3.2928\n",
            "Epoch 12 Batch 261 Loss 3.2935\n",
            "Epoch 12 Batch 264 Loss 3.2921\n",
            "Epoch 12 Batch 267 Loss 3.2906\n",
            "Epoch 12 Batch 270 Loss 3.2915\n",
            "Epoch 12 Batch 273 Loss 3.2909\n",
            "Epoch 12 Batch 276 Loss 3.2903\n",
            "Epoch 12 Batch 279 Loss 3.2911\n",
            "Epoch 12 Batch 282 Loss 3.2919\n",
            "Epoch 12 Batch 285 Loss 3.2925\n",
            "Epoch 12 Batch 288 Loss 3.2918\n",
            "Epoch 12 Batch 291 Loss 3.2917\n",
            "Epoch 12 Batch 294 Loss 3.2914\n",
            "Epoch 12 Batch 297 Loss 3.2902\n",
            "Epoch 12 Batch 300 Loss 3.2897\n",
            "Epoch 12 Batch 303 Loss 3.2893\n",
            "Epoch 12 Batch 306 Loss 3.2890\n",
            "Epoch 12 Batch 309 Loss 3.2876\n",
            "Epoch 12 Batch 312 Loss 3.2877\n",
            "Epoch 12 Batch 315 Loss 3.2874\n",
            "Epoch 12 Batch 318 Loss 3.2857\n",
            "Epoch 12 Batch 321 Loss 3.2864\n",
            "Epoch 12 Batch 324 Loss 3.2858\n",
            "Epoch 12 Batch 327 Loss 3.2854\n",
            "Epoch 12 Batch 330 Loss 3.2857\n",
            "Epoch 12 Batch 333 Loss 3.2864\n",
            "Epoch 12 Batch 336 Loss 3.2868\n",
            "Epoch 12 Batch 339 Loss 3.2872\n",
            "Epoch 12 Batch 342 Loss 3.2873\n",
            "Epoch 12 Batch 345 Loss 3.2877\n",
            "Epoch 12 Batch 348 Loss 3.2878\n",
            "Epoch 12 Batch 351 Loss 3.2881\n",
            "Epoch 12 Batch 354 Loss 3.2882\n",
            "Epoch 12 Batch 357 Loss 3.2874\n",
            "Epoch 12 Batch 360 Loss 3.2879\n",
            "Epoch 12 Batch 363 Loss 3.2877\n",
            "Epoch 12 Batch 366 Loss 3.2874\n",
            "Epoch 12 Batch 369 Loss 3.2877\n",
            "Epoch 12 Batch 372 Loss 3.2873\n",
            "Epoch 12 Batch 375 Loss 3.2885\n",
            "Epoch 12 Batch 378 Loss 3.2886\n",
            "Epoch 12 Batch 381 Loss 3.2883\n",
            "Epoch 12 Batch 384 Loss 3.2888\n",
            "Epoch 12 Batch 387 Loss 3.2882\n",
            "Epoch 12 Batch 390 Loss 3.2873\n",
            "Epoch 12 Batch 393 Loss 3.2874\n",
            "Epoch 12 Batch 396 Loss 3.2879\n",
            "Epoch 12 Batch 399 Loss 3.2880\n",
            "Epoch 12 Batch 402 Loss 3.2881\n",
            "Epoch 12 Batch 405 Loss 3.2877\n",
            "Epoch 12 Batch 408 Loss 3.2872\n",
            "Epoch 12 Batch 411 Loss 3.2882\n",
            "Epoch 12 Batch 414 Loss 3.2892\n",
            "Epoch 12 Batch 417 Loss 3.2890\n",
            "Epoch 12 Batch 420 Loss 3.2896\n",
            "Epoch 12 Batch 423 Loss 3.2893\n",
            "Epoch 12 Batch 426 Loss 3.2887\n",
            "Epoch 12 Batch 429 Loss 3.2890\n",
            "Epoch 12 Batch 432 Loss 3.2894\n",
            "Epoch 12 Batch 435 Loss 3.2894\n",
            "Epoch 12 Batch 438 Loss 3.2897\n",
            "Epoch 12 Batch 441 Loss 3.2900\n",
            "Epoch 12 Batch 444 Loss 3.2896\n",
            "Epoch 12 Batch 447 Loss 3.2904\n",
            "Epoch 12 Batch 450 Loss 3.2901\n",
            "Epoch 12 Batch 453 Loss 3.2900\n",
            "Epoch 12 Batch 456 Loss 3.2907\n",
            "Epoch 12 Batch 459 Loss 3.2910\n",
            "Epoch 12 Batch 462 Loss 3.2912\n",
            "Epoch 12 Batch 465 Loss 3.2910\n",
            "Epoch 12 Batch 468 Loss 3.2907\n",
            "Epoch 12 Batch 471 Loss 3.2904\n",
            "Epoch 12 Batch 474 Loss 3.2895\n",
            "Epoch 12 Batch 477 Loss 3.2901\n",
            "Epoch 12 Batch 480 Loss 3.2893\n",
            "Epoch 12 Batch 483 Loss 3.2892\n",
            "Epoch 12 Batch 486 Loss 3.2894\n",
            "Epoch 12 Batch 489 Loss 3.2889\n",
            "Epoch 12 Batch 492 Loss 3.2893\n",
            "Epoch 12 Batch 495 Loss 3.2885\n",
            "Epoch 12 Batch 498 Loss 3.2885\n",
            "Epoch 12 Batch 501 Loss 3.2888\n",
            "Epoch 12 Batch 504 Loss 3.2890\n",
            "Epoch 12 Batch 507 Loss 3.2884\n",
            "Epoch 12 Batch 510 Loss 3.2883\n",
            "Epoch 12 Batch 513 Loss 3.2887\n",
            "Epoch 12 Batch 516 Loss 3.2887\n",
            "Epoch 12 Batch 519 Loss 3.2882\n",
            "Epoch 12 Batch 522 Loss 3.2884\n",
            "Epoch 12 Batch 525 Loss 3.2882\n",
            "Epoch 12 Batch 528 Loss 3.2877\n",
            "Epoch 12 Batch 531 Loss 3.2879\n",
            "Epoch 12 Batch 534 Loss 3.2884\n",
            "Epoch 12 Batch 537 Loss 3.2883\n",
            "Epoch 12 Batch 540 Loss 3.2873\n",
            "Epoch 12 Batch 543 Loss 3.2874\n",
            "Epoch 12 Batch 546 Loss 3.2868\n",
            "Epoch 12 Batch 549 Loss 3.2869\n",
            "Epoch 12 Batch 552 Loss 3.2869\n",
            "Epoch 12 Batch 555 Loss 3.2859\n",
            "Epoch 12 Batch 558 Loss 3.2861\n",
            "Epoch 12 Batch 561 Loss 3.2856\n",
            "Epoch 12 Batch 564 Loss 3.2848\n",
            "Epoch 12 Batch 567 Loss 3.2845\n",
            "Epoch 12 Batch 570 Loss 3.2844\n",
            "Epoch 12 Batch 573 Loss 3.2846\n",
            "Epoch 12 Batch 576 Loss 3.2847\n",
            "Epoch 12 Batch 579 Loss 3.2849\n",
            "Epoch 12 Batch 582 Loss 3.2845\n",
            "Epoch 12 Batch 585 Loss 3.2855\n",
            "Epoch 12 Batch 588 Loss 3.2849\n",
            "Epoch 12 Batch 591 Loss 3.2844\n",
            "Epoch 12 Batch 594 Loss 3.2850\n",
            "Epoch 12 Batch 597 Loss 3.2850\n",
            "Epoch 12 Batch 600 Loss 3.2855\n",
            "Epoch 12 Batch 603 Loss 3.2851\n",
            "Epoch 12 Batch 606 Loss 3.2855\n",
            "Epoch 12 Batch 609 Loss 3.2858\n",
            "Epoch 12 Batch 612 Loss 3.2858\n",
            "Epoch 12 Batch 615 Loss 3.2857\n",
            "Epoch 12 Batch 618 Loss 3.2861\n",
            "Epoch 12 Batch 621 Loss 3.2867\n",
            "Epoch 12 Batch 624 Loss 3.2871\n",
            "Epoch 12 Batch 627 Loss 3.2872\n",
            "Epoch 12 Batch 630 Loss 3.2871\n",
            "Epoch 12 Batch 633 Loss 3.2882\n",
            "Epoch 12 Batch 636 Loss 3.2885\n",
            "Epoch 12 Batch 639 Loss 3.2886\n",
            "Epoch 12 Batch 642 Loss 3.2885\n",
            "Epoch 12 Batch 645 Loss 3.2882\n",
            "Epoch 12 Batch 648 Loss 3.2881\n",
            "Epoch 12 Batch 651 Loss 3.2886\n",
            "Epoch 12 Batch 654 Loss 3.2889\n",
            "Epoch 12 Batch 657 Loss 3.2892\n",
            "Epoch 12 Batch 660 Loss 3.2890\n",
            "Epoch 12 Batch 663 Loss 3.2892\n",
            "Epoch 12 Batch 666 Loss 3.2892\n",
            "Epoch 12 Batch 669 Loss 3.2891\n",
            "Epoch 12 Batch 672 Loss 3.2895\n",
            "Epoch 12 Batch 675 Loss 3.2899\n",
            "Epoch 12 Batch 678 Loss 3.2902\n",
            "Epoch 12 Batch 681 Loss 3.2902\n",
            "Epoch 12 Batch 684 Loss 3.2901\n",
            "Epoch 12 Batch 687 Loss 3.2900\n",
            "Epoch 12 Batch 690 Loss 3.2901\n",
            "Epoch 12 Batch 693 Loss 3.2908\n",
            "Epoch 12 Batch 696 Loss 3.2910\n",
            "Epoch 12 Batch 699 Loss 3.2909\n",
            "Epoch 12 Batch 702 Loss 3.2904\n",
            "Epoch 12 Batch 705 Loss 3.2903\n",
            "Epoch 12 Batch 708 Loss 3.2901\n",
            "Epoch 12 Batch 711 Loss 3.2906\n",
            "Epoch 12 Batch 714 Loss 3.2906\n",
            "Epoch 12 Batch 717 Loss 3.2913\n",
            "Epoch 12 Batch 720 Loss 3.2912\n",
            "Epoch 12 Batch 723 Loss 3.2909\n",
            "Epoch 12 Batch 726 Loss 3.2907\n",
            "Epoch 12 Batch 729 Loss 3.2909\n",
            "Epoch 12 Batch 732 Loss 3.2909\n",
            "Epoch 12 Batch 735 Loss 3.2912\n",
            "Epoch 12 Batch 738 Loss 3.2914\n",
            "Epoch 12 Batch 741 Loss 3.2917\n",
            "Epoch 12 Batch 744 Loss 3.2915\n",
            "Epoch 12 Batch 747 Loss 3.2922\n",
            "Epoch 12 Batch 750 Loss 3.2926\n",
            "Epoch 12 Batch 753 Loss 3.2931\n",
            "Epoch 12 Batch 756 Loss 3.2938\n",
            "Epoch 12 Batch 759 Loss 3.2937\n",
            "Epoch 12 Batch 762 Loss 3.2938\n",
            "Epoch 12 Batch 765 Loss 3.2934\n",
            "Epoch 12 Batch 768 Loss 3.2943\n",
            "Epoch 12 Batch 771 Loss 3.2944\n",
            "Epoch 12 Batch 774 Loss 3.2948\n",
            "Epoch 12 Batch 777 Loss 3.2951\n",
            "Epoch 12 Batch 780 Loss 3.2951\n",
            "Epoch 12 Batch 783 Loss 3.2946\n",
            "Epoch 12 Batch 786 Loss 3.2947\n",
            "Epoch 12 Batch 789 Loss 3.2945\n",
            "Epoch 12 Batch 792 Loss 3.2951\n",
            "Epoch 12 Batch 795 Loss 3.2957\n",
            "Epoch 12 Batch 798 Loss 3.2957\n",
            "Epoch 12 Batch 801 Loss 3.2967\n",
            "Epoch 12 Batch 804 Loss 3.2967\n",
            "Epoch 12 Batch 807 Loss 3.2970\n",
            "Epoch 12 Batch 810 Loss 3.2981\n",
            "Epoch 12 Batch 813 Loss 3.2991\n",
            "Epoch 12 Batch 816 Loss 3.2997\n",
            "Epoch 12 Batch 819 Loss 3.2996\n",
            "Epoch 12 Batch 822 Loss 3.3003\n",
            "Epoch 12 Batch 825 Loss 3.3006\n",
            "Epoch 12 Batch 828 Loss 3.3009\n",
            "Epoch 12 Batch 831 Loss 3.3010\n",
            "Epoch 12 Batch 834 Loss 3.3010\n",
            "Epoch 12 Batch 837 Loss 3.3011\n",
            "Epoch 12 Batch 840 Loss 3.3011\n",
            "Epoch 12 Batch 843 Loss 3.3016\n",
            "Epoch 12 Batch 846 Loss 3.3022\n",
            "Epoch 12 Batch 849 Loss 3.3027\n",
            "Epoch 12 Batch 852 Loss 3.3034\n",
            "Epoch 12 Batch 855 Loss 3.3041\n",
            "Epoch 12 Batch 858 Loss 3.3045\n",
            "Epoch 12 Loss 3.3050\n",
            "Time taken for 1 epoch: 329.3640947341919 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.2846\n",
            "Epoch 13 Batch 3 Loss 3.1519\n",
            "Epoch 13 Batch 6 Loss 3.1185\n",
            "Epoch 13 Batch 9 Loss 3.1191\n",
            "Epoch 13 Batch 12 Loss 3.1200\n",
            "Epoch 13 Batch 15 Loss 3.1219\n",
            "Epoch 13 Batch 18 Loss 3.1486\n",
            "Epoch 13 Batch 21 Loss 3.1509\n",
            "Epoch 13 Batch 24 Loss 3.1402\n",
            "Epoch 13 Batch 27 Loss 3.1348\n",
            "Epoch 13 Batch 30 Loss 3.1419\n",
            "Epoch 13 Batch 33 Loss 3.1402\n",
            "Epoch 13 Batch 36 Loss 3.1448\n",
            "Epoch 13 Batch 39 Loss 3.1390\n",
            "Epoch 13 Batch 42 Loss 3.1340\n",
            "Epoch 13 Batch 45 Loss 3.1359\n",
            "Epoch 13 Batch 48 Loss 3.1328\n",
            "Epoch 13 Batch 51 Loss 3.1347\n",
            "Epoch 13 Batch 54 Loss 3.1344\n",
            "Epoch 13 Batch 57 Loss 3.1365\n",
            "Epoch 13 Batch 60 Loss 3.1369\n",
            "Epoch 13 Batch 63 Loss 3.1394\n",
            "Epoch 13 Batch 66 Loss 3.1369\n",
            "Epoch 13 Batch 69 Loss 3.1337\n",
            "Epoch 13 Batch 72 Loss 3.1331\n",
            "Epoch 13 Batch 75 Loss 3.1312\n",
            "Epoch 13 Batch 78 Loss 3.1306\n",
            "Epoch 13 Batch 81 Loss 3.1268\n",
            "Epoch 13 Batch 84 Loss 3.1256\n",
            "Epoch 13 Batch 87 Loss 3.1263\n",
            "Epoch 13 Batch 90 Loss 3.1263\n",
            "Epoch 13 Batch 93 Loss 3.1288\n",
            "Epoch 13 Batch 96 Loss 3.1285\n",
            "Epoch 13 Batch 99 Loss 3.1312\n",
            "Epoch 13 Batch 102 Loss 3.1329\n",
            "Epoch 13 Batch 105 Loss 3.1371\n",
            "Epoch 13 Batch 108 Loss 3.1370\n",
            "Epoch 13 Batch 111 Loss 3.1361\n",
            "Epoch 13 Batch 114 Loss 3.1350\n",
            "Epoch 13 Batch 117 Loss 3.1357\n",
            "Epoch 13 Batch 120 Loss 3.1344\n",
            "Epoch 13 Batch 123 Loss 3.1339\n",
            "Epoch 13 Batch 126 Loss 3.1363\n",
            "Epoch 13 Batch 129 Loss 3.1363\n",
            "Epoch 13 Batch 132 Loss 3.1374\n",
            "Epoch 13 Batch 135 Loss 3.1359\n",
            "Epoch 13 Batch 138 Loss 3.1383\n",
            "Epoch 13 Batch 141 Loss 3.1401\n",
            "Epoch 13 Batch 144 Loss 3.1414\n",
            "Epoch 13 Batch 147 Loss 3.1384\n",
            "Epoch 13 Batch 150 Loss 3.1388\n",
            "Epoch 13 Batch 153 Loss 3.1382\n",
            "Epoch 13 Batch 156 Loss 3.1385\n",
            "Epoch 13 Batch 159 Loss 3.1389\n",
            "Epoch 13 Batch 162 Loss 3.1400\n",
            "Epoch 13 Batch 165 Loss 3.1398\n",
            "Epoch 13 Batch 168 Loss 3.1402\n",
            "Epoch 13 Batch 171 Loss 3.1413\n",
            "Epoch 13 Batch 174 Loss 3.1421\n",
            "Epoch 13 Batch 177 Loss 3.1426\n",
            "Epoch 13 Batch 180 Loss 3.1426\n",
            "Epoch 13 Batch 183 Loss 3.1434\n",
            "Epoch 13 Batch 186 Loss 3.1457\n",
            "Epoch 13 Batch 189 Loss 3.1456\n",
            "Epoch 13 Batch 192 Loss 3.1460\n",
            "Epoch 13 Batch 195 Loss 3.1465\n",
            "Epoch 13 Batch 198 Loss 3.1493\n",
            "Epoch 13 Batch 201 Loss 3.1489\n",
            "Epoch 13 Batch 204 Loss 3.1499\n",
            "Epoch 13 Batch 207 Loss 3.1514\n",
            "Epoch 13 Batch 210 Loss 3.1519\n",
            "Epoch 13 Batch 213 Loss 3.1526\n",
            "Epoch 13 Batch 216 Loss 3.1514\n",
            "Epoch 13 Batch 219 Loss 3.1522\n",
            "Epoch 13 Batch 222 Loss 3.1531\n",
            "Epoch 13 Batch 225 Loss 3.1522\n",
            "Epoch 13 Batch 228 Loss 3.1544\n",
            "Epoch 13 Batch 231 Loss 3.1554\n",
            "Epoch 13 Batch 234 Loss 3.1551\n",
            "Epoch 13 Batch 237 Loss 3.1546\n",
            "Epoch 13 Batch 240 Loss 3.1532\n",
            "Epoch 13 Batch 243 Loss 3.1527\n",
            "Epoch 13 Batch 246 Loss 3.1528\n",
            "Epoch 13 Batch 249 Loss 3.1515\n",
            "Epoch 13 Batch 252 Loss 3.1495\n",
            "Epoch 13 Batch 255 Loss 3.1495\n",
            "Epoch 13 Batch 258 Loss 3.1503\n",
            "Epoch 13 Batch 261 Loss 3.1516\n",
            "Epoch 13 Batch 264 Loss 3.1520\n",
            "Epoch 13 Batch 267 Loss 3.1523\n",
            "Epoch 13 Batch 270 Loss 3.1524\n",
            "Epoch 13 Batch 273 Loss 3.1527\n",
            "Epoch 13 Batch 276 Loss 3.1535\n",
            "Epoch 13 Batch 279 Loss 3.1556\n",
            "Epoch 13 Batch 282 Loss 3.1548\n",
            "Epoch 13 Batch 285 Loss 3.1548\n",
            "Epoch 13 Batch 288 Loss 3.1551\n",
            "Epoch 13 Batch 291 Loss 3.1556\n",
            "Epoch 13 Batch 294 Loss 3.1553\n",
            "Epoch 13 Batch 297 Loss 3.1550\n",
            "Epoch 13 Batch 300 Loss 3.1554\n",
            "Epoch 13 Batch 303 Loss 3.1548\n",
            "Epoch 13 Batch 306 Loss 3.1562\n",
            "Epoch 13 Batch 309 Loss 3.1572\n",
            "Epoch 13 Batch 312 Loss 3.1581\n",
            "Epoch 13 Batch 315 Loss 3.1585\n",
            "Epoch 13 Batch 318 Loss 3.1589\n",
            "Epoch 13 Batch 321 Loss 3.1589\n",
            "Epoch 13 Batch 324 Loss 3.1603\n",
            "Epoch 13 Batch 327 Loss 3.1606\n",
            "Epoch 13 Batch 330 Loss 3.1608\n",
            "Epoch 13 Batch 333 Loss 3.1611\n",
            "Epoch 13 Batch 336 Loss 3.1607\n",
            "Epoch 13 Batch 339 Loss 3.1613\n",
            "Epoch 13 Batch 342 Loss 3.1619\n",
            "Epoch 13 Batch 345 Loss 3.1609\n",
            "Epoch 13 Batch 348 Loss 3.1618\n",
            "Epoch 13 Batch 351 Loss 3.1621\n",
            "Epoch 13 Batch 354 Loss 3.1619\n",
            "Epoch 13 Batch 357 Loss 3.1612\n",
            "Epoch 13 Batch 360 Loss 3.1603\n",
            "Epoch 13 Batch 363 Loss 3.1610\n",
            "Epoch 13 Batch 366 Loss 3.1593\n",
            "Epoch 13 Batch 369 Loss 3.1595\n",
            "Epoch 13 Batch 372 Loss 3.1587\n",
            "Epoch 13 Batch 375 Loss 3.1580\n",
            "Epoch 13 Batch 378 Loss 3.1578\n",
            "Epoch 13 Batch 381 Loss 3.1585\n",
            "Epoch 13 Batch 384 Loss 3.1593\n",
            "Epoch 13 Batch 387 Loss 3.1597\n",
            "Epoch 13 Batch 390 Loss 3.1597\n",
            "Epoch 13 Batch 393 Loss 3.1594\n",
            "Epoch 13 Batch 396 Loss 3.1593\n",
            "Epoch 13 Batch 399 Loss 3.1587\n",
            "Epoch 13 Batch 402 Loss 3.1596\n",
            "Epoch 13 Batch 405 Loss 3.1597\n",
            "Epoch 13 Batch 408 Loss 3.1590\n",
            "Epoch 13 Batch 411 Loss 3.1580\n",
            "Epoch 13 Batch 414 Loss 3.1573\n",
            "Epoch 13 Batch 417 Loss 3.1583\n",
            "Epoch 13 Batch 420 Loss 3.1585\n",
            "Epoch 13 Batch 423 Loss 3.1584\n",
            "Epoch 13 Batch 426 Loss 3.1589\n",
            "Epoch 13 Batch 429 Loss 3.1599\n",
            "Epoch 13 Batch 432 Loss 3.1600\n",
            "Epoch 13 Batch 435 Loss 3.1605\n",
            "Epoch 13 Batch 438 Loss 3.1600\n",
            "Epoch 13 Batch 441 Loss 3.1604\n",
            "Epoch 13 Batch 444 Loss 3.1600\n",
            "Epoch 13 Batch 447 Loss 3.1601\n",
            "Epoch 13 Batch 450 Loss 3.1595\n",
            "Epoch 13 Batch 453 Loss 3.1601\n",
            "Epoch 13 Batch 456 Loss 3.1590\n",
            "Epoch 13 Batch 459 Loss 3.1584\n",
            "Epoch 13 Batch 462 Loss 3.1585\n",
            "Epoch 13 Batch 465 Loss 3.1589\n",
            "Epoch 13 Batch 468 Loss 3.1576\n",
            "Epoch 13 Batch 471 Loss 3.1571\n",
            "Epoch 13 Batch 474 Loss 3.1569\n",
            "Epoch 13 Batch 477 Loss 3.1566\n",
            "Epoch 13 Batch 480 Loss 3.1568\n",
            "Epoch 13 Batch 483 Loss 3.1572\n",
            "Epoch 13 Batch 486 Loss 3.1575\n",
            "Epoch 13 Batch 489 Loss 3.1571\n",
            "Epoch 13 Batch 492 Loss 3.1566\n",
            "Epoch 13 Batch 495 Loss 3.1560\n",
            "Epoch 13 Batch 498 Loss 3.1560\n",
            "Epoch 13 Batch 501 Loss 3.1555\n",
            "Epoch 13 Batch 504 Loss 3.1553\n",
            "Epoch 13 Batch 507 Loss 3.1548\n",
            "Epoch 13 Batch 510 Loss 3.1548\n",
            "Epoch 13 Batch 513 Loss 3.1541\n",
            "Epoch 13 Batch 516 Loss 3.1542\n",
            "Epoch 13 Batch 519 Loss 3.1547\n",
            "Epoch 13 Batch 522 Loss 3.1545\n",
            "Epoch 13 Batch 525 Loss 3.1539\n",
            "Epoch 13 Batch 528 Loss 3.1541\n",
            "Epoch 13 Batch 531 Loss 3.1538\n",
            "Epoch 13 Batch 534 Loss 3.1534\n",
            "Epoch 13 Batch 537 Loss 3.1534\n",
            "Epoch 13 Batch 540 Loss 3.1532\n",
            "Epoch 13 Batch 543 Loss 3.1525\n",
            "Epoch 13 Batch 546 Loss 3.1520\n",
            "Epoch 13 Batch 549 Loss 3.1520\n",
            "Epoch 13 Batch 552 Loss 3.1511\n",
            "Epoch 13 Batch 555 Loss 3.1515\n",
            "Epoch 13 Batch 558 Loss 3.1519\n",
            "Epoch 13 Batch 561 Loss 3.1519\n",
            "Epoch 13 Batch 564 Loss 3.1519\n",
            "Epoch 13 Batch 567 Loss 3.1518\n",
            "Epoch 13 Batch 570 Loss 3.1522\n",
            "Epoch 13 Batch 573 Loss 3.1524\n",
            "Epoch 13 Batch 576 Loss 3.1527\n",
            "Epoch 13 Batch 579 Loss 3.1523\n",
            "Epoch 13 Batch 582 Loss 3.1515\n",
            "Epoch 13 Batch 585 Loss 3.1509\n",
            "Epoch 13 Batch 588 Loss 3.1505\n",
            "Epoch 13 Batch 591 Loss 3.1502\n",
            "Epoch 13 Batch 594 Loss 3.1497\n",
            "Epoch 13 Batch 597 Loss 3.1502\n",
            "Epoch 13 Batch 600 Loss 3.1506\n",
            "Epoch 13 Batch 603 Loss 3.1507\n",
            "Epoch 13 Batch 606 Loss 3.1510\n",
            "Epoch 13 Batch 609 Loss 3.1512\n",
            "Epoch 13 Batch 612 Loss 3.1514\n",
            "Epoch 13 Batch 615 Loss 3.1519\n",
            "Epoch 13 Batch 618 Loss 3.1524\n",
            "Epoch 13 Batch 621 Loss 3.1522\n",
            "Epoch 13 Batch 624 Loss 3.1525\n",
            "Epoch 13 Batch 627 Loss 3.1526\n",
            "Epoch 13 Batch 630 Loss 3.1530\n",
            "Epoch 13 Batch 633 Loss 3.1529\n",
            "Epoch 13 Batch 636 Loss 3.1526\n",
            "Epoch 13 Batch 639 Loss 3.1527\n",
            "Epoch 13 Batch 642 Loss 3.1526\n",
            "Epoch 13 Batch 645 Loss 3.1524\n",
            "Epoch 13 Batch 648 Loss 3.1527\n",
            "Epoch 13 Batch 651 Loss 3.1529\n",
            "Epoch 13 Batch 654 Loss 3.1531\n",
            "Epoch 13 Batch 657 Loss 3.1538\n",
            "Epoch 13 Batch 660 Loss 3.1534\n",
            "Epoch 13 Batch 663 Loss 3.1533\n",
            "Epoch 13 Batch 666 Loss 3.1535\n",
            "Epoch 13 Batch 669 Loss 3.1534\n",
            "Epoch 13 Batch 672 Loss 3.1535\n",
            "Epoch 13 Batch 675 Loss 3.1539\n",
            "Epoch 13 Batch 678 Loss 3.1535\n",
            "Epoch 13 Batch 681 Loss 3.1534\n",
            "Epoch 13 Batch 684 Loss 3.1536\n",
            "Epoch 13 Batch 687 Loss 3.1539\n",
            "Epoch 13 Batch 690 Loss 3.1545\n",
            "Epoch 13 Batch 693 Loss 3.1548\n",
            "Epoch 13 Batch 696 Loss 3.1546\n",
            "Epoch 13 Batch 699 Loss 3.1544\n",
            "Epoch 13 Batch 702 Loss 3.1547\n",
            "Epoch 13 Batch 705 Loss 3.1548\n",
            "Epoch 13 Batch 708 Loss 3.1551\n",
            "Epoch 13 Batch 711 Loss 3.1556\n",
            "Epoch 13 Batch 714 Loss 3.1562\n",
            "Epoch 13 Batch 717 Loss 3.1565\n",
            "Epoch 13 Batch 720 Loss 3.1567\n",
            "Epoch 13 Batch 723 Loss 3.1569\n",
            "Epoch 13 Batch 726 Loss 3.1565\n",
            "Epoch 13 Batch 729 Loss 3.1569\n",
            "Epoch 13 Batch 732 Loss 3.1571\n",
            "Epoch 13 Batch 735 Loss 3.1572\n",
            "Epoch 13 Batch 738 Loss 3.1571\n",
            "Epoch 13 Batch 741 Loss 3.1569\n",
            "Epoch 13 Batch 744 Loss 3.1569\n",
            "Epoch 13 Batch 747 Loss 3.1575\n",
            "Epoch 13 Batch 750 Loss 3.1580\n",
            "Epoch 13 Batch 753 Loss 3.1579\n",
            "Epoch 13 Batch 756 Loss 3.1580\n",
            "Epoch 13 Batch 759 Loss 3.1582\n",
            "Epoch 13 Batch 762 Loss 3.1584\n",
            "Epoch 13 Batch 765 Loss 3.1590\n",
            "Epoch 13 Batch 768 Loss 3.1590\n",
            "Epoch 13 Batch 771 Loss 3.1592\n",
            "Epoch 13 Batch 774 Loss 3.1597\n",
            "Epoch 13 Batch 777 Loss 3.1602\n",
            "Epoch 13 Batch 780 Loss 3.1606\n",
            "Epoch 13 Batch 783 Loss 3.1604\n",
            "Epoch 13 Batch 786 Loss 3.1604\n",
            "Epoch 13 Batch 789 Loss 3.1605\n",
            "Epoch 13 Batch 792 Loss 3.1610\n",
            "Epoch 13 Batch 795 Loss 3.1620\n",
            "Epoch 13 Batch 798 Loss 3.1624\n",
            "Epoch 13 Batch 801 Loss 3.1626\n",
            "Epoch 13 Batch 804 Loss 3.1633\n",
            "Epoch 13 Batch 807 Loss 3.1634\n",
            "Epoch 13 Batch 810 Loss 3.1638\n",
            "Epoch 13 Batch 813 Loss 3.1637\n",
            "Epoch 13 Batch 816 Loss 3.1639\n",
            "Epoch 13 Batch 819 Loss 3.1636\n",
            "Epoch 13 Batch 822 Loss 3.1636\n",
            "Epoch 13 Batch 825 Loss 3.1645\n",
            "Epoch 13 Batch 828 Loss 3.1652\n",
            "Epoch 13 Batch 831 Loss 3.1656\n",
            "Epoch 13 Batch 834 Loss 3.1662\n",
            "Epoch 13 Batch 837 Loss 3.1666\n",
            "Epoch 13 Batch 840 Loss 3.1669\n",
            "Epoch 13 Batch 843 Loss 3.1671\n",
            "Epoch 13 Batch 846 Loss 3.1678\n",
            "Epoch 13 Batch 849 Loss 3.1682\n",
            "Epoch 13 Batch 852 Loss 3.1686\n",
            "Epoch 13 Batch 855 Loss 3.1688\n",
            "Epoch 13 Batch 858 Loss 3.1693\n",
            "Epoch 13 Loss 3.1700\n",
            "Time taken for 1 epoch: 329.09592032432556 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.9511\n",
            "Epoch 14 Batch 3 Loss 3.0383\n",
            "Epoch 14 Batch 6 Loss 3.0584\n",
            "Epoch 14 Batch 9 Loss 3.0599\n",
            "Epoch 14 Batch 12 Loss 3.0560\n",
            "Epoch 14 Batch 15 Loss 3.0374\n",
            "Epoch 14 Batch 18 Loss 3.0376\n",
            "Epoch 14 Batch 21 Loss 3.0323\n",
            "Epoch 14 Batch 24 Loss 3.0351\n",
            "Epoch 14 Batch 27 Loss 3.0362\n",
            "Epoch 14 Batch 30 Loss 3.0307\n",
            "Epoch 14 Batch 33 Loss 3.0250\n",
            "Epoch 14 Batch 36 Loss 3.0228\n",
            "Epoch 14 Batch 39 Loss 3.0235\n",
            "Epoch 14 Batch 42 Loss 3.0233\n",
            "Epoch 14 Batch 45 Loss 3.0175\n",
            "Epoch 14 Batch 48 Loss 3.0132\n",
            "Epoch 14 Batch 51 Loss 3.0101\n",
            "Epoch 14 Batch 54 Loss 3.0154\n",
            "Epoch 14 Batch 57 Loss 3.0131\n",
            "Epoch 14 Batch 60 Loss 3.0095\n",
            "Epoch 14 Batch 63 Loss 3.0049\n",
            "Epoch 14 Batch 66 Loss 3.0024\n",
            "Epoch 14 Batch 69 Loss 3.0054\n",
            "Epoch 14 Batch 72 Loss 3.0055\n",
            "Epoch 14 Batch 75 Loss 3.0131\n",
            "Epoch 14 Batch 78 Loss 3.0134\n",
            "Epoch 14 Batch 81 Loss 3.0114\n",
            "Epoch 14 Batch 84 Loss 3.0080\n",
            "Epoch 14 Batch 87 Loss 3.0055\n",
            "Epoch 14 Batch 90 Loss 3.0068\n",
            "Epoch 14 Batch 93 Loss 3.0077\n",
            "Epoch 14 Batch 96 Loss 3.0042\n",
            "Epoch 14 Batch 99 Loss 3.0041\n",
            "Epoch 14 Batch 102 Loss 3.0033\n",
            "Epoch 14 Batch 105 Loss 3.0073\n",
            "Epoch 14 Batch 108 Loss 3.0116\n",
            "Epoch 14 Batch 111 Loss 3.0126\n",
            "Epoch 14 Batch 114 Loss 3.0134\n",
            "Epoch 14 Batch 117 Loss 3.0140\n",
            "Epoch 14 Batch 120 Loss 3.0143\n",
            "Epoch 14 Batch 123 Loss 3.0162\n",
            "Epoch 14 Batch 126 Loss 3.0132\n",
            "Epoch 14 Batch 129 Loss 3.0113\n",
            "Epoch 14 Batch 132 Loss 3.0136\n",
            "Epoch 14 Batch 135 Loss 3.0132\n",
            "Epoch 14 Batch 138 Loss 3.0125\n",
            "Epoch 14 Batch 141 Loss 3.0124\n",
            "Epoch 14 Batch 144 Loss 3.0140\n",
            "Epoch 14 Batch 147 Loss 3.0119\n",
            "Epoch 14 Batch 150 Loss 3.0149\n",
            "Epoch 14 Batch 153 Loss 3.0150\n",
            "Epoch 14 Batch 156 Loss 3.0124\n",
            "Epoch 14 Batch 159 Loss 3.0116\n",
            "Epoch 14 Batch 162 Loss 3.0087\n",
            "Epoch 14 Batch 165 Loss 3.0074\n",
            "Epoch 14 Batch 168 Loss 3.0082\n",
            "Epoch 14 Batch 171 Loss 3.0098\n",
            "Epoch 14 Batch 174 Loss 3.0106\n",
            "Epoch 14 Batch 177 Loss 3.0095\n",
            "Epoch 14 Batch 180 Loss 3.0096\n",
            "Epoch 14 Batch 183 Loss 3.0106\n",
            "Epoch 14 Batch 186 Loss 3.0132\n",
            "Epoch 14 Batch 189 Loss 3.0132\n",
            "Epoch 14 Batch 192 Loss 3.0141\n",
            "Epoch 14 Batch 195 Loss 3.0136\n",
            "Epoch 14 Batch 198 Loss 3.0138\n",
            "Epoch 14 Batch 201 Loss 3.0161\n",
            "Epoch 14 Batch 204 Loss 3.0163\n",
            "Epoch 14 Batch 207 Loss 3.0169\n",
            "Epoch 14 Batch 210 Loss 3.0178\n",
            "Epoch 14 Batch 213 Loss 3.0174\n",
            "Epoch 14 Batch 216 Loss 3.0177\n",
            "Epoch 14 Batch 219 Loss 3.0169\n",
            "Epoch 14 Batch 222 Loss 3.0174\n",
            "Epoch 14 Batch 225 Loss 3.0168\n",
            "Epoch 14 Batch 228 Loss 3.0184\n",
            "Epoch 14 Batch 231 Loss 3.0183\n",
            "Epoch 14 Batch 234 Loss 3.0181\n",
            "Epoch 14 Batch 237 Loss 3.0190\n",
            "Epoch 14 Batch 240 Loss 3.0185\n",
            "Epoch 14 Batch 243 Loss 3.0186\n",
            "Epoch 14 Batch 246 Loss 3.0170\n",
            "Epoch 14 Batch 249 Loss 3.0172\n",
            "Epoch 14 Batch 252 Loss 3.0175\n",
            "Epoch 14 Batch 255 Loss 3.0213\n",
            "Epoch 14 Batch 258 Loss 3.0226\n",
            "Epoch 14 Batch 261 Loss 3.0232\n",
            "Epoch 14 Batch 264 Loss 3.0248\n",
            "Epoch 14 Batch 267 Loss 3.0249\n",
            "Epoch 14 Batch 270 Loss 3.0252\n",
            "Epoch 14 Batch 273 Loss 3.0253\n",
            "Epoch 14 Batch 276 Loss 3.0246\n",
            "Epoch 14 Batch 279 Loss 3.0253\n",
            "Epoch 14 Batch 282 Loss 3.0253\n",
            "Epoch 14 Batch 285 Loss 3.0243\n",
            "Epoch 14 Batch 288 Loss 3.0263\n",
            "Epoch 14 Batch 291 Loss 3.0280\n",
            "Epoch 14 Batch 294 Loss 3.0259\n",
            "Epoch 14 Batch 297 Loss 3.0252\n",
            "Epoch 14 Batch 300 Loss 3.0252\n",
            "Epoch 14 Batch 303 Loss 3.0248\n",
            "Epoch 14 Batch 306 Loss 3.0248\n",
            "Epoch 14 Batch 309 Loss 3.0239\n",
            "Epoch 14 Batch 312 Loss 3.0236\n",
            "Epoch 14 Batch 315 Loss 3.0244\n",
            "Epoch 14 Batch 318 Loss 3.0266\n",
            "Epoch 14 Batch 321 Loss 3.0272\n",
            "Epoch 14 Batch 324 Loss 3.0278\n",
            "Epoch 14 Batch 327 Loss 3.0278\n",
            "Epoch 14 Batch 330 Loss 3.0282\n",
            "Epoch 14 Batch 333 Loss 3.0285\n",
            "Epoch 14 Batch 336 Loss 3.0289\n",
            "Epoch 14 Batch 339 Loss 3.0292\n",
            "Epoch 14 Batch 342 Loss 3.0296\n",
            "Epoch 14 Batch 345 Loss 3.0294\n",
            "Epoch 14 Batch 348 Loss 3.0303\n",
            "Epoch 14 Batch 351 Loss 3.0303\n",
            "Epoch 14 Batch 354 Loss 3.0309\n",
            "Epoch 14 Batch 357 Loss 3.0311\n",
            "Epoch 14 Batch 360 Loss 3.0306\n",
            "Epoch 14 Batch 363 Loss 3.0306\n",
            "Epoch 14 Batch 366 Loss 3.0314\n",
            "Epoch 14 Batch 369 Loss 3.0311\n",
            "Epoch 14 Batch 372 Loss 3.0312\n",
            "Epoch 14 Batch 375 Loss 3.0314\n",
            "Epoch 14 Batch 378 Loss 3.0303\n",
            "Epoch 14 Batch 381 Loss 3.0303\n",
            "Epoch 14 Batch 384 Loss 3.0294\n",
            "Epoch 14 Batch 387 Loss 3.0282\n",
            "Epoch 14 Batch 390 Loss 3.0285\n",
            "Epoch 14 Batch 393 Loss 3.0289\n",
            "Epoch 14 Batch 396 Loss 3.0293\n",
            "Epoch 14 Batch 399 Loss 3.0291\n",
            "Epoch 14 Batch 402 Loss 3.0286\n",
            "Epoch 14 Batch 405 Loss 3.0281\n",
            "Epoch 14 Batch 408 Loss 3.0283\n",
            "Epoch 14 Batch 411 Loss 3.0273\n",
            "Epoch 14 Batch 414 Loss 3.0276\n",
            "Epoch 14 Batch 417 Loss 3.0280\n",
            "Epoch 14 Batch 420 Loss 3.0287\n",
            "Epoch 14 Batch 423 Loss 3.0292\n",
            "Epoch 14 Batch 426 Loss 3.0289\n",
            "Epoch 14 Batch 429 Loss 3.0291\n",
            "Epoch 14 Batch 432 Loss 3.0290\n",
            "Epoch 14 Batch 435 Loss 3.0289\n",
            "Epoch 14 Batch 438 Loss 3.0299\n",
            "Epoch 14 Batch 441 Loss 3.0305\n",
            "Epoch 14 Batch 444 Loss 3.0307\n",
            "Epoch 14 Batch 447 Loss 3.0307\n",
            "Epoch 14 Batch 450 Loss 3.0311\n",
            "Epoch 14 Batch 453 Loss 3.0307\n",
            "Epoch 14 Batch 456 Loss 3.0303\n",
            "Epoch 14 Batch 459 Loss 3.0314\n",
            "Epoch 14 Batch 462 Loss 3.0318\n",
            "Epoch 14 Batch 465 Loss 3.0314\n",
            "Epoch 14 Batch 468 Loss 3.0320\n",
            "Epoch 14 Batch 471 Loss 3.0321\n",
            "Epoch 14 Batch 474 Loss 3.0321\n",
            "Epoch 14 Batch 477 Loss 3.0323\n",
            "Epoch 14 Batch 480 Loss 3.0320\n",
            "Epoch 14 Batch 483 Loss 3.0322\n",
            "Epoch 14 Batch 486 Loss 3.0327\n",
            "Epoch 14 Batch 489 Loss 3.0325\n",
            "Epoch 14 Batch 492 Loss 3.0326\n",
            "Epoch 14 Batch 495 Loss 3.0322\n",
            "Epoch 14 Batch 498 Loss 3.0318\n",
            "Epoch 14 Batch 501 Loss 3.0311\n",
            "Epoch 14 Batch 504 Loss 3.0307\n",
            "Epoch 14 Batch 507 Loss 3.0312\n",
            "Epoch 14 Batch 510 Loss 3.0317\n",
            "Epoch 14 Batch 513 Loss 3.0320\n",
            "Epoch 14 Batch 516 Loss 3.0317\n",
            "Epoch 14 Batch 519 Loss 3.0321\n",
            "Epoch 14 Batch 522 Loss 3.0314\n",
            "Epoch 14 Batch 525 Loss 3.0313\n",
            "Epoch 14 Batch 528 Loss 3.0320\n",
            "Epoch 14 Batch 531 Loss 3.0320\n",
            "Epoch 14 Batch 534 Loss 3.0322\n",
            "Epoch 14 Batch 537 Loss 3.0315\n",
            "Epoch 14 Batch 540 Loss 3.0304\n",
            "Epoch 14 Batch 543 Loss 3.0307\n",
            "Epoch 14 Batch 546 Loss 3.0305\n",
            "Epoch 14 Batch 549 Loss 3.0302\n",
            "Epoch 14 Batch 552 Loss 3.0301\n",
            "Epoch 14 Batch 555 Loss 3.0301\n",
            "Epoch 14 Batch 558 Loss 3.0302\n",
            "Epoch 14 Batch 561 Loss 3.0299\n",
            "Epoch 14 Batch 564 Loss 3.0300\n",
            "Epoch 14 Batch 567 Loss 3.0309\n",
            "Epoch 14 Batch 570 Loss 3.0311\n",
            "Epoch 14 Batch 573 Loss 3.0312\n",
            "Epoch 14 Batch 576 Loss 3.0313\n",
            "Epoch 14 Batch 579 Loss 3.0314\n",
            "Epoch 14 Batch 582 Loss 3.0312\n",
            "Epoch 14 Batch 585 Loss 3.0314\n",
            "Epoch 14 Batch 588 Loss 3.0316\n",
            "Epoch 14 Batch 591 Loss 3.0317\n",
            "Epoch 14 Batch 594 Loss 3.0320\n",
            "Epoch 14 Batch 597 Loss 3.0320\n",
            "Epoch 14 Batch 600 Loss 3.0319\n",
            "Epoch 14 Batch 603 Loss 3.0316\n",
            "Epoch 14 Batch 606 Loss 3.0313\n",
            "Epoch 14 Batch 609 Loss 3.0317\n",
            "Epoch 14 Batch 612 Loss 3.0322\n",
            "Epoch 14 Batch 615 Loss 3.0323\n",
            "Epoch 14 Batch 618 Loss 3.0320\n",
            "Epoch 14 Batch 621 Loss 3.0318\n",
            "Epoch 14 Batch 624 Loss 3.0322\n",
            "Epoch 14 Batch 627 Loss 3.0324\n",
            "Epoch 14 Batch 630 Loss 3.0323\n",
            "Epoch 14 Batch 633 Loss 3.0325\n",
            "Epoch 14 Batch 636 Loss 3.0325\n",
            "Epoch 14 Batch 639 Loss 3.0321\n",
            "Epoch 14 Batch 642 Loss 3.0318\n",
            "Epoch 14 Batch 645 Loss 3.0323\n",
            "Epoch 14 Batch 648 Loss 3.0326\n",
            "Epoch 14 Batch 651 Loss 3.0327\n",
            "Epoch 14 Batch 654 Loss 3.0331\n",
            "Epoch 14 Batch 657 Loss 3.0337\n",
            "Epoch 14 Batch 660 Loss 3.0337\n",
            "Epoch 14 Batch 663 Loss 3.0333\n",
            "Epoch 14 Batch 666 Loss 3.0328\n",
            "Epoch 14 Batch 669 Loss 3.0329\n",
            "Epoch 14 Batch 672 Loss 3.0330\n",
            "Epoch 14 Batch 675 Loss 3.0333\n",
            "Epoch 14 Batch 678 Loss 3.0336\n",
            "Epoch 14 Batch 681 Loss 3.0337\n",
            "Epoch 14 Batch 684 Loss 3.0337\n",
            "Epoch 14 Batch 687 Loss 3.0340\n",
            "Epoch 14 Batch 690 Loss 3.0346\n",
            "Epoch 14 Batch 693 Loss 3.0351\n",
            "Epoch 14 Batch 696 Loss 3.0349\n",
            "Epoch 14 Batch 699 Loss 3.0349\n",
            "Epoch 14 Batch 702 Loss 3.0353\n",
            "Epoch 14 Batch 705 Loss 3.0358\n",
            "Epoch 14 Batch 708 Loss 3.0367\n",
            "Epoch 14 Batch 711 Loss 3.0372\n",
            "Epoch 14 Batch 714 Loss 3.0375\n",
            "Epoch 14 Batch 717 Loss 3.0377\n",
            "Epoch 14 Batch 720 Loss 3.0379\n",
            "Epoch 14 Batch 723 Loss 3.0380\n",
            "Epoch 14 Batch 726 Loss 3.0383\n",
            "Epoch 14 Batch 729 Loss 3.0387\n",
            "Epoch 14 Batch 732 Loss 3.0391\n",
            "Epoch 14 Batch 735 Loss 3.0392\n",
            "Epoch 14 Batch 738 Loss 3.0391\n",
            "Epoch 14 Batch 741 Loss 3.0394\n",
            "Epoch 14 Batch 744 Loss 3.0396\n",
            "Epoch 14 Batch 747 Loss 3.0399\n",
            "Epoch 14 Batch 750 Loss 3.0404\n",
            "Epoch 14 Batch 753 Loss 3.0409\n",
            "Epoch 14 Batch 756 Loss 3.0409\n",
            "Epoch 14 Batch 759 Loss 3.0409\n",
            "Epoch 14 Batch 762 Loss 3.0408\n",
            "Epoch 14 Batch 765 Loss 3.0410\n",
            "Epoch 14 Batch 768 Loss 3.0412\n",
            "Epoch 14 Batch 771 Loss 3.0412\n",
            "Epoch 14 Batch 774 Loss 3.0413\n",
            "Epoch 14 Batch 777 Loss 3.0418\n",
            "Epoch 14 Batch 780 Loss 3.0427\n",
            "Epoch 14 Batch 783 Loss 3.0432\n",
            "Epoch 14 Batch 786 Loss 3.0436\n",
            "Epoch 14 Batch 789 Loss 3.0441\n",
            "Epoch 14 Batch 792 Loss 3.0439\n",
            "Epoch 14 Batch 795 Loss 3.0444\n",
            "Epoch 14 Batch 798 Loss 3.0445\n",
            "Epoch 14 Batch 801 Loss 3.0449\n",
            "Epoch 14 Batch 804 Loss 3.0452\n",
            "Epoch 14 Batch 807 Loss 3.0455\n",
            "Epoch 14 Batch 810 Loss 3.0460\n",
            "Epoch 14 Batch 813 Loss 3.0461\n",
            "Epoch 14 Batch 816 Loss 3.0462\n",
            "Epoch 14 Batch 819 Loss 3.0466\n",
            "Epoch 14 Batch 822 Loss 3.0468\n",
            "Epoch 14 Batch 825 Loss 3.0473\n",
            "Epoch 14 Batch 828 Loss 3.0476\n",
            "Epoch 14 Batch 831 Loss 3.0477\n",
            "Epoch 14 Batch 834 Loss 3.0483\n",
            "Epoch 14 Batch 837 Loss 3.0487\n",
            "Epoch 14 Batch 840 Loss 3.0494\n",
            "Epoch 14 Batch 843 Loss 3.0497\n",
            "Epoch 14 Batch 846 Loss 3.0499\n",
            "Epoch 14 Batch 849 Loss 3.0500\n",
            "Epoch 14 Batch 852 Loss 3.0501\n",
            "Epoch 14 Batch 855 Loss 3.0507\n",
            "Epoch 14 Batch 858 Loss 3.0510\n",
            "Epoch 14 Loss 3.0513\n",
            "Time taken for 1 epoch: 329.4323229789734 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.9193\n",
            "Epoch 15 Batch 3 Loss 2.8674\n",
            "Epoch 15 Batch 6 Loss 2.8521\n",
            "Epoch 15 Batch 9 Loss 2.8849\n",
            "Epoch 15 Batch 12 Loss 2.8881\n",
            "Epoch 15 Batch 15 Loss 2.9058\n",
            "Epoch 15 Batch 18 Loss 2.9034\n",
            "Epoch 15 Batch 21 Loss 2.9140\n",
            "Epoch 15 Batch 24 Loss 2.9154\n",
            "Epoch 15 Batch 27 Loss 2.9120\n",
            "Epoch 15 Batch 30 Loss 2.9187\n",
            "Epoch 15 Batch 33 Loss 2.9094\n",
            "Epoch 15 Batch 36 Loss 2.9111\n",
            "Epoch 15 Batch 39 Loss 2.9146\n",
            "Epoch 15 Batch 42 Loss 2.9112\n",
            "Epoch 15 Batch 45 Loss 2.9084\n",
            "Epoch 15 Batch 48 Loss 2.9079\n",
            "Epoch 15 Batch 51 Loss 2.9038\n",
            "Epoch 15 Batch 54 Loss 2.9028\n",
            "Epoch 15 Batch 57 Loss 2.9054\n",
            "Epoch 15 Batch 60 Loss 2.9037\n",
            "Epoch 15 Batch 63 Loss 2.9025\n",
            "Epoch 15 Batch 66 Loss 2.9037\n",
            "Epoch 15 Batch 69 Loss 2.9014\n",
            "Epoch 15 Batch 72 Loss 2.9014\n",
            "Epoch 15 Batch 75 Loss 2.8990\n",
            "Epoch 15 Batch 78 Loss 2.8986\n",
            "Epoch 15 Batch 81 Loss 2.8947\n",
            "Epoch 15 Batch 84 Loss 2.8951\n",
            "Epoch 15 Batch 87 Loss 2.8953\n",
            "Epoch 15 Batch 90 Loss 2.8939\n",
            "Epoch 15 Batch 93 Loss 2.8943\n",
            "Epoch 15 Batch 96 Loss 2.8969\n",
            "Epoch 15 Batch 99 Loss 2.8949\n",
            "Epoch 15 Batch 102 Loss 2.8993\n",
            "Epoch 15 Batch 105 Loss 2.8973\n",
            "Epoch 15 Batch 108 Loss 2.8961\n",
            "Epoch 15 Batch 111 Loss 2.8953\n",
            "Epoch 15 Batch 114 Loss 2.8966\n",
            "Epoch 15 Batch 117 Loss 2.9008\n",
            "Epoch 15 Batch 120 Loss 2.9008\n",
            "Epoch 15 Batch 123 Loss 2.8992\n",
            "Epoch 15 Batch 126 Loss 2.8995\n",
            "Epoch 15 Batch 129 Loss 2.9002\n",
            "Epoch 15 Batch 132 Loss 2.8997\n",
            "Epoch 15 Batch 135 Loss 2.9003\n",
            "Epoch 15 Batch 138 Loss 2.9017\n",
            "Epoch 15 Batch 141 Loss 2.9031\n",
            "Epoch 15 Batch 144 Loss 2.9059\n",
            "Epoch 15 Batch 147 Loss 2.9053\n",
            "Epoch 15 Batch 150 Loss 2.9060\n",
            "Epoch 15 Batch 153 Loss 2.9078\n",
            "Epoch 15 Batch 156 Loss 2.9081\n",
            "Epoch 15 Batch 159 Loss 2.9090\n",
            "Epoch 15 Batch 162 Loss 2.9089\n",
            "Epoch 15 Batch 165 Loss 2.9098\n",
            "Epoch 15 Batch 168 Loss 2.9108\n",
            "Epoch 15 Batch 171 Loss 2.9091\n",
            "Epoch 15 Batch 174 Loss 2.9097\n",
            "Epoch 15 Batch 177 Loss 2.9102\n",
            "Epoch 15 Batch 180 Loss 2.9085\n",
            "Epoch 15 Batch 183 Loss 2.9090\n",
            "Epoch 15 Batch 186 Loss 2.9081\n",
            "Epoch 15 Batch 189 Loss 2.9094\n",
            "Epoch 15 Batch 192 Loss 2.9096\n",
            "Epoch 15 Batch 195 Loss 2.9099\n",
            "Epoch 15 Batch 198 Loss 2.9110\n",
            "Epoch 15 Batch 201 Loss 2.9113\n",
            "Epoch 15 Batch 204 Loss 2.9109\n",
            "Epoch 15 Batch 207 Loss 2.9110\n",
            "Epoch 15 Batch 210 Loss 2.9107\n",
            "Epoch 15 Batch 213 Loss 2.9126\n",
            "Epoch 15 Batch 216 Loss 2.9127\n",
            "Epoch 15 Batch 219 Loss 2.9126\n",
            "Epoch 15 Batch 222 Loss 2.9141\n",
            "Epoch 15 Batch 225 Loss 2.9159\n",
            "Epoch 15 Batch 228 Loss 2.9164\n",
            "Epoch 15 Batch 231 Loss 2.9166\n",
            "Epoch 15 Batch 234 Loss 2.9158\n",
            "Epoch 15 Batch 237 Loss 2.9162\n",
            "Epoch 15 Batch 240 Loss 2.9156\n",
            "Epoch 15 Batch 243 Loss 2.9166\n",
            "Epoch 15 Batch 246 Loss 2.9175\n",
            "Epoch 15 Batch 249 Loss 2.9180\n",
            "Epoch 15 Batch 252 Loss 2.9190\n",
            "Epoch 15 Batch 255 Loss 2.9202\n",
            "Epoch 15 Batch 258 Loss 2.9201\n",
            "Epoch 15 Batch 261 Loss 2.9217\n",
            "Epoch 15 Batch 264 Loss 2.9222\n",
            "Epoch 15 Batch 267 Loss 2.9226\n",
            "Epoch 15 Batch 270 Loss 2.9248\n",
            "Epoch 15 Batch 273 Loss 2.9250\n",
            "Epoch 15 Batch 276 Loss 2.9250\n",
            "Epoch 15 Batch 279 Loss 2.9247\n",
            "Epoch 15 Batch 282 Loss 2.9244\n",
            "Epoch 15 Batch 285 Loss 2.9234\n",
            "Epoch 15 Batch 288 Loss 2.9237\n",
            "Epoch 15 Batch 291 Loss 2.9232\n",
            "Epoch 15 Batch 294 Loss 2.9242\n",
            "Epoch 15 Batch 297 Loss 2.9252\n",
            "Epoch 15 Batch 300 Loss 2.9252\n",
            "Epoch 15 Batch 303 Loss 2.9244\n",
            "Epoch 15 Batch 306 Loss 2.9247\n",
            "Epoch 15 Batch 309 Loss 2.9248\n",
            "Epoch 15 Batch 312 Loss 2.9250\n",
            "Epoch 15 Batch 315 Loss 2.9242\n",
            "Epoch 15 Batch 318 Loss 2.9243\n",
            "Epoch 15 Batch 321 Loss 2.9247\n",
            "Epoch 15 Batch 324 Loss 2.9246\n",
            "Epoch 15 Batch 327 Loss 2.9249\n",
            "Epoch 15 Batch 330 Loss 2.9254\n",
            "Epoch 15 Batch 333 Loss 2.9265\n",
            "Epoch 15 Batch 336 Loss 2.9271\n",
            "Epoch 15 Batch 339 Loss 2.9275\n",
            "Epoch 15 Batch 342 Loss 2.9285\n",
            "Epoch 15 Batch 345 Loss 2.9283\n",
            "Epoch 15 Batch 348 Loss 2.9277\n",
            "Epoch 15 Batch 351 Loss 2.9287\n",
            "Epoch 15 Batch 354 Loss 2.9280\n",
            "Epoch 15 Batch 357 Loss 2.9290\n",
            "Epoch 15 Batch 360 Loss 2.9289\n",
            "Epoch 15 Batch 363 Loss 2.9289\n",
            "Epoch 15 Batch 366 Loss 2.9284\n",
            "Epoch 15 Batch 369 Loss 2.9280\n",
            "Epoch 15 Batch 372 Loss 2.9281\n",
            "Epoch 15 Batch 375 Loss 2.9276\n",
            "Epoch 15 Batch 378 Loss 2.9261\n",
            "Epoch 15 Batch 381 Loss 2.9262\n",
            "Epoch 15 Batch 384 Loss 2.9258\n",
            "Epoch 15 Batch 387 Loss 2.9257\n",
            "Epoch 15 Batch 390 Loss 2.9251\n",
            "Epoch 15 Batch 393 Loss 2.9253\n",
            "Epoch 15 Batch 396 Loss 2.9254\n",
            "Epoch 15 Batch 399 Loss 2.9254\n",
            "Epoch 15 Batch 402 Loss 2.9253\n",
            "Epoch 15 Batch 405 Loss 2.9253\n",
            "Epoch 15 Batch 408 Loss 2.9246\n",
            "Epoch 15 Batch 411 Loss 2.9243\n",
            "Epoch 15 Batch 414 Loss 2.9246\n",
            "Epoch 15 Batch 417 Loss 2.9245\n",
            "Epoch 15 Batch 420 Loss 2.9241\n",
            "Epoch 15 Batch 423 Loss 2.9243\n",
            "Epoch 15 Batch 426 Loss 2.9245\n",
            "Epoch 15 Batch 429 Loss 2.9239\n",
            "Epoch 15 Batch 432 Loss 2.9242\n",
            "Epoch 15 Batch 435 Loss 2.9241\n",
            "Epoch 15 Batch 438 Loss 2.9241\n",
            "Epoch 15 Batch 441 Loss 2.9238\n",
            "Epoch 15 Batch 444 Loss 2.9234\n",
            "Epoch 15 Batch 447 Loss 2.9239\n",
            "Epoch 15 Batch 450 Loss 2.9243\n",
            "Epoch 15 Batch 453 Loss 2.9236\n",
            "Epoch 15 Batch 456 Loss 2.9231\n",
            "Epoch 15 Batch 459 Loss 2.9231\n",
            "Epoch 15 Batch 462 Loss 2.9231\n",
            "Epoch 15 Batch 465 Loss 2.9230\n",
            "Epoch 15 Batch 468 Loss 2.9227\n",
            "Epoch 15 Batch 471 Loss 2.9231\n",
            "Epoch 15 Batch 474 Loss 2.9232\n",
            "Epoch 15 Batch 477 Loss 2.9230\n",
            "Epoch 15 Batch 480 Loss 2.9233\n",
            "Epoch 15 Batch 483 Loss 2.9226\n",
            "Epoch 15 Batch 486 Loss 2.9224\n",
            "Epoch 15 Batch 489 Loss 2.9223\n",
            "Epoch 15 Batch 492 Loss 2.9223\n",
            "Epoch 15 Batch 495 Loss 2.9214\n",
            "Epoch 15 Batch 498 Loss 2.9208\n",
            "Epoch 15 Batch 501 Loss 2.9203\n",
            "Epoch 15 Batch 504 Loss 2.9201\n",
            "Epoch 15 Batch 507 Loss 2.9198\n",
            "Epoch 15 Batch 510 Loss 2.9196\n",
            "Epoch 15 Batch 513 Loss 2.9195\n",
            "Epoch 15 Batch 516 Loss 2.9200\n",
            "Epoch 15 Batch 519 Loss 2.9200\n",
            "Epoch 15 Batch 522 Loss 2.9193\n",
            "Epoch 15 Batch 525 Loss 2.9195\n",
            "Epoch 15 Batch 528 Loss 2.9199\n",
            "Epoch 15 Batch 531 Loss 2.9204\n",
            "Epoch 15 Batch 534 Loss 2.9209\n",
            "Epoch 15 Batch 537 Loss 2.9207\n",
            "Epoch 15 Batch 540 Loss 2.9205\n",
            "Epoch 15 Batch 543 Loss 2.9212\n",
            "Epoch 15 Batch 546 Loss 2.9209\n",
            "Epoch 15 Batch 549 Loss 2.9203\n",
            "Epoch 15 Batch 552 Loss 2.9201\n",
            "Epoch 15 Batch 555 Loss 2.9198\n",
            "Epoch 15 Batch 558 Loss 2.9201\n",
            "Epoch 15 Batch 561 Loss 2.9196\n",
            "Epoch 15 Batch 564 Loss 2.9196\n",
            "Epoch 15 Batch 567 Loss 2.9199\n",
            "Epoch 15 Batch 570 Loss 2.9203\n",
            "Epoch 15 Batch 573 Loss 2.9204\n",
            "Epoch 15 Batch 576 Loss 2.9207\n",
            "Epoch 15 Batch 579 Loss 2.9208\n",
            "Epoch 15 Batch 582 Loss 2.9210\n",
            "Epoch 15 Batch 585 Loss 2.9208\n",
            "Epoch 15 Batch 588 Loss 2.9215\n",
            "Epoch 15 Batch 591 Loss 2.9213\n",
            "Epoch 15 Batch 594 Loss 2.9209\n",
            "Epoch 15 Batch 597 Loss 2.9208\n",
            "Epoch 15 Batch 600 Loss 2.9210\n",
            "Epoch 15 Batch 603 Loss 2.9208\n",
            "Epoch 15 Batch 606 Loss 2.9201\n",
            "Epoch 15 Batch 609 Loss 2.9203\n",
            "Epoch 15 Batch 612 Loss 2.9201\n",
            "Epoch 15 Batch 615 Loss 2.9202\n",
            "Epoch 15 Batch 618 Loss 2.9201\n",
            "Epoch 15 Batch 621 Loss 2.9199\n",
            "Epoch 15 Batch 624 Loss 2.9205\n",
            "Epoch 15 Batch 627 Loss 2.9203\n",
            "Epoch 15 Batch 630 Loss 2.9210\n",
            "Epoch 15 Batch 633 Loss 2.9205\n",
            "Epoch 15 Batch 636 Loss 2.9209\n",
            "Epoch 15 Batch 639 Loss 2.9207\n",
            "Epoch 15 Batch 642 Loss 2.9213\n",
            "Epoch 15 Batch 645 Loss 2.9214\n",
            "Epoch 15 Batch 648 Loss 2.9216\n",
            "Epoch 15 Batch 651 Loss 2.9218\n",
            "Epoch 15 Batch 654 Loss 2.9222\n",
            "Epoch 15 Batch 657 Loss 2.9222\n",
            "Epoch 15 Batch 660 Loss 2.9227\n",
            "Epoch 15 Batch 663 Loss 2.9236\n",
            "Epoch 15 Batch 666 Loss 2.9242\n",
            "Epoch 15 Batch 669 Loss 2.9246\n",
            "Epoch 15 Batch 672 Loss 2.9245\n",
            "Epoch 15 Batch 675 Loss 2.9245\n",
            "Epoch 15 Batch 678 Loss 2.9244\n",
            "Epoch 15 Batch 681 Loss 2.9244\n",
            "Epoch 15 Batch 684 Loss 2.9246\n",
            "Epoch 15 Batch 687 Loss 2.9251\n",
            "Epoch 15 Batch 690 Loss 2.9251\n",
            "Epoch 15 Batch 693 Loss 2.9256\n",
            "Epoch 15 Batch 696 Loss 2.9255\n",
            "Epoch 15 Batch 699 Loss 2.9258\n",
            "Epoch 15 Batch 702 Loss 2.9264\n",
            "Epoch 15 Batch 705 Loss 2.9268\n",
            "Epoch 15 Batch 708 Loss 2.9277\n",
            "Epoch 15 Batch 711 Loss 2.9279\n",
            "Epoch 15 Batch 714 Loss 2.9283\n",
            "Epoch 15 Batch 717 Loss 2.9288\n",
            "Epoch 15 Batch 720 Loss 2.9291\n",
            "Epoch 15 Batch 723 Loss 2.9292\n",
            "Epoch 15 Batch 726 Loss 2.9292\n",
            "Epoch 15 Batch 729 Loss 2.9291\n",
            "Epoch 15 Batch 732 Loss 2.9294\n",
            "Epoch 15 Batch 735 Loss 2.9300\n",
            "Epoch 15 Batch 738 Loss 2.9296\n",
            "Epoch 15 Batch 741 Loss 2.9298\n",
            "Epoch 15 Batch 744 Loss 2.9305\n",
            "Epoch 15 Batch 747 Loss 2.9305\n",
            "Epoch 15 Batch 750 Loss 2.9312\n",
            "Epoch 15 Batch 753 Loss 2.9315\n",
            "Epoch 15 Batch 756 Loss 2.9323\n",
            "Epoch 15 Batch 759 Loss 2.9327\n",
            "Epoch 15 Batch 762 Loss 2.9327\n",
            "Epoch 15 Batch 765 Loss 2.9326\n",
            "Epoch 15 Batch 768 Loss 2.9334\n",
            "Epoch 15 Batch 771 Loss 2.9339\n",
            "Epoch 15 Batch 774 Loss 2.9336\n",
            "Epoch 15 Batch 777 Loss 2.9339\n",
            "Epoch 15 Batch 780 Loss 2.9343\n",
            "Epoch 15 Batch 783 Loss 2.9347\n",
            "Epoch 15 Batch 786 Loss 2.9350\n",
            "Epoch 15 Batch 789 Loss 2.9354\n",
            "Epoch 15 Batch 792 Loss 2.9360\n",
            "Epoch 15 Batch 795 Loss 2.9367\n",
            "Epoch 15 Batch 798 Loss 2.9372\n",
            "Epoch 15 Batch 801 Loss 2.9382\n",
            "Epoch 15 Batch 804 Loss 2.9388\n",
            "Epoch 15 Batch 807 Loss 2.9390\n",
            "Epoch 15 Batch 810 Loss 2.9394\n",
            "Epoch 15 Batch 813 Loss 2.9401\n",
            "Epoch 15 Batch 816 Loss 2.9403\n",
            "Epoch 15 Batch 819 Loss 2.9408\n",
            "Epoch 15 Batch 822 Loss 2.9411\n",
            "Epoch 15 Batch 825 Loss 2.9414\n",
            "Epoch 15 Batch 828 Loss 2.9415\n",
            "Epoch 15 Batch 831 Loss 2.9420\n",
            "Epoch 15 Batch 834 Loss 2.9423\n",
            "Epoch 15 Batch 837 Loss 2.9425\n",
            "Epoch 15 Batch 840 Loss 2.9426\n",
            "Epoch 15 Batch 843 Loss 2.9432\n",
            "Epoch 15 Batch 846 Loss 2.9432\n",
            "Epoch 15 Batch 849 Loss 2.9437\n",
            "Epoch 15 Batch 852 Loss 2.9442\n",
            "Epoch 15 Batch 855 Loss 2.9445\n",
            "Epoch 15 Batch 858 Loss 2.9447\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
            "Epoch 15 Loss 2.9451\n",
            "Time taken for 1 epoch: 330.27427196502686 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.6646\n",
            "Epoch 16 Batch 3 Loss 2.7556\n",
            "Epoch 16 Batch 6 Loss 2.8321\n",
            "Epoch 16 Batch 9 Loss 2.8153\n",
            "Epoch 16 Batch 12 Loss 2.8164\n",
            "Epoch 16 Batch 15 Loss 2.8047\n",
            "Epoch 16 Batch 18 Loss 2.8199\n",
            "Epoch 16 Batch 21 Loss 2.8202\n",
            "Epoch 16 Batch 24 Loss 2.8079\n",
            "Epoch 16 Batch 27 Loss 2.8026\n",
            "Epoch 16 Batch 30 Loss 2.7954\n",
            "Epoch 16 Batch 33 Loss 2.7928\n",
            "Epoch 16 Batch 36 Loss 2.7902\n",
            "Epoch 16 Batch 39 Loss 2.7802\n",
            "Epoch 16 Batch 42 Loss 2.7834\n",
            "Epoch 16 Batch 45 Loss 2.7826\n",
            "Epoch 16 Batch 48 Loss 2.7804\n",
            "Epoch 16 Batch 51 Loss 2.7847\n",
            "Epoch 16 Batch 54 Loss 2.7849\n",
            "Epoch 16 Batch 57 Loss 2.7854\n",
            "Epoch 16 Batch 60 Loss 2.7842\n",
            "Epoch 16 Batch 63 Loss 2.7820\n",
            "Epoch 16 Batch 66 Loss 2.7794\n",
            "Epoch 16 Batch 69 Loss 2.7791\n",
            "Epoch 16 Batch 72 Loss 2.7773\n",
            "Epoch 16 Batch 75 Loss 2.7787\n",
            "Epoch 16 Batch 78 Loss 2.7773\n",
            "Epoch 16 Batch 81 Loss 2.7775\n",
            "Epoch 16 Batch 84 Loss 2.7779\n",
            "Epoch 16 Batch 87 Loss 2.7814\n",
            "Epoch 16 Batch 90 Loss 2.7782\n",
            "Epoch 16 Batch 93 Loss 2.7813\n",
            "Epoch 16 Batch 96 Loss 2.7800\n",
            "Epoch 16 Batch 99 Loss 2.7823\n",
            "Epoch 16 Batch 102 Loss 2.7842\n",
            "Epoch 16 Batch 105 Loss 2.7854\n",
            "Epoch 16 Batch 108 Loss 2.7889\n",
            "Epoch 16 Batch 111 Loss 2.7893\n",
            "Epoch 16 Batch 114 Loss 2.7902\n",
            "Epoch 16 Batch 117 Loss 2.7924\n",
            "Epoch 16 Batch 120 Loss 2.7906\n",
            "Epoch 16 Batch 123 Loss 2.7910\n",
            "Epoch 16 Batch 126 Loss 2.7931\n",
            "Epoch 16 Batch 129 Loss 2.7938\n",
            "Epoch 16 Batch 132 Loss 2.7961\n",
            "Epoch 16 Batch 135 Loss 2.7984\n",
            "Epoch 16 Batch 138 Loss 2.7984\n",
            "Epoch 16 Batch 141 Loss 2.7976\n",
            "Epoch 16 Batch 144 Loss 2.7995\n",
            "Epoch 16 Batch 147 Loss 2.8011\n",
            "Epoch 16 Batch 150 Loss 2.8031\n",
            "Epoch 16 Batch 153 Loss 2.8047\n",
            "Epoch 16 Batch 156 Loss 2.8047\n",
            "Epoch 16 Batch 159 Loss 2.8031\n",
            "Epoch 16 Batch 162 Loss 2.8018\n",
            "Epoch 16 Batch 165 Loss 2.8006\n",
            "Epoch 16 Batch 168 Loss 2.8023\n",
            "Epoch 16 Batch 171 Loss 2.8054\n",
            "Epoch 16 Batch 174 Loss 2.8047\n",
            "Epoch 16 Batch 177 Loss 2.8067\n",
            "Epoch 16 Batch 180 Loss 2.8065\n",
            "Epoch 16 Batch 183 Loss 2.8104\n",
            "Epoch 16 Batch 186 Loss 2.8104\n",
            "Epoch 16 Batch 189 Loss 2.8097\n",
            "Epoch 16 Batch 192 Loss 2.8104\n",
            "Epoch 16 Batch 195 Loss 2.8089\n",
            "Epoch 16 Batch 198 Loss 2.8080\n",
            "Epoch 16 Batch 201 Loss 2.8065\n",
            "Epoch 16 Batch 204 Loss 2.8074\n",
            "Epoch 16 Batch 207 Loss 2.8085\n",
            "Epoch 16 Batch 210 Loss 2.8089\n",
            "Epoch 16 Batch 213 Loss 2.8089\n",
            "Epoch 16 Batch 216 Loss 2.8106\n",
            "Epoch 16 Batch 219 Loss 2.8111\n",
            "Epoch 16 Batch 222 Loss 2.8101\n",
            "Epoch 16 Batch 225 Loss 2.8088\n",
            "Epoch 16 Batch 228 Loss 2.8097\n",
            "Epoch 16 Batch 231 Loss 2.8099\n",
            "Epoch 16 Batch 234 Loss 2.8113\n",
            "Epoch 16 Batch 237 Loss 2.8120\n",
            "Epoch 16 Batch 240 Loss 2.8123\n",
            "Epoch 16 Batch 243 Loss 2.8135\n",
            "Epoch 16 Batch 246 Loss 2.8146\n",
            "Epoch 16 Batch 249 Loss 2.8138\n",
            "Epoch 16 Batch 252 Loss 2.8142\n",
            "Epoch 16 Batch 255 Loss 2.8154\n",
            "Epoch 16 Batch 258 Loss 2.8148\n",
            "Epoch 16 Batch 261 Loss 2.8139\n",
            "Epoch 16 Batch 264 Loss 2.8142\n",
            "Epoch 16 Batch 267 Loss 2.8144\n",
            "Epoch 16 Batch 270 Loss 2.8133\n",
            "Epoch 16 Batch 273 Loss 2.8124\n",
            "Epoch 16 Batch 276 Loss 2.8117\n",
            "Epoch 16 Batch 279 Loss 2.8130\n",
            "Epoch 16 Batch 282 Loss 2.8129\n",
            "Epoch 16 Batch 285 Loss 2.8132\n",
            "Epoch 16 Batch 288 Loss 2.8139\n",
            "Epoch 16 Batch 291 Loss 2.8139\n",
            "Epoch 16 Batch 294 Loss 2.8142\n",
            "Epoch 16 Batch 297 Loss 2.8143\n",
            "Epoch 16 Batch 300 Loss 2.8142\n",
            "Epoch 16 Batch 303 Loss 2.8144\n",
            "Epoch 16 Batch 306 Loss 2.8142\n",
            "Epoch 16 Batch 309 Loss 2.8154\n",
            "Epoch 16 Batch 312 Loss 2.8161\n",
            "Epoch 16 Batch 315 Loss 2.8173\n",
            "Epoch 16 Batch 318 Loss 2.8176\n",
            "Epoch 16 Batch 321 Loss 2.8190\n",
            "Epoch 16 Batch 324 Loss 2.8191\n",
            "Epoch 16 Batch 327 Loss 2.8191\n",
            "Epoch 16 Batch 330 Loss 2.8192\n",
            "Epoch 16 Batch 333 Loss 2.8205\n",
            "Epoch 16 Batch 336 Loss 2.8203\n",
            "Epoch 16 Batch 339 Loss 2.8209\n",
            "Epoch 16 Batch 342 Loss 2.8208\n",
            "Epoch 16 Batch 345 Loss 2.8210\n",
            "Epoch 16 Batch 348 Loss 2.8212\n",
            "Epoch 16 Batch 351 Loss 2.8221\n",
            "Epoch 16 Batch 354 Loss 2.8225\n",
            "Epoch 16 Batch 357 Loss 2.8222\n",
            "Epoch 16 Batch 360 Loss 2.8216\n",
            "Epoch 16 Batch 363 Loss 2.8222\n",
            "Epoch 16 Batch 366 Loss 2.8220\n",
            "Epoch 16 Batch 369 Loss 2.8224\n",
            "Epoch 16 Batch 372 Loss 2.8223\n",
            "Epoch 16 Batch 375 Loss 2.8222\n",
            "Epoch 16 Batch 378 Loss 2.8213\n",
            "Epoch 16 Batch 381 Loss 2.8206\n",
            "Epoch 16 Batch 384 Loss 2.8201\n",
            "Epoch 16 Batch 387 Loss 2.8202\n",
            "Epoch 16 Batch 390 Loss 2.8203\n",
            "Epoch 16 Batch 393 Loss 2.8202\n",
            "Epoch 16 Batch 396 Loss 2.8207\n",
            "Epoch 16 Batch 399 Loss 2.8210\n",
            "Epoch 16 Batch 402 Loss 2.8208\n",
            "Epoch 16 Batch 405 Loss 2.8203\n",
            "Epoch 16 Batch 408 Loss 2.8203\n",
            "Epoch 16 Batch 411 Loss 2.8207\n",
            "Epoch 16 Batch 414 Loss 2.8204\n",
            "Epoch 16 Batch 417 Loss 2.8199\n",
            "Epoch 16 Batch 420 Loss 2.8203\n",
            "Epoch 16 Batch 423 Loss 2.8200\n",
            "Epoch 16 Batch 426 Loss 2.8205\n",
            "Epoch 16 Batch 429 Loss 2.8205\n",
            "Epoch 16 Batch 432 Loss 2.8207\n",
            "Epoch 16 Batch 435 Loss 2.8197\n",
            "Epoch 16 Batch 438 Loss 2.8201\n",
            "Epoch 16 Batch 441 Loss 2.8195\n",
            "Epoch 16 Batch 444 Loss 2.8197\n",
            "Epoch 16 Batch 447 Loss 2.8201\n",
            "Epoch 16 Batch 450 Loss 2.8196\n",
            "Epoch 16 Batch 453 Loss 2.8204\n",
            "Epoch 16 Batch 456 Loss 2.8199\n",
            "Epoch 16 Batch 459 Loss 2.8206\n",
            "Epoch 16 Batch 462 Loss 2.8200\n",
            "Epoch 16 Batch 465 Loss 2.8202\n",
            "Epoch 16 Batch 468 Loss 2.8203\n",
            "Epoch 16 Batch 471 Loss 2.8204\n",
            "Epoch 16 Batch 474 Loss 2.8208\n",
            "Epoch 16 Batch 477 Loss 2.8211\n",
            "Epoch 16 Batch 480 Loss 2.8219\n",
            "Epoch 16 Batch 483 Loss 2.8231\n",
            "Epoch 16 Batch 486 Loss 2.8237\n",
            "Epoch 16 Batch 489 Loss 2.8238\n",
            "Epoch 16 Batch 492 Loss 2.8239\n",
            "Epoch 16 Batch 495 Loss 2.8238\n",
            "Epoch 16 Batch 498 Loss 2.8233\n",
            "Epoch 16 Batch 501 Loss 2.8225\n",
            "Epoch 16 Batch 504 Loss 2.8236\n",
            "Epoch 16 Batch 507 Loss 2.8243\n",
            "Epoch 16 Batch 510 Loss 2.8251\n",
            "Epoch 16 Batch 513 Loss 2.8246\n",
            "Epoch 16 Batch 516 Loss 2.8245\n",
            "Epoch 16 Batch 519 Loss 2.8248\n",
            "Epoch 16 Batch 522 Loss 2.8250\n",
            "Epoch 16 Batch 525 Loss 2.8251\n",
            "Epoch 16 Batch 528 Loss 2.8252\n",
            "Epoch 16 Batch 531 Loss 2.8249\n",
            "Epoch 16 Batch 534 Loss 2.8249\n",
            "Epoch 16 Batch 537 Loss 2.8251\n",
            "Epoch 16 Batch 540 Loss 2.8249\n",
            "Epoch 16 Batch 543 Loss 2.8253\n",
            "Epoch 16 Batch 546 Loss 2.8255\n",
            "Epoch 16 Batch 549 Loss 2.8253\n",
            "Epoch 16 Batch 552 Loss 2.8262\n",
            "Epoch 16 Batch 555 Loss 2.8259\n",
            "Epoch 16 Batch 558 Loss 2.8268\n",
            "Epoch 16 Batch 561 Loss 2.8266\n",
            "Epoch 16 Batch 564 Loss 2.8262\n",
            "Epoch 16 Batch 567 Loss 2.8260\n",
            "Epoch 16 Batch 570 Loss 2.8258\n",
            "Epoch 16 Batch 573 Loss 2.8263\n",
            "Epoch 16 Batch 576 Loss 2.8265\n",
            "Epoch 16 Batch 579 Loss 2.8264\n",
            "Epoch 16 Batch 582 Loss 2.8261\n",
            "Epoch 16 Batch 585 Loss 2.8259\n",
            "Epoch 16 Batch 588 Loss 2.8260\n",
            "Epoch 16 Batch 591 Loss 2.8263\n",
            "Epoch 16 Batch 594 Loss 2.8263\n",
            "Epoch 16 Batch 597 Loss 2.8260\n",
            "Epoch 16 Batch 600 Loss 2.8259\n",
            "Epoch 16 Batch 603 Loss 2.8267\n",
            "Epoch 16 Batch 606 Loss 2.8264\n",
            "Epoch 16 Batch 609 Loss 2.8264\n",
            "Epoch 16 Batch 612 Loss 2.8265\n",
            "Epoch 16 Batch 615 Loss 2.8268\n",
            "Epoch 16 Batch 618 Loss 2.8268\n",
            "Epoch 16 Batch 621 Loss 2.8272\n",
            "Epoch 16 Batch 624 Loss 2.8272\n",
            "Epoch 16 Batch 627 Loss 2.8267\n",
            "Epoch 16 Batch 630 Loss 2.8274\n",
            "Epoch 16 Batch 633 Loss 2.8275\n",
            "Epoch 16 Batch 636 Loss 2.8269\n",
            "Epoch 16 Batch 639 Loss 2.8268\n",
            "Epoch 16 Batch 642 Loss 2.8270\n",
            "Epoch 16 Batch 645 Loss 2.8267\n",
            "Epoch 16 Batch 648 Loss 2.8272\n",
            "Epoch 16 Batch 651 Loss 2.8276\n",
            "Epoch 16 Batch 654 Loss 2.8277\n",
            "Epoch 16 Batch 657 Loss 2.8276\n",
            "Epoch 16 Batch 660 Loss 2.8275\n",
            "Epoch 16 Batch 663 Loss 2.8280\n",
            "Epoch 16 Batch 666 Loss 2.8282\n",
            "Epoch 16 Batch 669 Loss 2.8287\n",
            "Epoch 16 Batch 672 Loss 2.8290\n",
            "Epoch 16 Batch 675 Loss 2.8300\n",
            "Epoch 16 Batch 678 Loss 2.8305\n",
            "Epoch 16 Batch 681 Loss 2.8313\n",
            "Epoch 16 Batch 684 Loss 2.8315\n",
            "Epoch 16 Batch 687 Loss 2.8322\n",
            "Epoch 16 Batch 690 Loss 2.8326\n",
            "Epoch 16 Batch 693 Loss 2.8328\n",
            "Epoch 16 Batch 696 Loss 2.8326\n",
            "Epoch 16 Batch 699 Loss 2.8327\n",
            "Epoch 16 Batch 702 Loss 2.8329\n",
            "Epoch 16 Batch 705 Loss 2.8329\n",
            "Epoch 16 Batch 708 Loss 2.8329\n",
            "Epoch 16 Batch 711 Loss 2.8333\n",
            "Epoch 16 Batch 714 Loss 2.8335\n",
            "Epoch 16 Batch 717 Loss 2.8341\n",
            "Epoch 16 Batch 720 Loss 2.8343\n",
            "Epoch 16 Batch 723 Loss 2.8346\n",
            "Epoch 16 Batch 726 Loss 2.8348\n",
            "Epoch 16 Batch 729 Loss 2.8354\n",
            "Epoch 16 Batch 732 Loss 2.8356\n",
            "Epoch 16 Batch 735 Loss 2.8358\n",
            "Epoch 16 Batch 738 Loss 2.8366\n",
            "Epoch 16 Batch 741 Loss 2.8364\n",
            "Epoch 16 Batch 744 Loss 2.8366\n",
            "Epoch 16 Batch 747 Loss 2.8370\n",
            "Epoch 16 Batch 750 Loss 2.8373\n",
            "Epoch 16 Batch 753 Loss 2.8369\n",
            "Epoch 16 Batch 756 Loss 2.8371\n",
            "Epoch 16 Batch 759 Loss 2.8371\n",
            "Epoch 16 Batch 762 Loss 2.8371\n",
            "Epoch 16 Batch 765 Loss 2.8377\n",
            "Epoch 16 Batch 768 Loss 2.8383\n",
            "Epoch 16 Batch 771 Loss 2.8384\n",
            "Epoch 16 Batch 774 Loss 2.8386\n",
            "Epoch 16 Batch 777 Loss 2.8393\n",
            "Epoch 16 Batch 780 Loss 2.8395\n",
            "Epoch 16 Batch 783 Loss 2.8397\n",
            "Epoch 16 Batch 786 Loss 2.8403\n",
            "Epoch 16 Batch 789 Loss 2.8404\n",
            "Epoch 16 Batch 792 Loss 2.8408\n",
            "Epoch 16 Batch 795 Loss 2.8409\n",
            "Epoch 16 Batch 798 Loss 2.8414\n",
            "Epoch 16 Batch 801 Loss 2.8421\n",
            "Epoch 16 Batch 804 Loss 2.8425\n",
            "Epoch 16 Batch 807 Loss 2.8432\n",
            "Epoch 16 Batch 810 Loss 2.8432\n",
            "Epoch 16 Batch 813 Loss 2.8438\n",
            "Epoch 16 Batch 816 Loss 2.8438\n",
            "Epoch 16 Batch 819 Loss 2.8446\n",
            "Epoch 16 Batch 822 Loss 2.8448\n",
            "Epoch 16 Batch 825 Loss 2.8450\n",
            "Epoch 16 Batch 828 Loss 2.8449\n",
            "Epoch 16 Batch 831 Loss 2.8453\n",
            "Epoch 16 Batch 834 Loss 2.8461\n",
            "Epoch 16 Batch 837 Loss 2.8460\n",
            "Epoch 16 Batch 840 Loss 2.8461\n",
            "Epoch 16 Batch 843 Loss 2.8464\n",
            "Epoch 16 Batch 846 Loss 2.8468\n",
            "Epoch 16 Batch 849 Loss 2.8475\n",
            "Epoch 16 Batch 852 Loss 2.8480\n",
            "Epoch 16 Batch 855 Loss 2.8483\n",
            "Epoch 16 Batch 858 Loss 2.8482\n",
            "Epoch 16 Loss 2.8483\n",
            "Time taken for 1 epoch: 329.53455233573914 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.6743\n",
            "Epoch 17 Batch 3 Loss 2.6941\n",
            "Epoch 17 Batch 6 Loss 2.6920\n",
            "Epoch 17 Batch 9 Loss 2.6955\n",
            "Epoch 17 Batch 12 Loss 2.6941\n",
            "Epoch 17 Batch 15 Loss 2.6820\n",
            "Epoch 17 Batch 18 Loss 2.6785\n",
            "Epoch 17 Batch 21 Loss 2.6737\n",
            "Epoch 17 Batch 24 Loss 2.6686\n",
            "Epoch 17 Batch 27 Loss 2.6733\n",
            "Epoch 17 Batch 30 Loss 2.6782\n",
            "Epoch 17 Batch 33 Loss 2.6721\n",
            "Epoch 17 Batch 36 Loss 2.6655\n",
            "Epoch 17 Batch 39 Loss 2.6746\n",
            "Epoch 17 Batch 42 Loss 2.6737\n",
            "Epoch 17 Batch 45 Loss 2.6712\n",
            "Epoch 17 Batch 48 Loss 2.6711\n",
            "Epoch 17 Batch 51 Loss 2.6672\n",
            "Epoch 17 Batch 54 Loss 2.6761\n",
            "Epoch 17 Batch 57 Loss 2.6731\n",
            "Epoch 17 Batch 60 Loss 2.6736\n",
            "Epoch 17 Batch 63 Loss 2.6718\n",
            "Epoch 17 Batch 66 Loss 2.6677\n",
            "Epoch 17 Batch 69 Loss 2.6721\n",
            "Epoch 17 Batch 72 Loss 2.6732\n",
            "Epoch 17 Batch 75 Loss 2.6741\n",
            "Epoch 17 Batch 78 Loss 2.6696\n",
            "Epoch 17 Batch 81 Loss 2.6717\n",
            "Epoch 17 Batch 84 Loss 2.6734\n",
            "Epoch 17 Batch 87 Loss 2.6710\n",
            "Epoch 17 Batch 90 Loss 2.6702\n",
            "Epoch 17 Batch 93 Loss 2.6698\n",
            "Epoch 17 Batch 96 Loss 2.6706\n",
            "Epoch 17 Batch 99 Loss 2.6705\n",
            "Epoch 17 Batch 102 Loss 2.6714\n",
            "Epoch 17 Batch 105 Loss 2.6726\n",
            "Epoch 17 Batch 108 Loss 2.6748\n",
            "Epoch 17 Batch 111 Loss 2.6770\n",
            "Epoch 17 Batch 114 Loss 2.6795\n",
            "Epoch 17 Batch 117 Loss 2.6807\n",
            "Epoch 17 Batch 120 Loss 2.6817\n",
            "Epoch 17 Batch 123 Loss 2.6835\n",
            "Epoch 17 Batch 126 Loss 2.6847\n",
            "Epoch 17 Batch 129 Loss 2.6864\n",
            "Epoch 17 Batch 132 Loss 2.6868\n",
            "Epoch 17 Batch 135 Loss 2.6867\n",
            "Epoch 17 Batch 138 Loss 2.6873\n",
            "Epoch 17 Batch 141 Loss 2.6872\n",
            "Epoch 17 Batch 144 Loss 2.6895\n",
            "Epoch 17 Batch 147 Loss 2.6897\n",
            "Epoch 17 Batch 150 Loss 2.6925\n",
            "Epoch 17 Batch 153 Loss 2.6953\n",
            "Epoch 17 Batch 156 Loss 2.6970\n",
            "Epoch 17 Batch 159 Loss 2.6986\n",
            "Epoch 17 Batch 162 Loss 2.6998\n",
            "Epoch 17 Batch 165 Loss 2.6988\n",
            "Epoch 17 Batch 168 Loss 2.6997\n",
            "Epoch 17 Batch 171 Loss 2.6996\n",
            "Epoch 17 Batch 174 Loss 2.6989\n",
            "Epoch 17 Batch 177 Loss 2.7002\n",
            "Epoch 17 Batch 180 Loss 2.6996\n",
            "Epoch 17 Batch 183 Loss 2.6990\n",
            "Epoch 17 Batch 186 Loss 2.7012\n",
            "Epoch 17 Batch 189 Loss 2.7004\n",
            "Epoch 17 Batch 192 Loss 2.6999\n",
            "Epoch 17 Batch 195 Loss 2.6998\n",
            "Epoch 17 Batch 198 Loss 2.7018\n",
            "Epoch 17 Batch 201 Loss 2.7021\n",
            "Epoch 17 Batch 204 Loss 2.7015\n",
            "Epoch 17 Batch 207 Loss 2.7011\n",
            "Epoch 17 Batch 210 Loss 2.7005\n",
            "Epoch 17 Batch 213 Loss 2.7006\n",
            "Epoch 17 Batch 216 Loss 2.7007\n",
            "Epoch 17 Batch 219 Loss 2.7012\n",
            "Epoch 17 Batch 222 Loss 2.7012\n",
            "Epoch 17 Batch 225 Loss 2.7018\n",
            "Epoch 17 Batch 228 Loss 2.7017\n",
            "Epoch 17 Batch 231 Loss 2.7019\n",
            "Epoch 17 Batch 234 Loss 2.7030\n",
            "Epoch 17 Batch 237 Loss 2.7051\n",
            "Epoch 17 Batch 240 Loss 2.7065\n",
            "Epoch 17 Batch 243 Loss 2.7093\n",
            "Epoch 17 Batch 246 Loss 2.7089\n",
            "Epoch 17 Batch 249 Loss 2.7090\n",
            "Epoch 17 Batch 252 Loss 2.7090\n",
            "Epoch 17 Batch 255 Loss 2.7087\n",
            "Epoch 17 Batch 258 Loss 2.7099\n",
            "Epoch 17 Batch 261 Loss 2.7109\n",
            "Epoch 17 Batch 264 Loss 2.7104\n",
            "Epoch 17 Batch 267 Loss 2.7121\n",
            "Epoch 17 Batch 270 Loss 2.7123\n",
            "Epoch 17 Batch 273 Loss 2.7151\n",
            "Epoch 17 Batch 276 Loss 2.7159\n",
            "Epoch 17 Batch 279 Loss 2.7151\n",
            "Epoch 17 Batch 282 Loss 2.7161\n",
            "Epoch 17 Batch 285 Loss 2.7153\n",
            "Epoch 17 Batch 288 Loss 2.7155\n",
            "Epoch 17 Batch 291 Loss 2.7162\n",
            "Epoch 17 Batch 294 Loss 2.7165\n",
            "Epoch 17 Batch 297 Loss 2.7185\n",
            "Epoch 17 Batch 300 Loss 2.7184\n",
            "Epoch 17 Batch 303 Loss 2.7190\n",
            "Epoch 17 Batch 306 Loss 2.7189\n",
            "Epoch 17 Batch 309 Loss 2.7190\n",
            "Epoch 17 Batch 312 Loss 2.7206\n",
            "Epoch 17 Batch 315 Loss 2.7213\n",
            "Epoch 17 Batch 318 Loss 2.7205\n",
            "Epoch 17 Batch 321 Loss 2.7214\n",
            "Epoch 17 Batch 324 Loss 2.7213\n",
            "Epoch 17 Batch 327 Loss 2.7218\n",
            "Epoch 17 Batch 330 Loss 2.7220\n",
            "Epoch 17 Batch 333 Loss 2.7228\n",
            "Epoch 17 Batch 336 Loss 2.7231\n",
            "Epoch 17 Batch 339 Loss 2.7227\n",
            "Epoch 17 Batch 342 Loss 2.7234\n",
            "Epoch 17 Batch 345 Loss 2.7228\n",
            "Epoch 17 Batch 348 Loss 2.7228\n",
            "Epoch 17 Batch 351 Loss 2.7230\n",
            "Epoch 17 Batch 354 Loss 2.7231\n",
            "Epoch 17 Batch 357 Loss 2.7231\n",
            "Epoch 17 Batch 360 Loss 2.7241\n",
            "Epoch 17 Batch 363 Loss 2.7241\n",
            "Epoch 17 Batch 366 Loss 2.7241\n",
            "Epoch 17 Batch 369 Loss 2.7237\n",
            "Epoch 17 Batch 372 Loss 2.7246\n",
            "Epoch 17 Batch 375 Loss 2.7250\n",
            "Epoch 17 Batch 378 Loss 2.7247\n",
            "Epoch 17 Batch 381 Loss 2.7254\n",
            "Epoch 17 Batch 384 Loss 2.7256\n",
            "Epoch 17 Batch 387 Loss 2.7256\n",
            "Epoch 17 Batch 390 Loss 2.7258\n",
            "Epoch 17 Batch 393 Loss 2.7245\n",
            "Epoch 17 Batch 396 Loss 2.7243\n",
            "Epoch 17 Batch 399 Loss 2.7238\n",
            "Epoch 17 Batch 402 Loss 2.7243\n",
            "Epoch 17 Batch 405 Loss 2.7249\n",
            "Epoch 17 Batch 408 Loss 2.7252\n",
            "Epoch 17 Batch 411 Loss 2.7249\n",
            "Epoch 17 Batch 414 Loss 2.7243\n",
            "Epoch 17 Batch 417 Loss 2.7247\n",
            "Epoch 17 Batch 420 Loss 2.7249\n",
            "Epoch 17 Batch 423 Loss 2.7254\n",
            "Epoch 17 Batch 426 Loss 2.7253\n",
            "Epoch 17 Batch 429 Loss 2.7262\n",
            "Epoch 17 Batch 432 Loss 2.7256\n",
            "Epoch 17 Batch 435 Loss 2.7259\n",
            "Epoch 17 Batch 438 Loss 2.7265\n",
            "Epoch 17 Batch 441 Loss 2.7263\n",
            "Epoch 17 Batch 444 Loss 2.7260\n",
            "Epoch 17 Batch 447 Loss 2.7267\n",
            "Epoch 17 Batch 450 Loss 2.7266\n",
            "Epoch 17 Batch 453 Loss 2.7261\n",
            "Epoch 17 Batch 456 Loss 2.7264\n",
            "Epoch 17 Batch 459 Loss 2.7267\n",
            "Epoch 17 Batch 462 Loss 2.7265\n",
            "Epoch 17 Batch 465 Loss 2.7268\n",
            "Epoch 17 Batch 468 Loss 2.7274\n",
            "Epoch 17 Batch 471 Loss 2.7272\n",
            "Epoch 17 Batch 474 Loss 2.7267\n",
            "Epoch 17 Batch 477 Loss 2.7275\n",
            "Epoch 17 Batch 480 Loss 2.7270\n",
            "Epoch 17 Batch 483 Loss 2.7270\n",
            "Epoch 17 Batch 486 Loss 2.7273\n",
            "Epoch 17 Batch 489 Loss 2.7274\n",
            "Epoch 17 Batch 492 Loss 2.7274\n",
            "Epoch 17 Batch 495 Loss 2.7272\n",
            "Epoch 17 Batch 498 Loss 2.7269\n",
            "Epoch 17 Batch 501 Loss 2.7273\n",
            "Epoch 17 Batch 504 Loss 2.7276\n",
            "Epoch 17 Batch 507 Loss 2.7278\n",
            "Epoch 17 Batch 510 Loss 2.7272\n",
            "Epoch 17 Batch 513 Loss 2.7263\n",
            "Epoch 17 Batch 516 Loss 2.7261\n",
            "Epoch 17 Batch 519 Loss 2.7265\n",
            "Epoch 17 Batch 522 Loss 2.7265\n",
            "Epoch 17 Batch 525 Loss 2.7266\n",
            "Epoch 17 Batch 528 Loss 2.7273\n",
            "Epoch 17 Batch 531 Loss 2.7280\n",
            "Epoch 17 Batch 534 Loss 2.7279\n",
            "Epoch 17 Batch 537 Loss 2.7275\n",
            "Epoch 17 Batch 540 Loss 2.7277\n",
            "Epoch 17 Batch 543 Loss 2.7279\n",
            "Epoch 17 Batch 546 Loss 2.7280\n",
            "Epoch 17 Batch 549 Loss 2.7281\n",
            "Epoch 17 Batch 552 Loss 2.7283\n",
            "Epoch 17 Batch 555 Loss 2.7283\n",
            "Epoch 17 Batch 558 Loss 2.7280\n",
            "Epoch 17 Batch 561 Loss 2.7283\n",
            "Epoch 17 Batch 564 Loss 2.7282\n",
            "Epoch 17 Batch 567 Loss 2.7282\n",
            "Epoch 17 Batch 570 Loss 2.7281\n",
            "Epoch 17 Batch 573 Loss 2.7277\n",
            "Epoch 17 Batch 576 Loss 2.7281\n",
            "Epoch 17 Batch 579 Loss 2.7283\n",
            "Epoch 17 Batch 582 Loss 2.7284\n",
            "Epoch 17 Batch 585 Loss 2.7289\n",
            "Epoch 17 Batch 588 Loss 2.7291\n",
            "Epoch 17 Batch 591 Loss 2.7297\n",
            "Epoch 17 Batch 594 Loss 2.7293\n",
            "Epoch 17 Batch 597 Loss 2.7296\n",
            "Epoch 17 Batch 600 Loss 2.7298\n",
            "Epoch 17 Batch 603 Loss 2.7297\n",
            "Epoch 17 Batch 606 Loss 2.7291\n",
            "Epoch 17 Batch 609 Loss 2.7292\n",
            "Epoch 17 Batch 612 Loss 2.7298\n",
            "Epoch 17 Batch 615 Loss 2.7304\n",
            "Epoch 17 Batch 618 Loss 2.7303\n",
            "Epoch 17 Batch 621 Loss 2.7310\n",
            "Epoch 17 Batch 624 Loss 2.7320\n",
            "Epoch 17 Batch 627 Loss 2.7323\n",
            "Epoch 17 Batch 630 Loss 2.7324\n",
            "Epoch 17 Batch 633 Loss 2.7323\n",
            "Epoch 17 Batch 636 Loss 2.7324\n",
            "Epoch 17 Batch 639 Loss 2.7326\n",
            "Epoch 17 Batch 642 Loss 2.7328\n",
            "Epoch 17 Batch 645 Loss 2.7331\n",
            "Epoch 17 Batch 648 Loss 2.7333\n",
            "Epoch 17 Batch 651 Loss 2.7335\n",
            "Epoch 17 Batch 654 Loss 2.7334\n",
            "Epoch 17 Batch 657 Loss 2.7337\n",
            "Epoch 17 Batch 660 Loss 2.7343\n",
            "Epoch 17 Batch 663 Loss 2.7341\n",
            "Epoch 17 Batch 666 Loss 2.7336\n",
            "Epoch 17 Batch 669 Loss 2.7346\n",
            "Epoch 17 Batch 672 Loss 2.7349\n",
            "Epoch 17 Batch 675 Loss 2.7352\n",
            "Epoch 17 Batch 678 Loss 2.7345\n",
            "Epoch 17 Batch 681 Loss 2.7351\n",
            "Epoch 17 Batch 684 Loss 2.7358\n",
            "Epoch 17 Batch 687 Loss 2.7360\n",
            "Epoch 17 Batch 690 Loss 2.7369\n",
            "Epoch 17 Batch 693 Loss 2.7376\n",
            "Epoch 17 Batch 696 Loss 2.7379\n",
            "Epoch 17 Batch 699 Loss 2.7388\n",
            "Epoch 17 Batch 702 Loss 2.7394\n",
            "Epoch 17 Batch 705 Loss 2.7398\n",
            "Epoch 17 Batch 708 Loss 2.7400\n",
            "Epoch 17 Batch 711 Loss 2.7400\n",
            "Epoch 17 Batch 714 Loss 2.7400\n",
            "Epoch 17 Batch 717 Loss 2.7406\n",
            "Epoch 17 Batch 720 Loss 2.7406\n",
            "Epoch 17 Batch 723 Loss 2.7406\n",
            "Epoch 17 Batch 726 Loss 2.7415\n",
            "Epoch 17 Batch 729 Loss 2.7413\n",
            "Epoch 17 Batch 732 Loss 2.7414\n",
            "Epoch 17 Batch 735 Loss 2.7418\n",
            "Epoch 17 Batch 738 Loss 2.7420\n",
            "Epoch 17 Batch 741 Loss 2.7428\n",
            "Epoch 17 Batch 744 Loss 2.7433\n",
            "Epoch 17 Batch 747 Loss 2.7439\n",
            "Epoch 17 Batch 750 Loss 2.7441\n",
            "Epoch 17 Batch 753 Loss 2.7446\n",
            "Epoch 17 Batch 756 Loss 2.7451\n",
            "Epoch 17 Batch 759 Loss 2.7455\n",
            "Epoch 17 Batch 762 Loss 2.7458\n",
            "Epoch 17 Batch 765 Loss 2.7457\n",
            "Epoch 17 Batch 768 Loss 2.7460\n",
            "Epoch 17 Batch 771 Loss 2.7461\n",
            "Epoch 17 Batch 774 Loss 2.7465\n",
            "Epoch 17 Batch 777 Loss 2.7468\n",
            "Epoch 17 Batch 780 Loss 2.7475\n",
            "Epoch 17 Batch 783 Loss 2.7478\n",
            "Epoch 17 Batch 786 Loss 2.7483\n",
            "Epoch 17 Batch 789 Loss 2.7483\n",
            "Epoch 17 Batch 792 Loss 2.7485\n",
            "Epoch 17 Batch 795 Loss 2.7489\n",
            "Epoch 17 Batch 798 Loss 2.7493\n",
            "Epoch 17 Batch 801 Loss 2.7494\n",
            "Epoch 17 Batch 804 Loss 2.7498\n",
            "Epoch 17 Batch 807 Loss 2.7502\n",
            "Epoch 17 Batch 810 Loss 2.7506\n",
            "Epoch 17 Batch 813 Loss 2.7515\n",
            "Epoch 17 Batch 816 Loss 2.7516\n",
            "Epoch 17 Batch 819 Loss 2.7527\n",
            "Epoch 17 Batch 822 Loss 2.7533\n",
            "Epoch 17 Batch 825 Loss 2.7537\n",
            "Epoch 17 Batch 828 Loss 2.7536\n",
            "Epoch 17 Batch 831 Loss 2.7539\n",
            "Epoch 17 Batch 834 Loss 2.7539\n",
            "Epoch 17 Batch 837 Loss 2.7545\n",
            "Epoch 17 Batch 840 Loss 2.7548\n",
            "Epoch 17 Batch 843 Loss 2.7553\n",
            "Epoch 17 Batch 846 Loss 2.7554\n",
            "Epoch 17 Batch 849 Loss 2.7555\n",
            "Epoch 17 Batch 852 Loss 2.7557\n",
            "Epoch 17 Batch 855 Loss 2.7565\n",
            "Epoch 17 Batch 858 Loss 2.7572\n",
            "Epoch 17 Loss 2.7572\n",
            "Time taken for 1 epoch: 329.50042510032654 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.5401\n",
            "Epoch 18 Batch 3 Loss 2.5415\n",
            "Epoch 18 Batch 6 Loss 2.5629\n",
            "Epoch 18 Batch 9 Loss 2.5732\n",
            "Epoch 18 Batch 12 Loss 2.5808\n",
            "Epoch 18 Batch 15 Loss 2.5980\n",
            "Epoch 18 Batch 18 Loss 2.6172\n",
            "Epoch 18 Batch 21 Loss 2.6082\n",
            "Epoch 18 Batch 24 Loss 2.6050\n",
            "Epoch 18 Batch 27 Loss 2.6010\n",
            "Epoch 18 Batch 30 Loss 2.5979\n",
            "Epoch 18 Batch 33 Loss 2.6033\n",
            "Epoch 18 Batch 36 Loss 2.5967\n",
            "Epoch 18 Batch 39 Loss 2.6020\n",
            "Epoch 18 Batch 42 Loss 2.6063\n",
            "Epoch 18 Batch 45 Loss 2.6032\n",
            "Epoch 18 Batch 48 Loss 2.6011\n",
            "Epoch 18 Batch 51 Loss 2.5984\n",
            "Epoch 18 Batch 54 Loss 2.6030\n",
            "Epoch 18 Batch 57 Loss 2.6065\n",
            "Epoch 18 Batch 60 Loss 2.6043\n",
            "Epoch 18 Batch 63 Loss 2.6068\n",
            "Epoch 18 Batch 66 Loss 2.6070\n",
            "Epoch 18 Batch 69 Loss 2.6054\n",
            "Epoch 18 Batch 72 Loss 2.6014\n",
            "Epoch 18 Batch 75 Loss 2.6048\n",
            "Epoch 18 Batch 78 Loss 2.6035\n",
            "Epoch 18 Batch 81 Loss 2.6012\n",
            "Epoch 18 Batch 84 Loss 2.6029\n",
            "Epoch 18 Batch 87 Loss 2.6031\n",
            "Epoch 18 Batch 90 Loss 2.6039\n",
            "Epoch 18 Batch 93 Loss 2.6050\n",
            "Epoch 18 Batch 96 Loss 2.6060\n",
            "Epoch 18 Batch 99 Loss 2.6090\n",
            "Epoch 18 Batch 102 Loss 2.6084\n",
            "Epoch 18 Batch 105 Loss 2.6097\n",
            "Epoch 18 Batch 108 Loss 2.6089\n",
            "Epoch 18 Batch 111 Loss 2.6089\n",
            "Epoch 18 Batch 114 Loss 2.6079\n",
            "Epoch 18 Batch 117 Loss 2.6079\n",
            "Epoch 18 Batch 120 Loss 2.6075\n",
            "Epoch 18 Batch 123 Loss 2.6116\n",
            "Epoch 18 Batch 126 Loss 2.6145\n",
            "Epoch 18 Batch 129 Loss 2.6163\n",
            "Epoch 18 Batch 132 Loss 2.6151\n",
            "Epoch 18 Batch 135 Loss 2.6141\n",
            "Epoch 18 Batch 138 Loss 2.6176\n",
            "Epoch 18 Batch 141 Loss 2.6156\n",
            "Epoch 18 Batch 144 Loss 2.6156\n",
            "Epoch 18 Batch 147 Loss 2.6167\n",
            "Epoch 18 Batch 150 Loss 2.6172\n",
            "Epoch 18 Batch 153 Loss 2.6158\n",
            "Epoch 18 Batch 156 Loss 2.6162\n",
            "Epoch 18 Batch 159 Loss 2.6162\n",
            "Epoch 18 Batch 162 Loss 2.6170\n",
            "Epoch 18 Batch 165 Loss 2.6176\n",
            "Epoch 18 Batch 168 Loss 2.6181\n",
            "Epoch 18 Batch 171 Loss 2.6173\n",
            "Epoch 18 Batch 174 Loss 2.6184\n",
            "Epoch 18 Batch 177 Loss 2.6180\n",
            "Epoch 18 Batch 180 Loss 2.6169\n",
            "Epoch 18 Batch 183 Loss 2.6178\n",
            "Epoch 18 Batch 186 Loss 2.6165\n",
            "Epoch 18 Batch 189 Loss 2.6176\n",
            "Epoch 18 Batch 192 Loss 2.6184\n",
            "Epoch 18 Batch 195 Loss 2.6188\n",
            "Epoch 18 Batch 198 Loss 2.6206\n",
            "Epoch 18 Batch 201 Loss 2.6232\n",
            "Epoch 18 Batch 204 Loss 2.6230\n",
            "Epoch 18 Batch 207 Loss 2.6225\n",
            "Epoch 18 Batch 210 Loss 2.6250\n",
            "Epoch 18 Batch 213 Loss 2.6254\n",
            "Epoch 18 Batch 216 Loss 2.6262\n",
            "Epoch 18 Batch 219 Loss 2.6258\n",
            "Epoch 18 Batch 222 Loss 2.6253\n",
            "Epoch 18 Batch 225 Loss 2.6257\n",
            "Epoch 18 Batch 228 Loss 2.6257\n",
            "Epoch 18 Batch 231 Loss 2.6260\n",
            "Epoch 18 Batch 234 Loss 2.6266\n",
            "Epoch 18 Batch 237 Loss 2.6266\n",
            "Epoch 18 Batch 240 Loss 2.6281\n",
            "Epoch 18 Batch 243 Loss 2.6271\n",
            "Epoch 18 Batch 246 Loss 2.6276\n",
            "Epoch 18 Batch 249 Loss 2.6278\n",
            "Epoch 18 Batch 252 Loss 2.6295\n",
            "Epoch 18 Batch 255 Loss 2.6293\n",
            "Epoch 18 Batch 258 Loss 2.6299\n",
            "Epoch 18 Batch 261 Loss 2.6299\n",
            "Epoch 18 Batch 264 Loss 2.6296\n",
            "Epoch 18 Batch 267 Loss 2.6308\n",
            "Epoch 18 Batch 270 Loss 2.6317\n",
            "Epoch 18 Batch 273 Loss 2.6310\n",
            "Epoch 18 Batch 276 Loss 2.6307\n",
            "Epoch 18 Batch 279 Loss 2.6313\n",
            "Epoch 18 Batch 282 Loss 2.6316\n",
            "Epoch 18 Batch 285 Loss 2.6309\n",
            "Epoch 18 Batch 288 Loss 2.6312\n",
            "Epoch 18 Batch 291 Loss 2.6313\n",
            "Epoch 18 Batch 294 Loss 2.6310\n",
            "Epoch 18 Batch 297 Loss 2.6307\n",
            "Epoch 18 Batch 300 Loss 2.6316\n",
            "Epoch 18 Batch 303 Loss 2.6312\n",
            "Epoch 18 Batch 306 Loss 2.6318\n",
            "Epoch 18 Batch 309 Loss 2.6329\n",
            "Epoch 18 Batch 312 Loss 2.6325\n",
            "Epoch 18 Batch 315 Loss 2.6335\n",
            "Epoch 18 Batch 318 Loss 2.6348\n",
            "Epoch 18 Batch 321 Loss 2.6341\n",
            "Epoch 18 Batch 324 Loss 2.6331\n",
            "Epoch 18 Batch 327 Loss 2.6338\n",
            "Epoch 18 Batch 330 Loss 2.6345\n",
            "Epoch 18 Batch 333 Loss 2.6348\n",
            "Epoch 18 Batch 336 Loss 2.6364\n",
            "Epoch 18 Batch 339 Loss 2.6371\n",
            "Epoch 18 Batch 342 Loss 2.6361\n",
            "Epoch 18 Batch 345 Loss 2.6367\n",
            "Epoch 18 Batch 348 Loss 2.6375\n",
            "Epoch 18 Batch 351 Loss 2.6365\n",
            "Epoch 18 Batch 354 Loss 2.6365\n",
            "Epoch 18 Batch 357 Loss 2.6373\n",
            "Epoch 18 Batch 360 Loss 2.6374\n",
            "Epoch 18 Batch 363 Loss 2.6381\n",
            "Epoch 18 Batch 366 Loss 2.6386\n",
            "Epoch 18 Batch 369 Loss 2.6391\n",
            "Epoch 18 Batch 372 Loss 2.6392\n",
            "Epoch 18 Batch 375 Loss 2.6398\n",
            "Epoch 18 Batch 378 Loss 2.6394\n",
            "Epoch 18 Batch 381 Loss 2.6396\n",
            "Epoch 18 Batch 384 Loss 2.6400\n",
            "Epoch 18 Batch 387 Loss 2.6408\n",
            "Epoch 18 Batch 390 Loss 2.6410\n",
            "Epoch 18 Batch 393 Loss 2.6416\n",
            "Epoch 18 Batch 396 Loss 2.6421\n",
            "Epoch 18 Batch 399 Loss 2.6418\n",
            "Epoch 18 Batch 402 Loss 2.6418\n",
            "Epoch 18 Batch 405 Loss 2.6426\n",
            "Epoch 18 Batch 408 Loss 2.6415\n",
            "Epoch 18 Batch 411 Loss 2.6408\n",
            "Epoch 18 Batch 414 Loss 2.6411\n",
            "Epoch 18 Batch 417 Loss 2.6412\n",
            "Epoch 18 Batch 420 Loss 2.6416\n",
            "Epoch 18 Batch 423 Loss 2.6410\n",
            "Epoch 18 Batch 426 Loss 2.6416\n",
            "Epoch 18 Batch 429 Loss 2.6416\n",
            "Epoch 18 Batch 432 Loss 2.6416\n",
            "Epoch 18 Batch 435 Loss 2.6419\n",
            "Epoch 18 Batch 438 Loss 2.6417\n",
            "Epoch 18 Batch 441 Loss 2.6421\n",
            "Epoch 18 Batch 444 Loss 2.6426\n",
            "Epoch 18 Batch 447 Loss 2.6436\n",
            "Epoch 18 Batch 450 Loss 2.6435\n",
            "Epoch 18 Batch 453 Loss 2.6437\n",
            "Epoch 18 Batch 456 Loss 2.6443\n",
            "Epoch 18 Batch 459 Loss 2.6438\n",
            "Epoch 18 Batch 462 Loss 2.6431\n",
            "Epoch 18 Batch 465 Loss 2.6424\n",
            "Epoch 18 Batch 468 Loss 2.6431\n",
            "Epoch 18 Batch 471 Loss 2.6427\n",
            "Epoch 18 Batch 474 Loss 2.6428\n",
            "Epoch 18 Batch 477 Loss 2.6421\n",
            "Epoch 18 Batch 480 Loss 2.6416\n",
            "Epoch 18 Batch 483 Loss 2.6416\n",
            "Epoch 18 Batch 486 Loss 2.6421\n",
            "Epoch 18 Batch 489 Loss 2.6419\n",
            "Epoch 18 Batch 492 Loss 2.6427\n",
            "Epoch 18 Batch 495 Loss 2.6426\n",
            "Epoch 18 Batch 498 Loss 2.6423\n",
            "Epoch 18 Batch 501 Loss 2.6423\n",
            "Epoch 18 Batch 504 Loss 2.6423\n",
            "Epoch 18 Batch 507 Loss 2.6423\n",
            "Epoch 18 Batch 510 Loss 2.6424\n",
            "Epoch 18 Batch 513 Loss 2.6418\n",
            "Epoch 18 Batch 516 Loss 2.6423\n",
            "Epoch 18 Batch 519 Loss 2.6422\n",
            "Epoch 18 Batch 522 Loss 2.6420\n",
            "Epoch 18 Batch 525 Loss 2.6418\n",
            "Epoch 18 Batch 528 Loss 2.6415\n",
            "Epoch 18 Batch 531 Loss 2.6412\n",
            "Epoch 18 Batch 534 Loss 2.6413\n",
            "Epoch 18 Batch 537 Loss 2.6415\n",
            "Epoch 18 Batch 540 Loss 2.6412\n",
            "Epoch 18 Batch 543 Loss 2.6415\n",
            "Epoch 18 Batch 546 Loss 2.6413\n",
            "Epoch 18 Batch 549 Loss 2.6412\n",
            "Epoch 18 Batch 552 Loss 2.6411\n",
            "Epoch 18 Batch 555 Loss 2.6409\n",
            "Epoch 18 Batch 558 Loss 2.6409\n",
            "Epoch 18 Batch 561 Loss 2.6414\n",
            "Epoch 18 Batch 564 Loss 2.6412\n",
            "Epoch 18 Batch 567 Loss 2.6412\n",
            "Epoch 18 Batch 570 Loss 2.6416\n",
            "Epoch 18 Batch 573 Loss 2.6412\n",
            "Epoch 18 Batch 576 Loss 2.6414\n",
            "Epoch 18 Batch 579 Loss 2.6420\n",
            "Epoch 18 Batch 582 Loss 2.6429\n",
            "Epoch 18 Batch 585 Loss 2.6435\n",
            "Epoch 18 Batch 588 Loss 2.6441\n",
            "Epoch 18 Batch 591 Loss 2.6446\n",
            "Epoch 18 Batch 594 Loss 2.6444\n",
            "Epoch 18 Batch 597 Loss 2.6444\n",
            "Epoch 18 Batch 600 Loss 2.6442\n",
            "Epoch 18 Batch 603 Loss 2.6447\n",
            "Epoch 18 Batch 606 Loss 2.6441\n",
            "Epoch 18 Batch 609 Loss 2.6439\n",
            "Epoch 18 Batch 612 Loss 2.6439\n",
            "Epoch 18 Batch 615 Loss 2.6442\n",
            "Epoch 18 Batch 618 Loss 2.6442\n",
            "Epoch 18 Batch 621 Loss 2.6442\n",
            "Epoch 18 Batch 624 Loss 2.6447\n",
            "Epoch 18 Batch 627 Loss 2.6451\n",
            "Epoch 18 Batch 630 Loss 2.6458\n",
            "Epoch 18 Batch 633 Loss 2.6454\n",
            "Epoch 18 Batch 636 Loss 2.6454\n",
            "Epoch 18 Batch 639 Loss 2.6456\n",
            "Epoch 18 Batch 642 Loss 2.6457\n",
            "Epoch 18 Batch 645 Loss 2.6464\n",
            "Epoch 18 Batch 648 Loss 2.6468\n",
            "Epoch 18 Batch 651 Loss 2.6471\n",
            "Epoch 18 Batch 654 Loss 2.6468\n",
            "Epoch 18 Batch 657 Loss 2.6469\n",
            "Epoch 18 Batch 660 Loss 2.6465\n",
            "Epoch 18 Batch 663 Loss 2.6463\n",
            "Epoch 18 Batch 666 Loss 2.6468\n",
            "Epoch 18 Batch 669 Loss 2.6472\n",
            "Epoch 18 Batch 672 Loss 2.6471\n",
            "Epoch 18 Batch 675 Loss 2.6474\n",
            "Epoch 18 Batch 678 Loss 2.6477\n",
            "Epoch 18 Batch 681 Loss 2.6478\n",
            "Epoch 18 Batch 684 Loss 2.6488\n",
            "Epoch 18 Batch 687 Loss 2.6486\n",
            "Epoch 18 Batch 690 Loss 2.6487\n",
            "Epoch 18 Batch 693 Loss 2.6492\n",
            "Epoch 18 Batch 696 Loss 2.6497\n",
            "Epoch 18 Batch 699 Loss 2.6501\n",
            "Epoch 18 Batch 702 Loss 2.6506\n",
            "Epoch 18 Batch 705 Loss 2.6508\n",
            "Epoch 18 Batch 708 Loss 2.6513\n",
            "Epoch 18 Batch 711 Loss 2.6521\n",
            "Epoch 18 Batch 714 Loss 2.6525\n",
            "Epoch 18 Batch 717 Loss 2.6529\n",
            "Epoch 18 Batch 720 Loss 2.6538\n",
            "Epoch 18 Batch 723 Loss 2.6536\n",
            "Epoch 18 Batch 726 Loss 2.6540\n",
            "Epoch 18 Batch 729 Loss 2.6550\n",
            "Epoch 18 Batch 732 Loss 2.6555\n",
            "Epoch 18 Batch 735 Loss 2.6559\n",
            "Epoch 18 Batch 738 Loss 2.6564\n",
            "Epoch 18 Batch 741 Loss 2.6568\n",
            "Epoch 18 Batch 744 Loss 2.6572\n",
            "Epoch 18 Batch 747 Loss 2.6581\n",
            "Epoch 18 Batch 750 Loss 2.6579\n",
            "Epoch 18 Batch 753 Loss 2.6581\n",
            "Epoch 18 Batch 756 Loss 2.6589\n",
            "Epoch 18 Batch 759 Loss 2.6592\n",
            "Epoch 18 Batch 762 Loss 2.6600\n",
            "Epoch 18 Batch 765 Loss 2.6603\n",
            "Epoch 18 Batch 768 Loss 2.6607\n",
            "Epoch 18 Batch 771 Loss 2.6617\n",
            "Epoch 18 Batch 774 Loss 2.6624\n",
            "Epoch 18 Batch 777 Loss 2.6631\n",
            "Epoch 18 Batch 780 Loss 2.6636\n",
            "Epoch 18 Batch 783 Loss 2.6645\n",
            "Epoch 18 Batch 786 Loss 2.6651\n",
            "Epoch 18 Batch 789 Loss 2.6653\n",
            "Epoch 18 Batch 792 Loss 2.6659\n",
            "Epoch 18 Batch 795 Loss 2.6664\n",
            "Epoch 18 Batch 798 Loss 2.6670\n",
            "Epoch 18 Batch 801 Loss 2.6675\n",
            "Epoch 18 Batch 804 Loss 2.6677\n",
            "Epoch 18 Batch 807 Loss 2.6677\n",
            "Epoch 18 Batch 810 Loss 2.6683\n",
            "Epoch 18 Batch 813 Loss 2.6686\n",
            "Epoch 18 Batch 816 Loss 2.6687\n",
            "Epoch 18 Batch 819 Loss 2.6694\n",
            "Epoch 18 Batch 822 Loss 2.6699\n",
            "Epoch 18 Batch 825 Loss 2.6700\n",
            "Epoch 18 Batch 828 Loss 2.6705\n",
            "Epoch 18 Batch 831 Loss 2.6709\n",
            "Epoch 18 Batch 834 Loss 2.6715\n",
            "Epoch 18 Batch 837 Loss 2.6719\n",
            "Epoch 18 Batch 840 Loss 2.6721\n",
            "Epoch 18 Batch 843 Loss 2.6727\n",
            "Epoch 18 Batch 846 Loss 2.6729\n",
            "Epoch 18 Batch 849 Loss 2.6727\n",
            "Epoch 18 Batch 852 Loss 2.6726\n",
            "Epoch 18 Batch 855 Loss 2.6731\n",
            "Epoch 18 Batch 858 Loss 2.6733\n",
            "Epoch 18 Loss 2.6740\n",
            "Time taken for 1 epoch: 329.6547667980194 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.7228\n",
            "Epoch 19 Batch 3 Loss 2.6089\n",
            "Epoch 19 Batch 6 Loss 2.5606\n",
            "Epoch 19 Batch 9 Loss 2.5632\n",
            "Epoch 19 Batch 12 Loss 2.5369\n",
            "Epoch 19 Batch 15 Loss 2.5390\n",
            "Epoch 19 Batch 18 Loss 2.5296\n",
            "Epoch 19 Batch 21 Loss 2.5064\n",
            "Epoch 19 Batch 24 Loss 2.5281\n",
            "Epoch 19 Batch 27 Loss 2.5127\n",
            "Epoch 19 Batch 30 Loss 2.5061\n",
            "Epoch 19 Batch 33 Loss 2.5066\n",
            "Epoch 19 Batch 36 Loss 2.5111\n",
            "Epoch 19 Batch 39 Loss 2.5066\n",
            "Epoch 19 Batch 42 Loss 2.5070\n",
            "Epoch 19 Batch 45 Loss 2.5038\n",
            "Epoch 19 Batch 48 Loss 2.5080\n",
            "Epoch 19 Batch 51 Loss 2.5113\n",
            "Epoch 19 Batch 54 Loss 2.5090\n",
            "Epoch 19 Batch 57 Loss 2.5176\n",
            "Epoch 19 Batch 60 Loss 2.5164\n",
            "Epoch 19 Batch 63 Loss 2.5179\n",
            "Epoch 19 Batch 66 Loss 2.5167\n",
            "Epoch 19 Batch 69 Loss 2.5193\n",
            "Epoch 19 Batch 72 Loss 2.5208\n",
            "Epoch 19 Batch 75 Loss 2.5215\n",
            "Epoch 19 Batch 78 Loss 2.5244\n",
            "Epoch 19 Batch 81 Loss 2.5250\n",
            "Epoch 19 Batch 84 Loss 2.5234\n",
            "Epoch 19 Batch 87 Loss 2.5234\n",
            "Epoch 19 Batch 90 Loss 2.5259\n",
            "Epoch 19 Batch 93 Loss 2.5285\n",
            "Epoch 19 Batch 96 Loss 2.5306\n",
            "Epoch 19 Batch 99 Loss 2.5326\n",
            "Epoch 19 Batch 102 Loss 2.5347\n",
            "Epoch 19 Batch 105 Loss 2.5348\n",
            "Epoch 19 Batch 108 Loss 2.5358\n",
            "Epoch 19 Batch 111 Loss 2.5365\n",
            "Epoch 19 Batch 114 Loss 2.5389\n",
            "Epoch 19 Batch 117 Loss 2.5406\n",
            "Epoch 19 Batch 120 Loss 2.5400\n",
            "Epoch 19 Batch 123 Loss 2.5400\n",
            "Epoch 19 Batch 126 Loss 2.5393\n",
            "Epoch 19 Batch 129 Loss 2.5410\n",
            "Epoch 19 Batch 132 Loss 2.5390\n",
            "Epoch 19 Batch 135 Loss 2.5410\n",
            "Epoch 19 Batch 138 Loss 2.5397\n",
            "Epoch 19 Batch 141 Loss 2.5400\n",
            "Epoch 19 Batch 144 Loss 2.5402\n",
            "Epoch 19 Batch 147 Loss 2.5410\n",
            "Epoch 19 Batch 150 Loss 2.5407\n",
            "Epoch 19 Batch 153 Loss 2.5410\n",
            "Epoch 19 Batch 156 Loss 2.5422\n",
            "Epoch 19 Batch 159 Loss 2.5433\n",
            "Epoch 19 Batch 162 Loss 2.5426\n",
            "Epoch 19 Batch 165 Loss 2.5411\n",
            "Epoch 19 Batch 168 Loss 2.5399\n",
            "Epoch 19 Batch 171 Loss 2.5388\n",
            "Epoch 19 Batch 174 Loss 2.5396\n",
            "Epoch 19 Batch 177 Loss 2.5403\n",
            "Epoch 19 Batch 180 Loss 2.5387\n",
            "Epoch 19 Batch 183 Loss 2.5399\n",
            "Epoch 19 Batch 186 Loss 2.5389\n",
            "Epoch 19 Batch 189 Loss 2.5397\n",
            "Epoch 19 Batch 192 Loss 2.5406\n",
            "Epoch 19 Batch 195 Loss 2.5427\n",
            "Epoch 19 Batch 198 Loss 2.5433\n",
            "Epoch 19 Batch 201 Loss 2.5446\n",
            "Epoch 19 Batch 204 Loss 2.5444\n",
            "Epoch 19 Batch 207 Loss 2.5436\n",
            "Epoch 19 Batch 210 Loss 2.5434\n",
            "Epoch 19 Batch 213 Loss 2.5435\n",
            "Epoch 19 Batch 216 Loss 2.5447\n",
            "Epoch 19 Batch 219 Loss 2.5451\n",
            "Epoch 19 Batch 222 Loss 2.5459\n",
            "Epoch 19 Batch 225 Loss 2.5446\n",
            "Epoch 19 Batch 228 Loss 2.5437\n",
            "Epoch 19 Batch 231 Loss 2.5447\n",
            "Epoch 19 Batch 234 Loss 2.5452\n",
            "Epoch 19 Batch 237 Loss 2.5456\n",
            "Epoch 19 Batch 240 Loss 2.5460\n",
            "Epoch 19 Batch 243 Loss 2.5451\n",
            "Epoch 19 Batch 246 Loss 2.5448\n",
            "Epoch 19 Batch 249 Loss 2.5477\n",
            "Epoch 19 Batch 252 Loss 2.5478\n",
            "Epoch 19 Batch 255 Loss 2.5495\n",
            "Epoch 19 Batch 258 Loss 2.5516\n",
            "Epoch 19 Batch 261 Loss 2.5510\n",
            "Epoch 19 Batch 264 Loss 2.5519\n",
            "Epoch 19 Batch 267 Loss 2.5510\n",
            "Epoch 19 Batch 270 Loss 2.5523\n",
            "Epoch 19 Batch 273 Loss 2.5522\n",
            "Epoch 19 Batch 276 Loss 2.5527\n",
            "Epoch 19 Batch 279 Loss 2.5519\n",
            "Epoch 19 Batch 282 Loss 2.5521\n",
            "Epoch 19 Batch 285 Loss 2.5515\n",
            "Epoch 19 Batch 288 Loss 2.5525\n",
            "Epoch 19 Batch 291 Loss 2.5519\n",
            "Epoch 19 Batch 294 Loss 2.5528\n",
            "Epoch 19 Batch 297 Loss 2.5526\n",
            "Epoch 19 Batch 300 Loss 2.5536\n",
            "Epoch 19 Batch 303 Loss 2.5538\n",
            "Epoch 19 Batch 306 Loss 2.5536\n",
            "Epoch 19 Batch 309 Loss 2.5526\n",
            "Epoch 19 Batch 312 Loss 2.5529\n",
            "Epoch 19 Batch 315 Loss 2.5527\n",
            "Epoch 19 Batch 318 Loss 2.5524\n",
            "Epoch 19 Batch 321 Loss 2.5519\n",
            "Epoch 19 Batch 324 Loss 2.5526\n",
            "Epoch 19 Batch 327 Loss 2.5531\n",
            "Epoch 19 Batch 330 Loss 2.5529\n",
            "Epoch 19 Batch 333 Loss 2.5539\n",
            "Epoch 19 Batch 336 Loss 2.5539\n",
            "Epoch 19 Batch 339 Loss 2.5542\n",
            "Epoch 19 Batch 342 Loss 2.5550\n",
            "Epoch 19 Batch 345 Loss 2.5556\n",
            "Epoch 19 Batch 348 Loss 2.5562\n",
            "Epoch 19 Batch 351 Loss 2.5569\n",
            "Epoch 19 Batch 354 Loss 2.5579\n",
            "Epoch 19 Batch 357 Loss 2.5579\n",
            "Epoch 19 Batch 360 Loss 2.5577\n",
            "Epoch 19 Batch 363 Loss 2.5570\n",
            "Epoch 19 Batch 366 Loss 2.5565\n",
            "Epoch 19 Batch 369 Loss 2.5574\n",
            "Epoch 19 Batch 372 Loss 2.5574\n",
            "Epoch 19 Batch 375 Loss 2.5579\n",
            "Epoch 19 Batch 378 Loss 2.5574\n",
            "Epoch 19 Batch 381 Loss 2.5577\n",
            "Epoch 19 Batch 384 Loss 2.5582\n",
            "Epoch 19 Batch 387 Loss 2.5585\n",
            "Epoch 19 Batch 390 Loss 2.5580\n",
            "Epoch 19 Batch 393 Loss 2.5581\n",
            "Epoch 19 Batch 396 Loss 2.5585\n",
            "Epoch 19 Batch 399 Loss 2.5589\n",
            "Epoch 19 Batch 402 Loss 2.5589\n",
            "Epoch 19 Batch 405 Loss 2.5596\n",
            "Epoch 19 Batch 408 Loss 2.5604\n",
            "Epoch 19 Batch 411 Loss 2.5605\n",
            "Epoch 19 Batch 414 Loss 2.5618\n",
            "Epoch 19 Batch 417 Loss 2.5613\n",
            "Epoch 19 Batch 420 Loss 2.5616\n",
            "Epoch 19 Batch 423 Loss 2.5615\n",
            "Epoch 19 Batch 426 Loss 2.5618\n",
            "Epoch 19 Batch 429 Loss 2.5618\n",
            "Epoch 19 Batch 432 Loss 2.5612\n",
            "Epoch 19 Batch 435 Loss 2.5614\n",
            "Epoch 19 Batch 438 Loss 2.5614\n",
            "Epoch 19 Batch 441 Loss 2.5616\n",
            "Epoch 19 Batch 444 Loss 2.5616\n",
            "Epoch 19 Batch 447 Loss 2.5623\n",
            "Epoch 19 Batch 450 Loss 2.5624\n",
            "Epoch 19 Batch 453 Loss 2.5623\n",
            "Epoch 19 Batch 456 Loss 2.5626\n",
            "Epoch 19 Batch 459 Loss 2.5624\n",
            "Epoch 19 Batch 462 Loss 2.5621\n",
            "Epoch 19 Batch 465 Loss 2.5619\n",
            "Epoch 19 Batch 468 Loss 2.5617\n",
            "Epoch 19 Batch 471 Loss 2.5617\n",
            "Epoch 19 Batch 474 Loss 2.5612\n",
            "Epoch 19 Batch 477 Loss 2.5613\n",
            "Epoch 19 Batch 480 Loss 2.5611\n",
            "Epoch 19 Batch 483 Loss 2.5614\n",
            "Epoch 19 Batch 486 Loss 2.5616\n",
            "Epoch 19 Batch 489 Loss 2.5614\n",
            "Epoch 19 Batch 492 Loss 2.5608\n",
            "Epoch 19 Batch 495 Loss 2.5608\n",
            "Epoch 19 Batch 498 Loss 2.5605\n",
            "Epoch 19 Batch 501 Loss 2.5605\n",
            "Epoch 19 Batch 504 Loss 2.5605\n",
            "Epoch 19 Batch 507 Loss 2.5607\n",
            "Epoch 19 Batch 510 Loss 2.5608\n",
            "Epoch 19 Batch 513 Loss 2.5612\n",
            "Epoch 19 Batch 516 Loss 2.5619\n",
            "Epoch 19 Batch 519 Loss 2.5623\n",
            "Epoch 19 Batch 522 Loss 2.5622\n",
            "Epoch 19 Batch 525 Loss 2.5628\n",
            "Epoch 19 Batch 528 Loss 2.5627\n",
            "Epoch 19 Batch 531 Loss 2.5620\n",
            "Epoch 19 Batch 534 Loss 2.5616\n",
            "Epoch 19 Batch 537 Loss 2.5616\n",
            "Epoch 19 Batch 540 Loss 2.5621\n",
            "Epoch 19 Batch 543 Loss 2.5624\n",
            "Epoch 19 Batch 546 Loss 2.5627\n",
            "Epoch 19 Batch 549 Loss 2.5623\n",
            "Epoch 19 Batch 552 Loss 2.5622\n",
            "Epoch 19 Batch 555 Loss 2.5627\n",
            "Epoch 19 Batch 558 Loss 2.5625\n",
            "Epoch 19 Batch 561 Loss 2.5627\n",
            "Epoch 19 Batch 564 Loss 2.5624\n",
            "Epoch 19 Batch 567 Loss 2.5624\n",
            "Epoch 19 Batch 570 Loss 2.5629\n",
            "Epoch 19 Batch 573 Loss 2.5631\n",
            "Epoch 19 Batch 576 Loss 2.5625\n",
            "Epoch 19 Batch 579 Loss 2.5630\n",
            "Epoch 19 Batch 582 Loss 2.5633\n",
            "Epoch 19 Batch 585 Loss 2.5629\n",
            "Epoch 19 Batch 588 Loss 2.5632\n",
            "Epoch 19 Batch 591 Loss 2.5632\n",
            "Epoch 19 Batch 594 Loss 2.5632\n",
            "Epoch 19 Batch 597 Loss 2.5637\n",
            "Epoch 19 Batch 600 Loss 2.5642\n",
            "Epoch 19 Batch 603 Loss 2.5644\n",
            "Epoch 19 Batch 606 Loss 2.5646\n",
            "Epoch 19 Batch 609 Loss 2.5646\n",
            "Epoch 19 Batch 612 Loss 2.5649\n",
            "Epoch 19 Batch 615 Loss 2.5645\n",
            "Epoch 19 Batch 618 Loss 2.5646\n",
            "Epoch 19 Batch 621 Loss 2.5649\n",
            "Epoch 19 Batch 624 Loss 2.5650\n",
            "Epoch 19 Batch 627 Loss 2.5651\n",
            "Epoch 19 Batch 630 Loss 2.5658\n",
            "Epoch 19 Batch 633 Loss 2.5658\n",
            "Epoch 19 Batch 636 Loss 2.5659\n",
            "Epoch 19 Batch 639 Loss 2.5663\n",
            "Epoch 19 Batch 642 Loss 2.5667\n",
            "Epoch 19 Batch 645 Loss 2.5673\n",
            "Epoch 19 Batch 648 Loss 2.5679\n",
            "Epoch 19 Batch 651 Loss 2.5678\n",
            "Epoch 19 Batch 654 Loss 2.5679\n",
            "Epoch 19 Batch 657 Loss 2.5679\n",
            "Epoch 19 Batch 660 Loss 2.5687\n",
            "Epoch 19 Batch 663 Loss 2.5686\n",
            "Epoch 19 Batch 666 Loss 2.5689\n",
            "Epoch 19 Batch 669 Loss 2.5691\n",
            "Epoch 19 Batch 672 Loss 2.5689\n",
            "Epoch 19 Batch 675 Loss 2.5692\n",
            "Epoch 19 Batch 678 Loss 2.5698\n",
            "Epoch 19 Batch 681 Loss 2.5700\n",
            "Epoch 19 Batch 684 Loss 2.5703\n",
            "Epoch 19 Batch 687 Loss 2.5706\n",
            "Epoch 19 Batch 690 Loss 2.5712\n",
            "Epoch 19 Batch 693 Loss 2.5717\n",
            "Epoch 19 Batch 696 Loss 2.5722\n",
            "Epoch 19 Batch 699 Loss 2.5729\n",
            "Epoch 19 Batch 702 Loss 2.5732\n",
            "Epoch 19 Batch 705 Loss 2.5737\n",
            "Epoch 19 Batch 708 Loss 2.5743\n",
            "Epoch 19 Batch 711 Loss 2.5750\n",
            "Epoch 19 Batch 714 Loss 2.5752\n",
            "Epoch 19 Batch 717 Loss 2.5755\n",
            "Epoch 19 Batch 720 Loss 2.5760\n",
            "Epoch 19 Batch 723 Loss 2.5770\n",
            "Epoch 19 Batch 726 Loss 2.5770\n",
            "Epoch 19 Batch 729 Loss 2.5778\n",
            "Epoch 19 Batch 732 Loss 2.5784\n",
            "Epoch 19 Batch 735 Loss 2.5789\n",
            "Epoch 19 Batch 738 Loss 2.5787\n",
            "Epoch 19 Batch 741 Loss 2.5790\n",
            "Epoch 19 Batch 744 Loss 2.5792\n",
            "Epoch 19 Batch 747 Loss 2.5794\n",
            "Epoch 19 Batch 750 Loss 2.5799\n",
            "Epoch 19 Batch 753 Loss 2.5805\n",
            "Epoch 19 Batch 756 Loss 2.5809\n",
            "Epoch 19 Batch 759 Loss 2.5810\n",
            "Epoch 19 Batch 762 Loss 2.5814\n",
            "Epoch 19 Batch 765 Loss 2.5821\n",
            "Epoch 19 Batch 768 Loss 2.5824\n",
            "Epoch 19 Batch 771 Loss 2.5830\n",
            "Epoch 19 Batch 774 Loss 2.5841\n",
            "Epoch 19 Batch 777 Loss 2.5849\n",
            "Epoch 19 Batch 780 Loss 2.5853\n",
            "Epoch 19 Batch 783 Loss 2.5857\n",
            "Epoch 19 Batch 786 Loss 2.5863\n",
            "Epoch 19 Batch 789 Loss 2.5871\n",
            "Epoch 19 Batch 792 Loss 2.5875\n",
            "Epoch 19 Batch 795 Loss 2.5878\n",
            "Epoch 19 Batch 798 Loss 2.5885\n",
            "Epoch 19 Batch 801 Loss 2.5890\n",
            "Epoch 19 Batch 804 Loss 2.5897\n",
            "Epoch 19 Batch 807 Loss 2.5901\n",
            "Epoch 19 Batch 810 Loss 2.5904\n",
            "Epoch 19 Batch 813 Loss 2.5911\n",
            "Epoch 19 Batch 816 Loss 2.5918\n",
            "Epoch 19 Batch 819 Loss 2.5921\n",
            "Epoch 19 Batch 822 Loss 2.5928\n",
            "Epoch 19 Batch 825 Loss 2.5930\n",
            "Epoch 19 Batch 828 Loss 2.5939\n",
            "Epoch 19 Batch 831 Loss 2.5942\n",
            "Epoch 19 Batch 834 Loss 2.5949\n",
            "Epoch 19 Batch 837 Loss 2.5956\n",
            "Epoch 19 Batch 840 Loss 2.5959\n",
            "Epoch 19 Batch 843 Loss 2.5962\n",
            "Epoch 19 Batch 846 Loss 2.5965\n",
            "Epoch 19 Batch 849 Loss 2.5969\n",
            "Epoch 19 Batch 852 Loss 2.5975\n",
            "Epoch 19 Batch 855 Loss 2.5975\n",
            "Epoch 19 Batch 858 Loss 2.5980\n",
            "Epoch 19 Loss 2.5984\n",
            "Time taken for 1 epoch: 329.289098739624 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.4214\n",
            "Epoch 20 Batch 3 Loss 2.4826\n",
            "Epoch 20 Batch 6 Loss 2.4774\n",
            "Epoch 20 Batch 9 Loss 2.4999\n",
            "Epoch 20 Batch 12 Loss 2.4921\n",
            "Epoch 20 Batch 15 Loss 2.4860\n",
            "Epoch 20 Batch 18 Loss 2.4719\n",
            "Epoch 20 Batch 21 Loss 2.4815\n",
            "Epoch 20 Batch 24 Loss 2.4838\n",
            "Epoch 20 Batch 27 Loss 2.4829\n",
            "Epoch 20 Batch 30 Loss 2.4846\n",
            "Epoch 20 Batch 33 Loss 2.4798\n",
            "Epoch 20 Batch 36 Loss 2.4708\n",
            "Epoch 20 Batch 39 Loss 2.4698\n",
            "Epoch 20 Batch 42 Loss 2.4648\n",
            "Epoch 20 Batch 45 Loss 2.4610\n",
            "Epoch 20 Batch 48 Loss 2.4650\n",
            "Epoch 20 Batch 51 Loss 2.4643\n",
            "Epoch 20 Batch 54 Loss 2.4675\n",
            "Epoch 20 Batch 57 Loss 2.4659\n",
            "Epoch 20 Batch 60 Loss 2.4646\n",
            "Epoch 20 Batch 63 Loss 2.4598\n",
            "Epoch 20 Batch 66 Loss 2.4634\n",
            "Epoch 20 Batch 69 Loss 2.4678\n",
            "Epoch 20 Batch 72 Loss 2.4643\n",
            "Epoch 20 Batch 75 Loss 2.4636\n",
            "Epoch 20 Batch 78 Loss 2.4654\n",
            "Epoch 20 Batch 81 Loss 2.4632\n",
            "Epoch 20 Batch 84 Loss 2.4654\n",
            "Epoch 20 Batch 87 Loss 2.4621\n",
            "Epoch 20 Batch 90 Loss 2.4605\n",
            "Epoch 20 Batch 93 Loss 2.4584\n",
            "Epoch 20 Batch 96 Loss 2.4594\n",
            "Epoch 20 Batch 99 Loss 2.4555\n",
            "Epoch 20 Batch 102 Loss 2.4568\n",
            "Epoch 20 Batch 105 Loss 2.4570\n",
            "Epoch 20 Batch 108 Loss 2.4586\n",
            "Epoch 20 Batch 111 Loss 2.4585\n",
            "Epoch 20 Batch 114 Loss 2.4563\n",
            "Epoch 20 Batch 117 Loss 2.4580\n",
            "Epoch 20 Batch 120 Loss 2.4586\n",
            "Epoch 20 Batch 123 Loss 2.4593\n",
            "Epoch 20 Batch 126 Loss 2.4619\n",
            "Epoch 20 Batch 129 Loss 2.4630\n",
            "Epoch 20 Batch 132 Loss 2.4613\n",
            "Epoch 20 Batch 135 Loss 2.4597\n",
            "Epoch 20 Batch 138 Loss 2.4626\n",
            "Epoch 20 Batch 141 Loss 2.4638\n",
            "Epoch 20 Batch 144 Loss 2.4626\n",
            "Epoch 20 Batch 147 Loss 2.4650\n",
            "Epoch 20 Batch 150 Loss 2.4676\n",
            "Epoch 20 Batch 153 Loss 2.4665\n",
            "Epoch 20 Batch 156 Loss 2.4669\n",
            "Epoch 20 Batch 159 Loss 2.4658\n",
            "Epoch 20 Batch 162 Loss 2.4672\n",
            "Epoch 20 Batch 165 Loss 2.4659\n",
            "Epoch 20 Batch 168 Loss 2.4667\n",
            "Epoch 20 Batch 171 Loss 2.4680\n",
            "Epoch 20 Batch 174 Loss 2.4673\n",
            "Epoch 20 Batch 177 Loss 2.4671\n",
            "Epoch 20 Batch 180 Loss 2.4700\n",
            "Epoch 20 Batch 183 Loss 2.4700\n",
            "Epoch 20 Batch 186 Loss 2.4691\n",
            "Epoch 20 Batch 189 Loss 2.4686\n",
            "Epoch 20 Batch 192 Loss 2.4694\n",
            "Epoch 20 Batch 195 Loss 2.4719\n",
            "Epoch 20 Batch 198 Loss 2.4699\n",
            "Epoch 20 Batch 201 Loss 2.4699\n",
            "Epoch 20 Batch 204 Loss 2.4716\n",
            "Epoch 20 Batch 207 Loss 2.4727\n",
            "Epoch 20 Batch 210 Loss 2.4742\n",
            "Epoch 20 Batch 213 Loss 2.4753\n",
            "Epoch 20 Batch 216 Loss 2.4764\n",
            "Epoch 20 Batch 219 Loss 2.4781\n",
            "Epoch 20 Batch 222 Loss 2.4778\n",
            "Epoch 20 Batch 225 Loss 2.4784\n",
            "Epoch 20 Batch 228 Loss 2.4790\n",
            "Epoch 20 Batch 231 Loss 2.4789\n",
            "Epoch 20 Batch 234 Loss 2.4787\n",
            "Epoch 20 Batch 237 Loss 2.4773\n",
            "Epoch 20 Batch 240 Loss 2.4785\n",
            "Epoch 20 Batch 243 Loss 2.4786\n",
            "Epoch 20 Batch 246 Loss 2.4790\n",
            "Epoch 20 Batch 249 Loss 2.4797\n",
            "Epoch 20 Batch 252 Loss 2.4804\n",
            "Epoch 20 Batch 255 Loss 2.4817\n",
            "Epoch 20 Batch 258 Loss 2.4825\n",
            "Epoch 20 Batch 261 Loss 2.4826\n",
            "Epoch 20 Batch 264 Loss 2.4838\n",
            "Epoch 20 Batch 267 Loss 2.4837\n",
            "Epoch 20 Batch 270 Loss 2.4847\n",
            "Epoch 20 Batch 273 Loss 2.4848\n",
            "Epoch 20 Batch 276 Loss 2.4852\n",
            "Epoch 20 Batch 279 Loss 2.4850\n",
            "Epoch 20 Batch 282 Loss 2.4859\n",
            "Epoch 20 Batch 285 Loss 2.4861\n",
            "Epoch 20 Batch 288 Loss 2.4866\n",
            "Epoch 20 Batch 291 Loss 2.4861\n",
            "Epoch 20 Batch 294 Loss 2.4870\n",
            "Epoch 20 Batch 297 Loss 2.4879\n",
            "Epoch 20 Batch 300 Loss 2.4888\n",
            "Epoch 20 Batch 303 Loss 2.4900\n",
            "Epoch 20 Batch 306 Loss 2.4907\n",
            "Epoch 20 Batch 309 Loss 2.4897\n",
            "Epoch 20 Batch 312 Loss 2.4899\n",
            "Epoch 20 Batch 315 Loss 2.4911\n",
            "Epoch 20 Batch 318 Loss 2.4922\n",
            "Epoch 20 Batch 321 Loss 2.4912\n",
            "Epoch 20 Batch 324 Loss 2.4918\n",
            "Epoch 20 Batch 327 Loss 2.4924\n",
            "Epoch 20 Batch 330 Loss 2.4924\n",
            "Epoch 20 Batch 333 Loss 2.4921\n",
            "Epoch 20 Batch 336 Loss 2.4920\n",
            "Epoch 20 Batch 339 Loss 2.4911\n",
            "Epoch 20 Batch 342 Loss 2.4916\n",
            "Epoch 20 Batch 345 Loss 2.4924\n",
            "Epoch 20 Batch 348 Loss 2.4918\n",
            "Epoch 20 Batch 351 Loss 2.4920\n",
            "Epoch 20 Batch 354 Loss 2.4922\n",
            "Epoch 20 Batch 357 Loss 2.4926\n",
            "Epoch 20 Batch 360 Loss 2.4935\n",
            "Epoch 20 Batch 363 Loss 2.4928\n",
            "Epoch 20 Batch 366 Loss 2.4929\n",
            "Epoch 20 Batch 369 Loss 2.4921\n",
            "Epoch 20 Batch 372 Loss 2.4916\n",
            "Epoch 20 Batch 375 Loss 2.4911\n",
            "Epoch 20 Batch 378 Loss 2.4905\n",
            "Epoch 20 Batch 381 Loss 2.4911\n",
            "Epoch 20 Batch 384 Loss 2.4913\n",
            "Epoch 20 Batch 387 Loss 2.4919\n",
            "Epoch 20 Batch 390 Loss 2.4921\n",
            "Epoch 20 Batch 393 Loss 2.4929\n",
            "Epoch 20 Batch 396 Loss 2.4931\n",
            "Epoch 20 Batch 399 Loss 2.4928\n",
            "Epoch 20 Batch 402 Loss 2.4928\n",
            "Epoch 20 Batch 405 Loss 2.4938\n",
            "Epoch 20 Batch 408 Loss 2.4939\n",
            "Epoch 20 Batch 411 Loss 2.4945\n",
            "Epoch 20 Batch 414 Loss 2.4948\n",
            "Epoch 20 Batch 417 Loss 2.4952\n",
            "Epoch 20 Batch 420 Loss 2.4947\n",
            "Epoch 20 Batch 423 Loss 2.4945\n",
            "Epoch 20 Batch 426 Loss 2.4950\n",
            "Epoch 20 Batch 429 Loss 2.4956\n",
            "Epoch 20 Batch 432 Loss 2.4950\n",
            "Epoch 20 Batch 435 Loss 2.4945\n",
            "Epoch 20 Batch 438 Loss 2.4945\n",
            "Epoch 20 Batch 441 Loss 2.4943\n",
            "Epoch 20 Batch 444 Loss 2.4945\n",
            "Epoch 20 Batch 447 Loss 2.4944\n",
            "Epoch 20 Batch 450 Loss 2.4943\n",
            "Epoch 20 Batch 453 Loss 2.4942\n",
            "Epoch 20 Batch 456 Loss 2.4937\n",
            "Epoch 20 Batch 459 Loss 2.4936\n",
            "Epoch 20 Batch 462 Loss 2.4935\n",
            "Epoch 20 Batch 465 Loss 2.4929\n",
            "Epoch 20 Batch 468 Loss 2.4932\n",
            "Epoch 20 Batch 471 Loss 2.4933\n",
            "Epoch 20 Batch 474 Loss 2.4931\n",
            "Epoch 20 Batch 477 Loss 2.4938\n",
            "Epoch 20 Batch 480 Loss 2.4942\n",
            "Epoch 20 Batch 483 Loss 2.4947\n",
            "Epoch 20 Batch 486 Loss 2.4946\n",
            "Epoch 20 Batch 489 Loss 2.4940\n",
            "Epoch 20 Batch 492 Loss 2.4941\n",
            "Epoch 20 Batch 495 Loss 2.4939\n",
            "Epoch 20 Batch 498 Loss 2.4940\n",
            "Epoch 20 Batch 501 Loss 2.4944\n",
            "Epoch 20 Batch 504 Loss 2.4945\n",
            "Epoch 20 Batch 507 Loss 2.4942\n",
            "Epoch 20 Batch 510 Loss 2.4942\n",
            "Epoch 20 Batch 513 Loss 2.4942\n",
            "Epoch 20 Batch 516 Loss 2.4934\n",
            "Epoch 20 Batch 519 Loss 2.4934\n",
            "Epoch 20 Batch 522 Loss 2.4933\n",
            "Epoch 20 Batch 525 Loss 2.4935\n",
            "Epoch 20 Batch 528 Loss 2.4940\n",
            "Epoch 20 Batch 531 Loss 2.4944\n",
            "Epoch 20 Batch 534 Loss 2.4946\n",
            "Epoch 20 Batch 537 Loss 2.4948\n",
            "Epoch 20 Batch 540 Loss 2.4944\n",
            "Epoch 20 Batch 543 Loss 2.4954\n",
            "Epoch 20 Batch 546 Loss 2.4951\n",
            "Epoch 20 Batch 549 Loss 2.4945\n",
            "Epoch 20 Batch 552 Loss 2.4943\n",
            "Epoch 20 Batch 555 Loss 2.4946\n",
            "Epoch 20 Batch 558 Loss 2.4954\n",
            "Epoch 20 Batch 561 Loss 2.4953\n",
            "Epoch 20 Batch 564 Loss 2.4951\n",
            "Epoch 20 Batch 567 Loss 2.4953\n",
            "Epoch 20 Batch 570 Loss 2.4959\n",
            "Epoch 20 Batch 573 Loss 2.4961\n",
            "Epoch 20 Batch 576 Loss 2.4967\n",
            "Epoch 20 Batch 579 Loss 2.4974\n",
            "Epoch 20 Batch 582 Loss 2.4974\n",
            "Epoch 20 Batch 585 Loss 2.4975\n",
            "Epoch 20 Batch 588 Loss 2.4978\n",
            "Epoch 20 Batch 591 Loss 2.4974\n",
            "Epoch 20 Batch 594 Loss 2.4970\n",
            "Epoch 20 Batch 597 Loss 2.4972\n",
            "Epoch 20 Batch 600 Loss 2.4974\n",
            "Epoch 20 Batch 603 Loss 2.4980\n",
            "Epoch 20 Batch 606 Loss 2.4980\n",
            "Epoch 20 Batch 609 Loss 2.4981\n",
            "Epoch 20 Batch 612 Loss 2.4983\n",
            "Epoch 20 Batch 615 Loss 2.4987\n",
            "Epoch 20 Batch 618 Loss 2.4987\n",
            "Epoch 20 Batch 621 Loss 2.4984\n",
            "Epoch 20 Batch 624 Loss 2.4981\n",
            "Epoch 20 Batch 627 Loss 2.4982\n",
            "Epoch 20 Batch 630 Loss 2.4985\n",
            "Epoch 20 Batch 633 Loss 2.4989\n",
            "Epoch 20 Batch 636 Loss 2.4991\n",
            "Epoch 20 Batch 639 Loss 2.4992\n",
            "Epoch 20 Batch 642 Loss 2.4997\n",
            "Epoch 20 Batch 645 Loss 2.5001\n",
            "Epoch 20 Batch 648 Loss 2.5004\n",
            "Epoch 20 Batch 651 Loss 2.5006\n",
            "Epoch 20 Batch 654 Loss 2.5011\n",
            "Epoch 20 Batch 657 Loss 2.5011\n",
            "Epoch 20 Batch 660 Loss 2.5016\n",
            "Epoch 20 Batch 663 Loss 2.5019\n",
            "Epoch 20 Batch 666 Loss 2.5019\n",
            "Epoch 20 Batch 669 Loss 2.5022\n",
            "Epoch 20 Batch 672 Loss 2.5023\n",
            "Epoch 20 Batch 675 Loss 2.5025\n",
            "Epoch 20 Batch 678 Loss 2.5031\n",
            "Epoch 20 Batch 681 Loss 2.5032\n",
            "Epoch 20 Batch 684 Loss 2.5035\n",
            "Epoch 20 Batch 687 Loss 2.5035\n",
            "Epoch 20 Batch 690 Loss 2.5046\n",
            "Epoch 20 Batch 693 Loss 2.5052\n",
            "Epoch 20 Batch 696 Loss 2.5050\n",
            "Epoch 20 Batch 699 Loss 2.5059\n",
            "Epoch 20 Batch 702 Loss 2.5061\n",
            "Epoch 20 Batch 705 Loss 2.5060\n",
            "Epoch 20 Batch 708 Loss 2.5060\n",
            "Epoch 20 Batch 711 Loss 2.5060\n",
            "Epoch 20 Batch 714 Loss 2.5064\n",
            "Epoch 20 Batch 717 Loss 2.5068\n",
            "Epoch 20 Batch 720 Loss 2.5074\n",
            "Epoch 20 Batch 723 Loss 2.5078\n",
            "Epoch 20 Batch 726 Loss 2.5084\n",
            "Epoch 20 Batch 729 Loss 2.5089\n",
            "Epoch 20 Batch 732 Loss 2.5096\n",
            "Epoch 20 Batch 735 Loss 2.5100\n",
            "Epoch 20 Batch 738 Loss 2.5103\n",
            "Epoch 20 Batch 741 Loss 2.5103\n",
            "Epoch 20 Batch 744 Loss 2.5107\n",
            "Epoch 20 Batch 747 Loss 2.5114\n",
            "Epoch 20 Batch 750 Loss 2.5111\n",
            "Epoch 20 Batch 753 Loss 2.5115\n",
            "Epoch 20 Batch 756 Loss 2.5121\n",
            "Epoch 20 Batch 759 Loss 2.5124\n",
            "Epoch 20 Batch 762 Loss 2.5125\n",
            "Epoch 20 Batch 765 Loss 2.5127\n",
            "Epoch 20 Batch 768 Loss 2.5132\n",
            "Epoch 20 Batch 771 Loss 2.5135\n",
            "Epoch 20 Batch 774 Loss 2.5136\n",
            "Epoch 20 Batch 777 Loss 2.5140\n",
            "Epoch 20 Batch 780 Loss 2.5144\n",
            "Epoch 20 Batch 783 Loss 2.5150\n",
            "Epoch 20 Batch 786 Loss 2.5153\n",
            "Epoch 20 Batch 789 Loss 2.5159\n",
            "Epoch 20 Batch 792 Loss 2.5162\n",
            "Epoch 20 Batch 795 Loss 2.5171\n",
            "Epoch 20 Batch 798 Loss 2.5175\n",
            "Epoch 20 Batch 801 Loss 2.5182\n",
            "Epoch 20 Batch 804 Loss 2.5185\n",
            "Epoch 20 Batch 807 Loss 2.5188\n",
            "Epoch 20 Batch 810 Loss 2.5194\n",
            "Epoch 20 Batch 813 Loss 2.5200\n",
            "Epoch 20 Batch 816 Loss 2.5204\n",
            "Epoch 20 Batch 819 Loss 2.5207\n",
            "Epoch 20 Batch 822 Loss 2.5209\n",
            "Epoch 20 Batch 825 Loss 2.5218\n",
            "Epoch 20 Batch 828 Loss 2.5224\n",
            "Epoch 20 Batch 831 Loss 2.5230\n",
            "Epoch 20 Batch 834 Loss 2.5235\n",
            "Epoch 20 Batch 837 Loss 2.5243\n",
            "Epoch 20 Batch 840 Loss 2.5249\n",
            "Epoch 20 Batch 843 Loss 2.5257\n",
            "Epoch 20 Batch 846 Loss 2.5262\n",
            "Epoch 20 Batch 849 Loss 2.5262\n",
            "Epoch 20 Batch 852 Loss 2.5268\n",
            "Epoch 20 Batch 855 Loss 2.5271\n",
            "Epoch 20 Batch 858 Loss 2.5274\n",
            "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
            "Epoch 20 Loss 2.5275\n",
            "Time taken for 1 epoch: 332.42500853538513 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 3 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_path = \"saved_model_2\"\n",
        "tf.saved_model.save(transformer, export_path)\n",
        "print(\"Model exported to:\", export_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BNIWhXf7cj8",
        "outputId": "1f9238ca-676f-4d9d-b9a9-8f921f4383e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_8_layer_call_fn, dropout_8_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 224). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to: saved_model_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights_list = []\n",
        "probabilities_list = []\n",
        "def evaluate(input_document, document_tokenizer, summary_tokenizer):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "    print('enc shape:', encoder_input)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]] * (decoder_maxlen - 1)\n",
        "    decoder_input = tf.expand_dims(decoder_input, 0)\n",
        "    print(decoder_input)\n",
        "    #decoder_input = tf.pad(decoder_input, [[0, 0], [0, decoder_maxlen - 1 - len(decoder_input[0])]], constant_values=0)\n",
        "\n",
        "    output = decoder_input\n",
        "    print('dec shape:', output)\n",
        "\n",
        "    for i in range(decoder_maxlen):\n",
        "        print('------')\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "        print(output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        attention_weights_list.append(attention_weights['decoder_layer4_block2'][0])\n",
        "        step_probabilities = predictions[:, i, :]\n",
        "        print('PROB', step_probabilities)\n",
        "        probabilities_list.append(step_probabilities)\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        print('id', predicted_id)\n",
        "        print('att', attention_weights['decoder_layer4_block2'][0])\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output[:, 1:], predicted_id], axis=-1)\n",
        "        print('predict', output)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "metadata": {
        "id": "ZkeCfveq7xDd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(input_document, document_tokenizer, summary_tokenizer):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document, document_tokenizer, summary_tokenizer)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "metadata": {
        "id": "nwZMBRVq75OR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\",\n",
        "    document_tokenizer,\n",
        "    summary_tokenizer\n",
        ")\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtGxk6SE77tG",
        "outputId": "15c3bf66-b1d4-492e-dfea-b0da978717c9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc shape: tf.Tensor(\n",
            "[[   48    77   606  2939   403   431  6434    15     5  1163     3  2380\n",
            "     71  7733   100     3 11785   100     5  1039     1   831  1284  1306\n",
            "   2682     2  5326   331  1093     1  8180   145  6658  3239   654    15\n",
            "     59    36  7791     7  1896   769     5     2  2399  2001   243    86\n",
            "      2     1   769     1    15  1338     3  3766     7  3426  1449     5\n",
            "   1306  2682     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]], shape=(1, 400), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2]], shape=(1, 74), dtype=int32)\n",
            "dec shape: tf.Tensor(\n",
            "[[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2]], shape=(1, 74), dtype=int32)\n",
            "------\n",
            "tf.Tensor(\n",
            "[[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "  2 2]], shape=(1, 74), dtype=int32)\n",
            "PROB tf.Tensor(\n",
            "[[-12.546207  -12.546219  -12.546761  ...  -6.136373   -7.5261645\n",
            "   -7.484682 ]], shape=(1, 29661), dtype=float32)\n",
            "id tf.Tensor([[4853]], shape=(1, 1), dtype=int32)\n",
            "att tf.Tensor(\n",
            "[[[0.01627228 0.01639797 0.01617941 ... 0.         0.         0.        ]\n",
            "  [0.01627582 0.01639386 0.01617789 ... 0.         0.         0.        ]\n",
            "  [0.01629905 0.01636428 0.01616983 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.0162295  0.0163897  0.01618057 ... 0.         0.         0.        ]\n",
            "  [0.01622869 0.01639127 0.01618209 ... 0.         0.         0.        ]\n",
            "  [0.01622837 0.01639258 0.01618314 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01625999 0.01621002 0.01629815 ... 0.         0.         0.        ]\n",
            "  [0.01625497 0.01621142 0.01629853 ... 0.         0.         0.        ]\n",
            "  [0.01623045 0.01622118 0.01628894 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01627892 0.01618392 0.01625831 ... 0.         0.         0.        ]\n",
            "  [0.01628217 0.01618327 0.01626191 ... 0.         0.         0.        ]\n",
            "  [0.01628562 0.01618265 0.01626457 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01627508 0.01599409 0.01616597 ... 0.         0.         0.        ]\n",
            "  [0.01627595 0.01599237 0.01616511 ... 0.         0.         0.        ]\n",
            "  [0.01627555 0.01599778 0.01616516 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01629    0.01599627 0.01619278 ... 0.         0.         0.        ]\n",
            "  [0.016289   0.01599472 0.01619288 ... 0.         0.         0.        ]\n",
            "  [0.01628881 0.01599315 0.01619253 ... 0.         0.         0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.01594171 0.01623127 0.01629258 ... 0.         0.         0.        ]\n",
            "  [0.01593834 0.01622525 0.01628953 ... 0.         0.         0.        ]\n",
            "  [0.01592826 0.01620626 0.01628699 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01593426 0.01625819 0.01629478 ... 0.         0.         0.        ]\n",
            "  [0.01593475 0.01625541 0.01629327 ... 0.         0.         0.        ]\n",
            "  [0.01593535 0.01625338 0.01629187 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01604964 0.01607188 0.01616325 ... 0.         0.         0.        ]\n",
            "  [0.01604607 0.01607056 0.01615985 ... 0.         0.         0.        ]\n",
            "  [0.0160254  0.0160669  0.0161433  ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01603638 0.01609031 0.01613695 ... 0.         0.         0.        ]\n",
            "  [0.01603707 0.01609328 0.01613711 ... 0.         0.         0.        ]\n",
            "  [0.01603841 0.01609639 0.01613643 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01641355 0.01624562 0.0160319  ... 0.         0.         0.        ]\n",
            "  [0.01641744 0.01624642 0.01603037 ... 0.         0.         0.        ]\n",
            "  [0.01642586 0.01624469 0.01602766 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01639729 0.01623199 0.01595313 ... 0.         0.         0.        ]\n",
            "  [0.01639871 0.01623137 0.01595642 ... 0.         0.         0.        ]\n",
            "  [0.01639957 0.01623082 0.01595973 ... 0.         0.         0.        ]]], shape=(8, 74, 400), dtype=float32)\n",
            "predict tf.Tensor(\n",
            "[[   2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2 4853]], shape=(1, 74), dtype=int32)\n",
            "------\n",
            "tf.Tensor(\n",
            "[[   2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
            "     2    2    2 4853]], shape=(1, 74), dtype=int32)\n",
            "PROB tf.Tensor(\n",
            "[[-12.5734625 -12.573468  -12.574013  ...  -6.114979   -7.5431485\n",
            "   -7.449786 ]], shape=(1, 29661), dtype=float32)\n",
            "id tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n",
            "att tf.Tensor(\n",
            "[[[0.01627228 0.01639797 0.01617941 ... 0.         0.         0.        ]\n",
            "  [0.01627582 0.01639386 0.01617789 ... 0.         0.         0.        ]\n",
            "  [0.01629905 0.01636428 0.01616983 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.0162295  0.0163897  0.01618057 ... 0.         0.         0.        ]\n",
            "  [0.01622869 0.01639127 0.01618209 ... 0.         0.         0.        ]\n",
            "  [0.01623516 0.0159828  0.0160277  ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01625999 0.01621002 0.01629815 ... 0.         0.         0.        ]\n",
            "  [0.01625497 0.01621142 0.01629853 ... 0.         0.         0.        ]\n",
            "  [0.01623045 0.01622118 0.01628894 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01627892 0.01618392 0.01625831 ... 0.         0.         0.        ]\n",
            "  [0.01628217 0.01618327 0.01626191 ... 0.         0.         0.        ]\n",
            "  [0.01571224 0.01634538 0.01610308 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01627508 0.01599409 0.01616597 ... 0.         0.         0.        ]\n",
            "  [0.01627595 0.01599237 0.01616511 ... 0.         0.         0.        ]\n",
            "  [0.01627555 0.01599778 0.01616516 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01629    0.01599627 0.01619278 ... 0.         0.         0.        ]\n",
            "  [0.016289   0.01599472 0.01619288 ... 0.         0.         0.        ]\n",
            "  [0.0160934  0.01626506 0.0161858  ... 0.         0.         0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.01594171 0.01623127 0.01629258 ... 0.         0.         0.        ]\n",
            "  [0.01593834 0.01622525 0.01628953 ... 0.         0.         0.        ]\n",
            "  [0.01592826 0.01620626 0.01628699 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01593426 0.01625819 0.01629478 ... 0.         0.         0.        ]\n",
            "  [0.01593475 0.01625541 0.01629327 ... 0.         0.         0.        ]\n",
            "  [0.01608774 0.0162228  0.01612627 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01604964 0.01607188 0.01616325 ... 0.         0.         0.        ]\n",
            "  [0.01604607 0.01607056 0.01615985 ... 0.         0.         0.        ]\n",
            "  [0.0160254  0.0160669  0.0161433  ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01603638 0.01609031 0.01613695 ... 0.         0.         0.        ]\n",
            "  [0.01603707 0.01609328 0.01613711 ... 0.         0.         0.        ]\n",
            "  [0.01634284 0.01612898 0.01609946 ... 0.         0.         0.        ]]\n",
            "\n",
            " [[0.01641355 0.01624562 0.0160319  ... 0.         0.         0.        ]\n",
            "  [0.01641744 0.01624642 0.01603037 ... 0.         0.         0.        ]\n",
            "  [0.01642586 0.01624469 0.01602766 ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.01639729 0.01623199 0.01595313 ... 0.         0.         0.        ]\n",
            "  [0.01639871 0.01623137 0.01595642 ... 0.         0.         0.        ]\n",
            "  [0.01586753 0.01612187 0.01591236 ... 0.         0.         0.        ]]], shape=(8, 74, 400), dtype=float32)\n",
            "<go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> <go> walmart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(attention_weights_list[0].shape)\n",
        "word_labels = summary_tokenizer.index_word.values()\n",
        "num_words = len(document_tokenizer.index_word)\n",
        "for i in range(len(attention_weights_list)):\n",
        "    for h in range(num_heads):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        data = attention_weights_list[i]\n",
        "        im = ax.imshow(data[h,:62,:62], cmap='hot')\n",
        "\n",
        "        # Add labels, title, and colorbar\n",
        "        #ax.set_xlabel('Encoder timestep')\n",
        "        #ax.set_xticks(len(word_labels))\n",
        "        #ax.set_xticklabels(word_labels)\n",
        "\n",
        "        ax.set_ylabel('Decoder timestep')\n",
        "        ax.set_title(f'Attention Heatmap - Step {i+1} Head {h+1}')\n",
        "        plt.colorbar(im)\n",
        "\n",
        "        # Save the heatmap as an image\n",
        "        plt.savefig(f'heatmap_step_{i+1}_head{h+1}.png', dpi=500)\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrIfwpYZ8U-0",
        "outputId": "53659779-f2e2-4316-bc51-e9dea3abb7e8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 74, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "F5D5cv2Jd8-6"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "UkpdiW6wnmiS"
      },
      "outputs": [],
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WZoEHvIxrYKZ",
        "outputId": "79eb6122-b4d1-412e-cdf7-cc3e1df91c3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'walmart to invest 50 mn in india report'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNVOWPXFIn0k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}